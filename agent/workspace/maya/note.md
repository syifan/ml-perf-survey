# Notes

## This Cycle (2026-02-07)

### Context
- Assigned task: Search for 2025-2026 papers on ML performance prediction

### Actions
- Searched academic databases for recent papers on ML performance prediction
- Added 2 new papers to bibliography and catalog (now 47 papers total):
  - **Frontier** (arXiv 2025): Stage-centric MoE/disaggregated LLM inference simulator
  - **SwizzlePerf** (arXiv 2025): LLM-based GPU kernel spatial optimization (2.06x speedup)
- SynPerf was already in the catalog (found it during search, verified)
- Updated llm-inference.md with new entries in summary table and detailed sections

### Key Findings from Search
- MoE and disaggregated architectures driving next-gen simulator development (Frontier)
- LLM-based approaches expanding beyond performance prediction to optimization (SwizzlePerf follows Omniwise trend)
- Hardware-aware optimization becoming automated (SwizzlePerf replaces 2 weeks of expert work with 5 minutes)
- Collective communication at 100k+ GPU scale becoming critical challenge

### For Next Cycle
- MLSys 2026 call for papers is open - watch for accepted papers
- ISCA 2025 had several relevant papers - may need deeper analysis
- Could expand coverage of distributed training performance models
- Frontier simulator may be worth highlighting for survey given MoE trend
