{
  "tool": "VIDUR",
  "version": "latest (GitHub, 2026-02)",
  "evaluation_date": "2026-02-16",
  "evaluation_platform": "GitHub Actions (Ubuntu 22.04, 2-core x86_64, 7 GB RAM, no GPU)",
  "mtap_version": "1.0",
  "composite_score": {
    "formula": "S(t) = sum(w_i * d_i(t))",
    "weights": [0.4, 0.2, 0.2, 0.1, 0.1],
    "dimension_scores": [2, 2, 2, 2, 3],
    "dimension_labels": ["M", "M", "M", "M", "H"],
    "total": 2.1,
    "max_possible": 3.0,
    "percentage": 70.0,
    "note": "S(t) = 0.4*2 + 0.2*2 + 0.2*2 + 0.1*2 + 0.1*3 = 2.1"
  },
  "dimensions": {
    "D1_prediction_fidelity": {
      "weight": 0.4,
      "score": 2,
      "label": "Medium",
      "rationale": "Self-reported <5% error vs real LLM serving traces (validated against vLLM, TensorRT-LLM); independently unverifiable without A100 hardware running real serving systems",
      "metrics": {
        "self_reported_accuracy": {
          "claimed_error": "<5% vs real LLM serving traces",
          "validated_against": ["vLLM", "TensorRT-LLM"],
          "source": "Agrawal et al., MLSys 2024"
        },
        "our_measurements": {
          "vllm_scheduler": {
            "model": "Llama-2-7b-hf",
            "device": "a100",
            "num_requests": 200,
            "qps": 2.0,
            "avg_e2e_s": 0.1767,
            "median_e2e_s": 0.177,
            "p90_e2e_s": 0.2609,
            "p99_e2e_s": 0.3198,
            "avg_ttft_s": 0.0273,
            "avg_tpot_s": 0.009347,
            "requests_preempted": 53
          },
          "sarathi_scheduler": {
            "model": "Llama-2-7b-hf",
            "device": "a100",
            "num_requests": 50,
            "qps": 2.0,
            "avg_e2e_s": 0.1575,
            "median_e2e_s": 0.1563,
            "p90_e2e_s": 0.2357,
            "p99_e2e_s": 0.2697,
            "avg_ttft_s": 0.0247,
            "avg_tpot_s": 0.009034,
            "requests_preempted": 0
          }
        },
        "internal_consistency": {
          "scheduler_ranking_correct": true,
          "preemption_behavior_correct": true,
          "scheduling_delay_near_zero_at_low_qps": true,
          "deterministic_with_fixed_seed": true,
          "e2e_latency_difference_pct": 12.19,
          "ttft_difference_pct": 10.53,
          "tpot_difference_pct": 3.46
        },
        "verification_status": "PLAUSIBLE_UNVERIFIED",
        "verification_blocker": "No A100 GPU access for ground-truth vLLM/TensorRT-LLM latency measurements"
      }
    },
    "D2_compositional_fidelity": {
      "weight": 0.2,
      "score": 2,
      "label": "Medium",
      "rationale": "Composes ML-predicted per-operator execution times with discrete-event scheduling simulation. Pre-profiled GPU execution data avoids kernel prediction but creates hardware dependency.",
      "metrics": {
        "composition_approach": {
          "kernel_prediction": "ML-based execution time predictor trained on GPU profiling data",
          "scheduling_simulation": "Discrete-event simulation of request batching, preemption, and memory management",
          "composition_method": "Per-iteration execution time from predictor + scheduling decisions from DES = end-to-end request latency"
        },
        "profiling_dependency": {
          "required_profiles": ["compute (per-operator execution time)", "network (inter-GPU latency)"],
          "pre_profiled_hardware": ["A100", "H100", "A40"],
          "new_hardware_effort": "Requires running profiling scripts on target GPU hardware"
        },
        "hidden_dependency": "Accuracy depends on pre-profiled GPU execution data; new GPUs require physical hardware access for profiling",
        "composition_gap_gamma": "Cannot be measured — no kernel-level prediction to compare against (uses pre-profiled data)"
      }
    },
    "D3_generalization_robustness": {
      "weight": 0.2,
      "score": 2,
      "label": "Medium",
      "rationale": "Supports 7+ LLM architectures across 3 GPU types, but limited to Llama-family models in practice and requires Python 3.10 (fragile temporal stability).",
      "metrics": {
        "workload_transfer": {
          "supported_models": ["Meta-Llama-3-8B", "Meta-Llama-3-70B", "Llama-2-7b-hf", "Llama-2-70b-hf", "CodeLlama-34b", "internlm-20b", "Qwen-72B"],
          "tested_models": ["Llama-2-7b-hf"],
          "workload_types": ["synthetic", "trace-based (Azure splitwise, arxiv summarization)"],
          "new_model_effort": "Requires GPU profiling (operator-level execution time measurement)"
        },
        "hardware_transfer": {
          "supported_gpus": ["A100 DGX", "H100 DGX", "4xA100 Node", "8xA40 Node"],
          "parallelism": ["TP1", "TP2", "TP4", "TP8", "Pipeline Parallelism"],
          "tested_hardware": ["A100 (simulated)"]
        },
        "temporal_stability": {
          "python_version_sensitive": true,
          "requires_python_310": true,
          "docker_support": false,
          "pip_only": true,
          "scikit_learn_dependency": true,
          "wandb_dependency": true,
          "note": "Python 3.11+ breaks argparse BooleanOptionalAction; no Docker image provided"
        }
      }
    },
    "D4_deployment_viability": {
      "weight": 0.1,
      "score": 2,
      "label": "Medium",
      "rationale": "Fast setup (~5 min pip install) but no Docker support, strict Python 3.10 requirement, and dependency on wandb/scikit-learn create fragility.",
      "metrics": {
        "time_to_first_prediction": {
          "value_minutes": 5,
          "includes_install": true,
          "includes_simulation": true
        },
        "deployment_method": "pip install (Python 3.10 venv)",
        "docker_support": false,
        "deterministic": true,
        "ci_compatible": "Partial (requires Python 3.10 setup in CI)",
        "failure_modes": [
          "Python 3.11+ incompatible (argparse BooleanOptionalAction)",
          "wandb import required (set WANDB_MODE=disabled)"
        ],
        "platform_requirements": "Linux/macOS with Python 3.10",
        "dependency_freshness": "scikit-learn, wandb, kaleido — standard but version-sensitive",
        "documentation_quality": "Excellent — README with full CLI examples, metrics docs"
      }
    },
    "D5_extensibility": {
      "weight": 0.1,
      "score": 3,
      "label": "High",
      "rationale": "Rich extensibility: 5 scheduling algorithms, pluggable request generators, config optimizer for hyperparameter search, and documented profiling pipeline for new models/GPUs.",
      "metrics": {
        "new_model_effort": {
          "method": "Profile operator execution times on target GPU, add to data/profiling/compute/",
          "estimated_effort": "Hours of GPU profiling + config file",
          "documentation": "docs/profiling.md"
        },
        "new_hardware_effort": {
          "method": "Run profiling scripts on new GPU, add network latency profiles",
          "estimated_effort": "Hours (requires physical GPU access)",
          "existing_profiles": ["A100", "H100", "A40"]
        },
        "scheduler_extensibility": {
          "built_in_schedulers": ["vLLM", "Orca", "Sarathi", "LightLLM", "FasterTransformer"],
          "custom_scheduler": "Subclass replica_scheduler base class",
          "documentation": "Source code examples"
        },
        "api_accessibility": {
          "programmatic_api": true,
          "cli_interface": true,
          "config_optimizer": true,
          "chrome_trace_output": true,
          "wandb_integration": true,
          "metrics_output": "20+ request-level and system-level metrics"
        }
      }
    }
  },
  "experimental_evidence": {
    "experiments_run": 2,
    "successful": 2,
    "failed": 0,
    "schedulers_tested": ["vLLM", "Sarathi"],
    "models_tested": ["Llama-2-7b-hf"],
    "hardware_simulated": ["A100"],
    "total_requests_simulated": 250,
    "metrics_collected": ["E2E latency", "TTFT", "TPOT", "preemption count", "scheduling delay", "throughput"]
  },
  "key_findings": [
    "Sarathi achieves 12.19% lower average E2E latency than vLLM (0.1575s vs 0.1767s), consistent with chunked-prefill avoiding preemption overhead",
    "vLLM preempts 53/200 requests (26.5%) while Sarathi preempts 0/50, matching algorithmic design (PagedAttention vs chunked prefill)",
    "TTFT is 10.53% lower with Sarathi (24.7ms vs 27.3ms), suggesting chunked prefill reduces queuing delay at low QPS",
    "TPOT difference is only 3.46% (9.03ms Sarathi vs 9.35ms vLLM), consistent with decode being scheduler-independent once batched",
    "Scheduling delay is near-zero at QPS=2.0 for both schedulers, as expected for low-utilization regime",
    "Published <5% error claim is plausible but unverifiable without A100 GPU running real vLLM/TensorRT-LLM"
  ],
  "limitations": [
    "D1 score based on self-reported accuracy — no independent hardware verification possible",
    "Only Llama-2-7B tested — no cross-architecture validation (BERT, GPT-2, etc.)",
    "Only 2 of 5 schedulers tested (vLLM, Sarathi) — Orca, LightLLM, FasterTransformer untested",
    "No Docker support — pip-only installation with Python 3.10 requirement creates deployment fragility",
    "Synthetic workload only — not tested with real Azure traces (splitwise, arxiv summarization)",
    "No multi-replica or pipeline-parallel configurations tested"
  ]
}
