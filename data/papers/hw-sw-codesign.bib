% Hardware-Software Co-Design for ML Performance Models
% Focus: NAS with hardware-aware predictors, AutoML, accelerator optimization, co-design
% Generated by Maya (Literature Scout) for issue #31
% Coverage: 2020-2025

% ===================================================================
% JOINT NEURAL ARCHITECTURE AND ACCELERATOR SEARCH
% ===================================================================

@inproceedings{naas2021,
  author    = {Lin, Yujun and Yang, Mengtian and Han, Song},
  title     = {NAAS: Neural Accelerator Architecture Search},
  booktitle = {Proceedings of the 58th ACM/IEEE Design Automation Conference (DAC)},
  year      = {2021},
  doi       = {10.1109/DAC18074.2021.9586250},
  url       = {https://arxiv.org/abs/2105.13258},
  note      = {Holistic co-search of NN, accelerator, and compiler mapping. 4.4x EDP reduction vs. Eyeriss. MIT HAN Lab.}
}

@inproceedings{hasco2021,
  author    = {Xiao, Qingcheng and Zheng, Size and Wu, Bingzhe and Xu, Pengcheng and Qian, Xuehai and Liang, Yun},
  title     = {HASCO: Towards Agile HArdware and Software CO-design for Tensor Computation},
  booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2021},
  doi       = {10.1109/ISCA52012.2021.00086},
  url       = {https://arxiv.org/abs/2105.01585},
  note      = {Tensor syntax trees as unified IR. Multi-objective Bayesian optimization. 1.25-1.44x latency reduction.}
}

@article{codebench2023,
  author    = {Odema, Moussa and others},
  title     = {CODEBench: A Neural Architecture and Hardware Accelerator Co-Design Framework},
  journal   = {ACM Transactions on Embedded Computing Systems},
  year      = {2023},
  doi       = {10.1145/3575798},
  url       = {https://arxiv.org/abs/2212.03965},
  note      = {BOSHCODE: surrogate model for CNN-accelerator pairs. 59.1\% lower latency, 60.8\% lower energy. Princeton/Stanford.}
}

@inproceedings{hao2021,
  author    = {Jain, Yash and others},
  title     = {HAO: Hardware-aware Neural Architecture Optimization for Efficient Inference},
  booktitle = {Proceedings of the 29th IEEE International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  year      = {2021},
  doi       = {10.1109/FCCM51124.2021.00019},
  url       = {https://arxiv.org/abs/2104.12766},
  note      = {Integer programming prunes design space for FPGA. 72.5\% ImageNet at 50 FPS, 60-135\% faster than prior work.}
}

% ===================================================================
% ML-BASED DESIGN SPACE EXPLORATION
% ===================================================================

@inproceedings{confuciux2020,
  author    = {Kao, Sheng-Chun and Jeong, Geonhwa and Krishna, Tushar},
  title     = {ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning},
  booktitle = {Proceedings of the 53rd IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2020},
  doi       = {10.1109/MICRO50266.2020.00068},
  url       = {https://arxiv.org/abs/2009.02010},
  note      = {REINFORCE RL for O(10\^72) design space. 4.7-24x faster convergence. Uses MAESTRO cost model.}
}

@inproceedings{gnndse2022,
  author    = {Sohrabizadeh, Atefeh and others},
  title     = {Improving GNN-Based Accelerator Design Automation with Meta Learning},
  booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference (DAC)},
  year      = {2022},
  doi       = {10.1145/3489517.3530629},
  url       = {https://par.nsf.gov/servlets/purl/10350142},
  note      = {GNN as HLS surrogate. MAML meta-learning for new kernel adaptation. UCLA.}
}

@inproceedings{gemmini2021,
  author    = {Genc, Hasan and Kim, Seah and Amid, Alon and Haj-Ali, Ameer and Iber, Abraham and Knez, Nik and Balocco, Stephanie and Shao, Yakun Sophia and Nikolic, Borivoje and Asanovic, Krste},
  title     = {Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration},
  booktitle = {Proceedings of the 58th ACM/IEEE Design Automation Conference (DAC)},
  year      = {2021},
  doi       = {10.1109/DAC18074.2021.9586115},
  url       = {https://arxiv.org/abs/1911.09925},
  note      = {Open-source systolic array generator. Up to 2,670x speedup over CPU. UC Berkeley.}
}

% ===================================================================
% ANALYTICAL COST MODELS FOR ACCELERATORS
% ===================================================================

@inproceedings{timeloop2019,
  author    = {Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Muber, Anurag and Venkataramani, Swagath and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},
  title     = {Timeloop: A Systematic Approach to DNN Accelerator Evaluation},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2019},
  doi       = {10.1109/ISPASS.2019.00042},
  url       = {https://accelergy.mit.edu/timeloop.pdf},
  note      = {Unified architecture representation. Automatic mapspace construction. Deterministic throughput/energy model. NVIDIA/MIT.}
}

@inproceedings{maestro2019,
  author    = {Kwon, Hyoukjun and Chatarasi, Prasanth and Sarber, Michael and Krishna, Tushar and Pellauer, Michael and Parashar, Angshuman},
  title     = {Understanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A Data-Centric Approach Using MAESTRO},
  booktitle = {Proceedings of the 52nd IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2019},
  doi       = {10.1145/3352460.3358252},
  url       = {https://arxiv.org/abs/1805.02566},
  note      = {Data-centric dataflow analysis. 20+ output metrics. Used by ConfuciuX. MICRO Top Picks 2020. Georgia Tech.}
}

@inproceedings{autodnnchip2020,
  author    = {Xu, Pengfei and others},
  title     = {AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs and ASICs},
  booktitle = {Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)},
  year      = {2020},
  doi       = {10.1145/3373087.3375306},
  url       = {https://arxiv.org/abs/2001.03535},
  note      = {Chip Predictor (<10\% error) + Chip Builder. Two-stage DSE. Up to 3.86x improvement. Rice/UIUC.}
}

% ===================================================================
% TINY ML AND EDGE DEPLOYMENT
% ===================================================================

@inproceedings{mcunetv2-2021,
  author    = {Lin, Ji and Chen, Wei-Ming and Cai, Han and Gan, Chuang and Han, Song},
  title     = {MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.15352},
  note      = {Patch-by-patch scheduling. 4-8x memory reduction. 71.8\% ImageNet under 32kB SRAM. MIT HAN Lab.}
}

@inproceedings{hadas2023,
  author    = {Siddiqui, Shoaib and others},
  title     = {HADAS: Hardware-Aware Dynamic Neural Architecture Search for Edge Performance Scaling},
  booktitle = {Proceedings of the Design, Automation and Test in Europe Conference (DATE)},
  year      = {2023},
  url       = {https://arxiv.org/abs/2212.03354},
  note      = {Joint backbone + early exits + DVFS optimization. Up to 57\% energy gains. UC Irvine.}
}

% ===================================================================
% TRANSFORMER AND ATTENTION ACCELERATORS
% ===================================================================

@article{energon2022,
  author    = {Zhou, Xingyu and Liu, Fangxin and others},
  title     = {Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention},
  journal   = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year      = {2022},
  doi       = {10.1109/TCAD.2022.3164320},
  url       = {https://arxiv.org/abs/2110.09310},
  note      = {Mix-Precision Multi-Round Filtering. 168x speedup vs. CPU, 10\^4x energy reduction. Filtering + Attention units.}
}

% ===================================================================
% ENERGY-AWARE TRAINING AND INFERENCE
% ===================================================================

@inproceedings{zeus2023,
  author    = {You, Jie and Chung, Jae-Won and Chowdhury, Mosharaf},
  title     = {Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training},
  booktitle = {Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
  year      = {2023},
  url       = {https://www.usenix.org/conference/nsdi23/presentation/you},
  note      = {Automatic batch size + power limit optimization. 15.3-75.8\% energy reduction. PyTorch integration.}
}

% ===================================================================
% HARDWARE-AWARE LATENCY PREDICTION
% ===================================================================

@inproceedings{latencypredictors2024,
  author    = {Laube, Kevin and others},
  title     = {On Latency Predictors for Neural Architecture Search},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  url       = {https://proceedings.mlsys.org/paper_files/paper/2024/file/f03cb785864596fa5901f1359d23fd81-Paper-Conference.pdf},
  note      = {End-to-end predictor training strategy. 22.5\% avg improvement. 5.8x NAS speedup. Automated device set design.}
}

