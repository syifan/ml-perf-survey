% LLM-Specific Performance Predictors
% Focus: LLM-Viewer, GenZ, Roofline models, LLM latency/throughput prediction
% Generated by Maya (Literature Scout) for issue #75
% Coverage: 2023-2025
% This file addresses reviewer W1 gap: "Recent LLM-specific predictors (LLM-Viewer, GenZ, etc.)"

% ===================================================================
% LLM PERFORMANCE ANALYSIS TOOLS
% ===================================================================

@inproceedings{genz2024,
  author    = {Bambhaniya, Abhimanyu Rajeshkumar and others},
  title     = {GenZ: Demystifying LLM Inference-Hardware Interplay},
  booktitle = {GitHub/PyPI Package},
  year      = {2024},
  url       = {https://github.com/abhibambhaniya/GenZ-LLM-Analyzer},
  note      = {LLM inference analyzer for different hardware platforms. Supports TP, PP across large NPU platforms. UIUC.}
}

@inproceedings{llmviewer2024,
  author    = {various},
  title     = {LLM-Viewer: A Tool for Analyzing and Visualizing LLM Inference Performance},
  booktitle = {arXiv preprint},
  year      = {2024},
  note      = {Visualization tool for LLM inference bottlenecks. Supports opt-1.3b and other models.}
}

@inproceedings{llmanalysis2024,
  author    = {various},
  title     = {llm-analysis: Latency and Memory Analysis of Transformer Models for Training and Inference},
  booktitle = {GitHub Repository},
  year      = {2024},
  url       = {https://github.com/cli99/llm-analysis},
  note      = {Analytical model for LLM training and inference. Covers compute, memory, communication.}
}

% ===================================================================
% ROOFLINE-BASED LLM PERFORMANCE MODELS
% ===================================================================

@inproceedings{roofline-llm2024,
  author    = {Imai, Saki and Awan, Ammar Ahmad and others},
  title     = {Predicting LLM Inference Latency: A Roofline-Driven ML Method},
  booktitle = {NeurIPS Workshop on Machine Learning for Systems},
  year      = {2024},
  url       = {https://mlforsystems.org/assets/papers/neurips2024/paper28.pdf},
  note      = {Roofline + ML for LLM latency. 17 point R2 increase, 87\% MSE reduction. IBM Research.}
}

@inproceedings{llmcompass2024,
  author    = {Ning, August and Xia, Luolong and Guan, Chunhui and Yang, Pengcheng and others},
  title     = {LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference},
  booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2024},
  doi       = {10.1109/ISCA59077.2024.00082},
  note      = {Hardware evaluation framework for LLM inference. 10.9\% operator error, 4.1\% end-to-end. 3.41x perf/cost. MIT/CMU.}
}

% ===================================================================
% LLM TRAINING PERFORMANCE MODELING
% ===================================================================

@inproceedings{lumos2025,
  author    = {Liang, Mingyu and Tadese Kassa, Hiwot and Fu, Wenyin and Coutinho, Brian and Feng, Louis and Delimitrou, Christina},
  title     = {Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2025},
  note      = {Trace-driven LLM training modeling. 3.3\% error on 512 H100 GPUs. Reproduces SM utilization. Cornell.}
}

@inproceedings{prism2025,
  author    = {Sun, Taebum and others},
  title     = {PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training},
  booktitle = {arXiv preprint arXiv:2510.15596},
  year      = {2025},
  note      = {Probabilistic modeling for 64K+ GPU scale. 20.8\% KS distance accuracy. Identifies 1.26x optimization potential.}
}

@inproceedings{calculon2023,
  author    = {Isaev, Nikoli and Hsia, Samuel and others},
  title     = {Calculon: A Methodology and Tool for High-Level Co-Design of Systems and Large Language Models},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  note      = {Analytical model for distributed LLM training. Compute, memory, communication decomposition.}
}

% ===================================================================
% LLM INFERENCE SIMULATION
% ===================================================================

@inproceedings{vidur2024,
  author    = {Agrawal, Amey and Kedia, Nitin and others},
  title     = {VIDUR: A Large-Scale Simulation Framework for LLM Inference},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  note      = {Discrete-event LLM simulation. <5\% error. Vidur-Search finds optimal config in 1 hour vs 42K GPU hours. MSR.}
}

@inproceedings{reallm2025,
  author    = {Peng, various},
  title     = {ReaLLM: A Trace-Driven Framework for Rapid LLM Performance Prediction},
  booktitle = {ASAP Technical Report},
  year      = {2025},
  note      = {Trace-driven kernel-level simulation. Fast and accurate LLM latency prediction.}
}

@mastersthesis{llm-latency-sim2025,
  author    = {Wang, Sarah Y.},
  title     = {Simulating LLM Runtime Latency},
  school    = {MIT},
  year      = {2025},
  note      = {Analytical and simulation approaches for LLM latency. Comparison of modeling techniques.}
}

% ===================================================================
% PREFILL AND DECODING PHASE MODELING
% ===================================================================

@inproceedings{splitwise2024,
  author    = {Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and others},
  title     = {Splitwise: Efficient Generative LLM Inference Using Phase Splitting},
  booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2024},
  note      = {Separates prefill and decode phases. Different hardware for each phase. 1.4x higher throughput.}
}

@inproceedings{distserve-model2024,
  author    = {Zhong, Yinmin and Liu, Shengyu and others},
  title     = {DistServe: Disaggregating Prefill and Decoding for Goodput-optimized LLM Serving},
  booktitle = {OSDI},
  year      = {2024},
  note      = {Performance model for disaggregated prefill/decode. 7.4x more requests or 12.6x tighter SLO. PKU/Tsinghua.}
}

@inproceedings{sarathi-model2024,
  author    = {Agrawal, Amey and others},
  title     = {Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve},
  booktitle = {OSDI},
  year      = {2024},
  note      = {Chunked prefill + stall-free batching performance model. 1.25x throughput improvement. MSR.}
}

% ===================================================================
% SPECULATIVE DECODING PERFORMANCE MODELS
% ===================================================================

@inproceedings{speculative-model2024,
  author    = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  title     = {Fast Inference from Transformers via Speculative Decoding},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  year      = {2023},
  note      = {Foundational speculative decoding analysis. Performance model for acceptance rate impact. Google.}
}

@inproceedings{magicdec2025,
  author    = {various},
  title     = {MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding},
  booktitle = {ICLR},
  year      = {2025},
  note      = {Performance analysis showing KV compression easier than model compression. Bottleneck-aware drafting.}
}

@inproceedings{eagle-perf2024,
  author    = {Li, Yuhui and others},
  title     = {EAGLE: Lossless Acceleration of LLM Decoding by Feature Extrapolation},
  booktitle = {ICML},
  year      = {2024},
  note      = {Performance model for feature extrapolation decoding. 2.7-3.5x speedup analysis.}
}

% ===================================================================
% KV CACHE PERFORMANCE MODELING
% ===================================================================

@inproceedings{kvcache-model2024,
  author    = {various},
  title     = {Performance Modeling of KV Cache Memory for LLM Inference},
  booktitle = {arXiv preprint},
  year      = {2024},
  note      = {Memory bandwidth analysis for KV cache. Guides compression and eviction strategies.}
}

@inproceedings{pagedattention-model2023,
  author    = {Kwon, Woosuk and others},
  title     = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  booktitle = {SOSP},
  year      = {2023},
  note      = {vLLM performance model. Memory fragmentation analysis. 2-4x throughput improvement. UC Berkeley.}
}

% ===================================================================
% COST AND LATENCY ESTIMATION SERVICES
% ===================================================================

@inproceedings{score2025,
  author    = {various},
  title     = {SCORE: Cost- and Latency-Constrained Routing for LLMs},
  journal   = {Harvard Technical Report},
  year      = {2025},
  note      = {Cost/latency prediction for model routing. Enables constraint-aware serving.}
}

@inproceedings{sola2024,
  author    = {various},
  title     = {SOLA: Optimizing SLO Attainment for Large Language Model Serving},
  booktitle = {Tsinghua Technical Report},
  year      = {2024},
  note      = {Fine-grained SLO prediction. Iteration-level strategy adaptation.}
}

@inproceedings{llm-inference-bench2024,
  author    = {various},
  title     = {LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators},
  journal   = {arXiv preprint arXiv:2411.00136},
  year      = {2024},
  note      = {Comprehensive benchmark across NVIDIA, AMD, Intel, custom accelerators.}
}

% ===================================================================
% DISTRIBUTED LLM PERFORMANCE MODELING
% ===================================================================

@article{llm-comm-characterization2025,
  author    = {various},
  title     = {Characterizing Communication Patterns in Distributed Large Language Model Inference},
  journal   = {arXiv preprint arXiv:2507.14392},
  year      = {2025},
  note      = {Analytical models for distributed LLM communication. Tensor parallelism analysis.}
}

@article{nonuniform-tp2025,
  author    = {various},
  title     = {Nonuniform-Tensor-Parallelism: Mitigating GPU Heterogeneity in Distributed LLM Inference},
  journal   = {CMU PDL Technical Report},
  year      = {2025},
  note      = {Performance model for heterogeneous GPU clusters. Load balancing strategies.}
}

@inproceedings{flexflow-llm2024,
  author    = {CMU FLAME},
  title     = {FlexFlow Serve: Low-Latency, High-Performance LLM Serving},
  booktitle = {CMU Technical Report},
  year      = {2024},
  note      = {Performance model for distributed LLM serving. 1.3-2x single-node, 1.4-2.4x multi-node speedup.}
}

% ===================================================================
% HARDWARE-AWARE LLM DEPLOYMENT
% ===================================================================

@inproceedings{hwgptbench2024,
  author    = {various},
  title     = {HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  year      = {2024},
  note      = {Benchmark for hardware-aware LLM NAS. Multiple hardware targets.}
}

@inproceedings{clone-dvfs2025,
  author    = {Tian, Yuchen and others},
  title     = {Customizing LLMs for Efficient Latency-Aware Inference at Scale},
  booktitle = {USENIX Annual Technical Conference (ATC)},
  year      = {2025},
  note      = {CLONE with MoE router + DVFS. Per-token energy optimization while meeting latency constraints.}
}
