% Hybrid and Transfer Learning Approaches for Performance Modeling
% Focus: Hybrid analytical+ML, transfer learning, few-shot, domain adaptation
% Generated by Maya (Literature Scout) for issue #16
% Coverage: 2021-2025

% ===================================================================
% HYBRID ANALYTICAL + ML MODELS
% ===================================================================

@article{synperf2025,
  author    = {various},
  title     = {SynPerf: A Hybrid Analytical-ML Framework for GPU Performance Prediction},
  journal   = {arXiv preprint arXiv:2601.14910},
  year      = {2025},
  url       = {https://arxiv.org/abs/2601.14910},
  note      = {Combines analytical pipeline demand modeling with MLP. 6.1\% kernel error, 8.5\% end-to-end. Guides Triton kernel optimization (1.7x speedup).}
}

@inproceedings{apollo2022,
  author    = {Zhao, Jie and others},
  title     = {Apollo: Automatic Partition-based Operator Fusion through Layer by Layer Optimization},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2022},
  url       = {https://proceedings.mlsys.org/paper_files/paper/2022/file/e175e8a86d28d935be4f43719651f86d-Paper.pdf},
  note      = {JIT fusion framework combining graph-level and operator-level optimization. 1.86x over TensorFlow, 1.37x over XLA.}
}

@inproceedings{metatune2021,
  author    = {Ahn, Sunghyun and others},
  title     = {MetaTune: Meta-Learning Based Cost Model for Fast and Efficient Auto-tuning Frameworks},
  booktitle = {arXiv preprint arXiv:2102.04199},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.04199},
  note      = {Meta-learning cost model for tensor program autotuning. Predicts optimization parameters for unseen convolutions.}
}

@inproceedings{ftuner2024,
  author    = {various},
  title     = {FTuner: A Fast Dynamic Shape Tensors Program Auto-Tuner for Deep Learning Compilers},
  booktitle = {arXiv preprint arXiv:2407.21418},
  year      = {2024},
  url       = {https://arxiv.org/abs/2407.21418},
  note      = {Uses uKernel abstraction for dynamic shape tensor programs. Avoids large design space and expensive cost model training.}
}

% ===================================================================
% META-LEARNING FOR HARDWARE PREDICTION
% ===================================================================

@inproceedings{help2021,
  author    = {Lee, Hayeon and Lee, Sewoong and Chung, Jongchan and Hwang, Sung Ju},
  title     = {HELP: Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/e3251075554389fe91d17a794861d47b-Abstract.html},
  note      = {NeurIPS Spotlight. Meta-learns hardware embeddings. 10-sample adaptation to unseen devices. Open source.}
}

@inproceedings{multipredict2023,
  author    = {Akhauri, Yash and Abdelfattah, Mohamed S.},
  title     = {Multi-Predict: Few Shot Predictors For Efficient Neural Architecture Search},
  booktitle = {Proceedings of the 2nd International Workshop on Automated Machine Learning (AutoML)},
  year      = {2023},
  url       = {https://proceedings.mlr.press/v224/akhauri23a.html},
  note      = {Multi-task, multi-search-space, multi-HW adaptation. Search-space independent encoding via zero-cost proxies.}
}

@inproceedings{archgym2023,
  author    = {Krishnan, Srivatsan and others},
  title     = {ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
  booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2023},
  url       = {https://arxiv.org/abs/2306.08888},
  note      = {ML-aided architecture DSE framework. Discovers hyperparameter lottery. Proxy model with 0.61\% RMSE, 2000x faster.}
}

% ===================================================================
% TRANSFER LEARNING FOR CROSS-PLATFORM PREDICTION
% ===================================================================

@inproceedings{nnmeter2021,
  author    = {Zhang, Li Lyna and Han, Shihao and Wei, Jianyu and Zheng, Ningxin and Cao, Ting and Yang, Yuqing and Liu, Yunxin},
  title     = {nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices},
  booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)},
  year      = {2021},
  doi       = {10.1145/3458864.3467882},
  note      = {Kernel-level latency prediction. Adaptive sampling. Supports mobile CPU, GPU, VPU. Microsoft open source.}
}

@inproceedings{tenset2021,
  author    = {Zheng, Lianmin and Liu, Ruochen and others},
  title     = {TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS)},
  year      = {2021},
  url       = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/a684eceee76fc522773286a895bc8436-Paper-round1.pdf},
  note      = {52M program records across Intel/AMD/ARM CPU and NVIDIA GPU. Enables 10x speedup with pre-trained cost models.}
}

@article{cnnlatency-heterogeneous2024,
  author    = {Dudziak, {\L}ukasz and others},
  title     = {Inference Latency Prediction for CNNs on Heterogeneous Mobile Devices and ML Frameworks},
  journal   = {Performance Evaluation},
  volume    = {165},
  year      = {2024},
  doi       = {10.1016/j.peva.2024.102429},
  note      = {Operation-wise framework for cross-device prediction. Addresses hardware heterogeneity and framework diversity.}
}

@article{crossfeature-transfer2024,
  author    = {various},
  title     = {Cross-Feature Transfer Learning for Efficient Tensor Program Generation},
  journal   = {Applied Sciences},
  volume    = {14},
  number    = {2},
  pages     = {513},
  year      = {2024},
  doi       = {10.3390/app14020513},
  note      = {Learns joint NN-hardware feature space. GNN-based latency querying with multi-headed prediction.}
}

% ===================================================================
% FEW-SHOT LEARNING FOR NEW HARDWARE
% ===================================================================

@inproceedings{dcp-fewshot2024,
  author    = {various},
  title     = {DCP: Learning Accelerator Dataflow for Neural Networks via Propagation},
  booktitle = {Electronics},
  volume    = {14},
  number    = {15},
  year      = {2024},
  doi       = {10.3390/electronics14153085},
  note      = {Zero-shot and few-shot dataflow optimization. Fine-tunes neural predictor on unseen hardware configurations.}
}

@inproceedings{brpnas2020,
  author    = {Chau, Thomas and others},
  title     = {BRP-NAS: Prediction-based NAS using GCNs},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/768e78024aa8fdb9b8fe87be86f64745-Abstract.html},
  note      = {GCN-based performance predictor. Released LatBench dataset with latencies across desktop, embedded, mobile.}
}

@inproceedings{graf2024,
  author    = {Kadlecova, Gabriela and Lukasik, Jovita and Pilat, Martin and Vidnerova, Petra and Safari, Mahmoud and Neruda, Roman and Hutter, Frank},
  title     = {Surprisingly Strong Performance Prediction with Neural Graph Features},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year      = {2024},
  url       = {https://proceedings.mlr.press/v235/kadlecova24a.html},
  note      = {GRAF: Simple graph features outperform zero-cost proxies. Fast and interpretable NAS performance prediction.}
}

% ===================================================================
% GNN-BASED DESIGN SPACE EXPLORATION
% ===================================================================

@inproceedings{approxpilot2024,
  author    = {various},
  title     = {ApproxPilot: A GNN-based Accelerator Approximation Framework},
  booktitle = {arXiv preprint arXiv:2407.11324},
  year      = {2024},
  url       = {https://arxiv.org/abs/2407.11324},
  note      = {GNN captures physical connections for PPA and accuracy prediction. Outperforms SOTA with same accuracy constraints.}
}

@inproceedings{approxgnn2024,
  author    = {various},
  title     = {ApproxGNN: A Pretrained GNN for Parameter Prediction in Design Space Exploration for Approximate Computing},
  booktitle = {arXiv preprint arXiv:2507.16379},
  year      = {2024},
  url       = {https://arxiv.org/abs/2507.16379},
  note      = {Pretrained GNN for approximate accelerator DSE. 50\% better embedding accuracy, 30-54\% over statistical ML.}
}

@inproceedings{gnnbuilder2023,
  author    = {various},
  title     = {GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization},
  booktitle = {arXiv preprint arXiv:2303.16459},
  year      = {2023},
  url       = {https://arxiv.org/abs/2303.16459},
  note      = {End-to-end GNN accelerator framework with accurate performance models for DSE.}
}

@inproceedings{meta-dse-gnn2022,
  author    = {various},
  title     = {Improving GNN-based Accelerator Design Automation with Meta Learning},
  booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference (DAC)},
  year      = {2022},
  doi       = {10.1145/3489517.3530629},
  note      = {Meta-learning improves GNN-based accelerator design automation.}
}

% ===================================================================
% MASE AND MIXED-PRECISION FRAMEWORKS
% ===================================================================

@article{mase2024,
  author    = {various},
  title     = {An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators},
  journal   = {ACM Transactions on Design Automation of Electronic Systems},
  year      = {2024},
  doi       = {10.1145/3664652},
  note      = {MASE: Quantization search, QAT, FPGA generation. 24\% accuracy gain at 3\% energy overhead with 4-bit MX.}
}

@inproceedings{autoscaledse2024,
  author    = {various},
  title     = {AutoScaleDSE: A Scalable Design Space Exploration Engine for High-Level Synthesis},
  booktitle = {NSF Public Access Repository},
  year      = {2024},
  url       = {https://par.nsf.gov/biblio/10477702},
  note      = {Transfer learning for HLS DSE. Mixed-sharing multi-domain model outperforms single-domain approaches.}
}

% ===================================================================
% DISTRIBUTED TRAINING PERFORMANCE MODELS
% ===================================================================

@inproceedings{flexflow2022,
  author    = {Jia, Zhihao and others},
  title     = {Beyond Data and Model Parallelism for Deep Neural Networks},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2022},
  note      = {FlexFlow: Execution simulator for parallelization strategies. SOAP dimensions (Sample, Operator, Attribute, Parameter).}
}

@inproceedings{flexflowserve2024,
  author    = {various},
  title     = {FlexFlow Serve: Low-Latency, High-Performance LLM Serving},
  booktitle = {CMU FLAME Center},
  year      = {2024},
  url       = {https://www.cmu.edu/flame/research/2024/flexflow-serve-low-latency-high-performance-llm-serving.html},
  note      = {Distributed LLM serving. 1.3-2.0x single-node, 1.4-2.4x multi-node speedup over existing systems.}
}

