% Bayesian Optimization and Active Learning for Design Space Exploration
% Focus: BO for accelerator DSE, active learning sampling, multi-objective optimization
% Generated by Maya (Literature Scout) for issue #75
% Coverage: 2017-2025
% This file addresses reviewer W1 gap: "Bayesian optimization/active learning for DSE"

% ===================================================================
% FOUNDATIONAL BAYESIAN OPTIMIZATION FOR ACCELERATORS
% ===================================================================

@inproceedings{reagen-bayesopt2017,
  author    = {Reagen, Brandon and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adolf, Robert and Gelbart, Michael and Whatmough, Paul and Wei, Gu-Yeon and Brooks, David},
  title     = {A Case for Efficient Accelerator Design Space Exploration via Bayesian Optimization},
  booktitle = {ACM/IEEE International Symposium on Low Power Electronics and Design (ISLPED)},
  year      = {2017},
  doi       = {10.1109/ISLPED.2017.8009208},
  note      = {Foundational work. Multi-objective BO for DNN accelerators. Outperforms grid/random search. Harvard.}
}

@inproceedings{automl-codesign2021,
  author    = {Abdelfattah, Mohamed S. and Dudziak, {\L}ukasz and Chau, Thomas and Lee, Royson and Kim, Hyeji and Lane, Nicholas D.},
  title     = {Best of Both Worlds: AutoML Codesign of a CNN and its Hardware Accelerator},
  booktitle = {ACM/IEEE Design Automation Conference (DAC)},
  year      = {2021},
  note      = {Joint CNN + accelerator search with BO. Pareto-optimal designs. Samsung/Cambridge.}
}

% ===================================================================
% RECENT BAYESIAN AND ACTIVE LEARNING DSE
% ===================================================================

@inproceedings{arsflow2024,
  author    = {various},
  title     = {ARS-Flow 2.0: An Enhanced Design Space Exploration Flow for Accelerator-Rich Systems Based on Active Learning},
  journal   = {Integration, the VLSI Journal},
  year      = {2024},
  doi       = {10.1016/j.vlsi.2024.102299},
  note      = {PSO-optimized GP regression + self-adaptive MOGA + Pareto-oriented resampling. State-of-the-art DSE.}
}

@inproceedings{polaris2024,
  author    = {various},
  title     = {Polaris: Multi-Fidelity Design Space Exploration of Deep Learning Accelerators},
  booktitle = {arXiv preprint arXiv:2412.15548},
  year      = {2024},
  note      = {Transfer learning between low/high fidelity evaluations. Matches DOSA performance in 35 min vs 6 hours. 2.7x EDP reduction.}
}

@inproceedings{compass2024,
  author    = {various},
  title     = {Compass: A Collaborative HLS Design Space Exploration Framework via Graph Representation Learning and Ensemble Bayesian Optimization},
  booktitle = {arXiv preprint},
  year      = {2024},
  note      = {Graph learning + ensemble BO for HLS DSE. Collaborative multi-stage exploration.}
}

@inproceedings{autohls2024,
  author    = {various},
  title     = {AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs},
  booktitle = {arXiv preprint arXiv:2403.10686},
  year      = {2024},
  note      = {Learns with limited samples. Essential for DSE with long HLS synthesis times.}
}

@inproceedings{csdse2025,
  author    = {various},
  title     = {CSDSE: An Efficient Design Space Exploration Method for Deep Neural Network Accelerators via Cooperative Search},
  journal   = {Neurocomputing},
  year      = {2025},
  doi       = {10.1016/j.neucom.2025.129412},
  note      = {Heterogeneous agents for exploration/exploitation. Adaptive agent organization. Multi-scale search.}
}

% ===================================================================
% MULTI-OBJECTIVE BAYESIAN OPTIMIZATION
% ===================================================================

@inproceedings{mobo-hls2023,
  author    = {various},
  title     = {Multi-objective Design Space Exploration for High-Level Synthesis via Bayesian Optimization},
  booktitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year      = {2023},
  note      = {Pareto-aware acquisition functions. Handles area, latency, power simultaneously.}
}

@inproceedings{hwnas-bo2024,
  author    = {various},
  title     = {Hardware-Aware Bayesian Neural Architecture Search of Quantized CNNs},
  journal   = {IEEE Embedded Systems Letters},
  year      = {2024},
  doi       = {10.1109/LES.2024.3434379},
  note      = {BO for quantized CNN hardware search. Accuracy + efficiency trade-off.}
}

@inproceedings{accelerated-nas-bo2024,
  author    = {Klein, Aaron and others},
  title     = {Accelerated NAS via Pretrained Ensembles and Multi-fidelity Bayesian Optimization},
  booktitle = {Lecture Notes in Computer Science},
  year      = {2024},
  doi       = {10.1007/978-3-031-72332-2_17},
  note      = {Multi-fidelity BO with pretrained ensembles. Reduces NAS cost significantly.}
}

% ===================================================================
% ACTIVE LEARNING FOR HARDWARE DESIGN
% ===================================================================

@inproceedings{al-accelerator2020,
  author    = {various},
  title     = {Active Learning for Efficient Design Space Exploration of DNN Accelerators},
  booktitle = {IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  year      = {2020},
  note      = {Sample-efficient accelerator exploration. Uncertainty-driven sampling.}
}

@inproceedings{quickloop2024,
  author    = {various},
  title     = {FPGA-assisted Design Space Exploration of Parameterized AI Accelerators: A Quickloop Approach},
  journal   = {Journal of Systems Architecture},
  year      = {2024},
  doi       = {10.1016/j.sysarc.2024.103122},
  note      = {FPGA-in-the-loop DSE. Fast hardware evaluation for active learning.}
}

@inproceedings{memory-centric-dse2024,
  author    = {various},
  title     = {Design Space Exploration for Scalable DNN Accelerators Using a Memory-Centric Analytical Model for HW/SW Co-Design},
  journal   = {ACM Transactions on Design Automation of Electronic Systems},
  year      = {2025},
  doi       = {10.1145/3729227},
  note      = {Memory-centric cost model for accelerator DSE. Enables efficient exploration.}
}

% ===================================================================
% GNN-BASED SURROGATE MODELS FOR DSE
% ===================================================================

@inproceedings{gnndse2022,
  author    = {Sohrabizadeh, Atefeh and others},
  title     = {Improving GNN-Based Accelerator Design Automation with Meta Learning},
  booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference (DAC)},
  year      = {2022},
  doi       = {10.1145/3489517.3530629},
  note      = {GNN surrogate for HLS. MAML meta-learning for kernel adaptation. UCLA.}
}

@inproceedings{approxgnn-dse2024,
  author    = {various},
  title     = {ApproxGNN: A Pretrained GNN for Parameter Prediction in Design Space Exploration for Approximate Computing},
  booktitle = {arXiv preprint arXiv:2507.16379},
  year      = {2024},
  note      = {Pretrained GNN for approximate accelerator DSE. 50\% better embedding accuracy.}
}

@inproceedings{ironman2020,
  author    = {Xiao, Qingcheng and others},
  title     = {IronMan: GNN-assisted Design Space Exploration in High-Level Synthesis via Reinforcement Learning},
  booktitle = {ACM Great Lakes Symposium on VLSI},
  year      = {2020},
  note      = {RL + GNN for HLS DSE. Learns design patterns from existing designs.}
}

% ===================================================================
% TRANSFER LEARNING FOR DSE
% ===================================================================

@inproceedings{autoscaledse2024,
  author    = {various},
  title     = {AutoScaleDSE: A Scalable Design Space Exploration Engine for High-Level Synthesis},
  booktitle = {NSF Public Access Repository},
  year      = {2024},
  note      = {Transfer learning for HLS DSE. Mixed-sharing multi-domain model. Stanford/UIUC.}
}

@inproceedings{transfer-dse-fpga2023,
  author    = {various},
  title     = {Transfer Learning for Design Space Exploration of FPGA Accelerators},
  booktitle = {ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)},
  year      = {2023},
  note      = {Cross-platform DSE transfer. Reduces exploration cost for new FPGAs.}
}

% ===================================================================
% GAUSSIAN PROCESS MODELS FOR PERFORMANCE
% ===================================================================

@inproceedings{gp-cost-model2019,
  author    = {various},
  title     = {Gaussian Process Cost Models for DNN Accelerator Design},
  booktitle = {IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2019},
  note      = {GP-based uncertainty-aware cost modeling. Enables efficient exploration.}
}

@inproceedings{bo-compiler2022,
  author    = {various},
  title     = {Bayesian Optimization for Compiler Autotuning},
  booktitle = {ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  year      = {2022},
  note      = {BO for compiler flag optimization. Handles high-dimensional spaces.}
}

% ===================================================================
% REINFORCEMENT LEARNING FOR DSE
% ===================================================================

@inproceedings{confuciux2020,
  author    = {Kao, Sheng-Chun and Jeong, Geonhwa and Krishna, Tushar},
  title     = {ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning},
  booktitle = {Proceedings of the 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2020},
  doi       = {10.1109/MICRO50266.2020.00068},
  note      = {REINFORCE for O(10^72) design space. 4.7-24x faster convergence. Uses MAESTRO cost model.}
}

@inproceedings{archgym2023,
  author    = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and others},
  title     = {ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
  booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2023},
  doi       = {10.1145/3579371.3589049},
  note      = {OpenAI Gym interface for architecture DSE. Compares RL, BO, GA, random. Google/Stanford.}
}

@inproceedings{apollo-dse2024,
  author    = {various},
  title     = {Apollo: Learning to Explore Efficiently in Large-Scale Accelerator Design Spaces},
  booktitle = {arXiv preprint},
  year      = {2024},
  note      = {Learned exploration policy for DSE. Adapts to design space structure.}
}
