% MICRO 2026 Survey Paper - ML Performance Models
% Using MICRO 59 ACM sigconf template
% Last compiled: 2026-02-07 (rebuild triggered)

%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
\documentclass[sigconf, screen, review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information - for submission
\setcopyright{none}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{}

%% Conference information
\acmConference[MICRO 2026]{The 59th IEEE/ACM International Symposium on Microarchitecture}{November 2026}{Austin, TX, USA}
\acmISBN{}

%% Disable ACM reference format printing for submission
\settopmatter{printfolios=true}
\settopmatter{printacmref=false}

%% Anonymous submission
\author{Anonymous Author(s)}
\affiliation{%
  \institution{Under Review}
  \country{Anonymous}
}

%% Additional packages (acmart already loads amsmath, amsfonts, amssymb, booktabs)
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows.meta,positioning,fit,backgrounds,patterns,decorations.pathreplacing}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{plotmarks}

% Custom commands
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\begin{document}

\title{A Survey of High-Level Modeling and Simulation Methods for Modern Machine Learning Workloads}
\subtitle{\normalsize{MICRO 2026 Submission -- Confidential Draft -- Do NOT Distribute!!}}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

%%%%%% -- PAPER CONTENT STARTS-- %%%%%%%%

\begin{abstract}
We survey 22 performance modeling tools from 53 papers (2016--2026) and independently evaluate five---NeuSight, ASTRA-sim, VIDUR, Timeloop, nn-Meter---through accuracy-centered experiments spanning 146 GPU configurations, collective benchmarks, LLM serving simulations, energy validation, and reproducibility testing.
Three findings emerge.
First, self-reported accuracy is unreliable: NeuSight claims 2.3\% MAPE but we measure 5.87--27.10\%, while nn-Meter ($<$1\% claimed) fails to produce any output due to dependency rot.
Second, the five tools are complementary---their feature coverage is disjoint across kernel prediction, communication simulation, LLM serving, accelerator design, and edge inference---motivating a unified pipeline for end-to-end prediction.
Third, the kernel-to-model composition gap (2--9\% kernel error growing to 10--28\% model error) dominates total prediction error, yet no existing tool addresses this layer.
\end{abstract}

%%
%% Keywords
\keywords{ML workload performance prediction, DNN accelerator modeling, GPU simulation, distributed training simulation, LLM inference serving, design space exploration, survey}

\maketitle

% ==============================================================================
% INTRODUCTION
% ==============================================================================
\section{Introduction}
\label{sec:introduction}

Machine learning workloads dominate compute across datacenters and edge devices, demanding hardware from Google's TPU~\cite{tpuv1_2017,tpuv4_2023} to custom accelerators.
The shift toward domain-specific architectures~\cite{hennessy2019golden} makes performance prediction critical for design space exploration, parallelization selection, and hardware-software co-design---yet ML workloads pose unique challenges: diverse computational patterns across GPUs, TPUs, custom accelerators, and multi-device clusters.

A rich ecosystem has emerged---analytical models (Timeloop~\cite{timeloop2019}, MAESTRO~\cite{maestro2019}), trace-driven simulators (ASTRA-sim~\cite{astrasim2023}, VIDUR~\cite{vidur2024}), and hybrid approaches (NeuSight~\cite{neusight2025})---yet no prior work examines \emph{why} certain approaches succeed on certain platforms, or how errors propagate across the abstraction stack.
Existing surveys focus on ML \emph{techniques} for modeling~\cite{granite2022} or specific hardware~\cite{timeloop2019}; we identify cross-cutting principles that explain when and why different approaches work.

Our central contribution is an \textbf{accuracy-centered independent verification framework} paired with an \textbf{LLM-focused benchmark suite}, replacing the field's reliance on self-reported accuracy with reproducible evaluation that reveals claimed error rates are overstated by $2$--$4\times$.
We make four contributions:
\begin{itemize}
    \item A \textbf{28-scenario LLM benchmark suite} spanning training and inference (data/tensor/pipeline parallelism, FP8, LoRA, MoE, serving, KV cache, speculative decoding, quantization), revealing 50\% of scenarios have zero tool support (Section~\ref{sec:eval-framework}).
    \item \textbf{Independent accuracy verification} of five tools (146 GPU configurations, collective benchmarks, LLM serving simulations, energy validation), demonstrating self-reported claims are systematically overstated (Section~\ref{sec:eval-results}).
    \item A \textbf{unified simulation pipeline} across five layers---kernel prediction, model composition, distributed training, LLM serving, and hardware design---identifying the kernel-to-model composition gap as the critical missing piece (Section~\ref{sec:unified-pipeline}).
    \item A \textbf{coverage matrix} exposing structural gaps with a \textbf{research agenda} for composition modeling, unified formats, cross-hardware transfer, and continuous validation (Sections~\ref{sec:taxonomy},~\ref{sec:challenges}).
\end{itemize}

Figure~\ref{fig:timeline} illustrates the evolution of performance modeling tools from early analytical frameworks to modern hybrid approaches.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[yearnode/.style={font=\scriptsize\bfseries, text=black!70}, eventnode/.style={font=\tiny, text width=2cm, align=center}]
\draw[thick, ->, black!40] (0,0) -- (9,0);
\foreach \x/\year in {0.3/2016, 1.5/2018, 3/2020, 4.5/2022, 6/2024, 7.2/2025, 8.5/2026} { \draw[thick, black!40] (\x,-0.1) -- (\x,0.1); \node[yearnode, below] at (\x,-0.15) {\year}; }
\node[eventnode, above] at (0.3,0.3) {Eyeriss\\Paleo}; \node[eventnode, below] at (1.2,-0.3) {TVM}; \node[eventnode, above] at (2.2,0.3) {Timeloop\\MAESTRO}; \node[eventnode, below] at (3,-0.3) {Ansor\\ASTRA-sim};
\node[eventnode, above] at (3.8,0.3) {nn-Meter\\Habitat}; \node[eventnode, below] at (4.5,-0.3) {Sparseloop\\Accel-Sim}; \node[eventnode, above] at (5.3,0.3) {TLP\\ASTRA-sim 2.0}; \node[eventnode, below] at (6,-0.3) {VIDUR\\Splitwise};
\node[eventnode, above] at (7.2,0.3) {NeuSight\\AMALI}; \node[eventnode, below] at (8.5,-0.3) {Frontier\\Lumos};
\draw[thick, ->, blue!50, dashed] (0.5,1.2) -- (8.2,1.2); \node[font=\tiny, text=blue!60, above] at (4.3,1.2) {Toward hybrid analytical+learned models};
\end{tikzpicture}%
}
\caption{Evolution of performance modeling tools (2016--2026).}
\label{fig:timeline}
\end{figure}

% ==============================================================================
% SURVEY METHODOLOGY
% ==============================================================================
\section{Survey Methodology}
\label{sec:methodology}

We searched ACM Digital Library, IEEE Xplore, Semantic Scholar, and arXiv for ML performance modeling papers, with citation tracking from seminal works.
Target venues include architecture (MICRO, ISCA, HPCA, ASPLOS), systems (MLSys, OSDI, SOSP, NSDI), and related (NeurIPS, MobiSys, DAC, ISPASS).
From 287 candidates, screening yielded 53 papers proposing or evaluating ML performance prediction tools, supplemented by 12 foundational works.
We cover 2016--2026 and classify by \emph{methodology type} (analytical, simulation, trace-driven, ML-augmented, hybrid), \emph{target platform}, and \emph{abstraction level}.

\textbf{Related surveys.}
Prior work addresses adjacent topics: ML for processor DSE~\cite{rakhshanfar2021survey}, DNN hardware design~\cite{sze2017efficient}, simulation infrastructure~\cite{gpgpusim2009,binkert2011gem5,sst2012}, and standardized measurement~\cite{mlperf_training2020,mlperf_inference2020}.
Foundational approaches include DianNao~\cite{diannao2014}, Eyeriss~\cite{eyeriss2016}, and Paleo~\cite{paleo2017}; the closest prior work~\cite{latencypredictorsnas2024} compares edge predictors for NAS.

\textbf{Scope boundaries.}
We exclude proprietary tools (NVIDIA Nsight~\cite{nsightcompute2019}, Google TPU models) as non-reproducible.
Compiler cost models (Halide~\cite{halide2013}, MLIR~\cite{mlir2020}, Triton~\cite{triton2019}) and cluster schedulers (Pollux~\cite{pollux2021}, Sia~\cite{sia2023}) share techniques but serve distinct use cases.

% ==============================================================================
% BACKGROUND
% ==============================================================================
\section{Background}
\label{sec:background}

\subsection{ML Workload Characteristics}
\label{subsec:workload-characteristics}

ML workloads are computation graphs with statically known operator shapes amenable to analytical modeling, though MoE and dynamic inference introduce input-dependent control flow~\cite{pytorch2019,tensorflow2016}.
Performance depends on dataflow/tiling, KV cache management~\cite{vllm2023}, and compute--memory--network interactions across data, tensor, pipeline, and expert parallelism~\cite{llama3scaling2025}.
LLM inference splits into compute-bound prefill and memory-bound decode~\cite{splitwise2024}, modeled under batched serving~\cite{sarathi2024,orca2022}; training adds quadratic attention scaling, checkpointing, and mixed-precision effects~\cite{llama3scaling2025}.

\subsection{Modeling Methodologies}
\label{subsec:modeling-methodologies}

We classify approaches into five categories:
\textbf{analytical models} (closed-form, e.g., roofline~\cite{williams2009roofline}; $\mu$s evaluation, per-architecture derivation);
\textbf{cycle-accurate simulators} (GPGPU-Sim~\cite{gpgpusim2009}, Accel-Sim~\cite{accelsim2020}; high fidelity, $1000$--$10000\times$ slowdown);
\textbf{trace-driven simulators} (ASTRA-sim~\cite{astrasim2023}, VIDUR~\cite{vidur2024}; orders-of-magnitude faster);
\textbf{ML-augmented} (nn-Meter~\cite{nnmeter2021}; learns from profiling, limited generalization);
\textbf{hybrid} (NeuSight~\cite{neusight2025}, Habitat~\cite{habitat2021}; analytical structure with learned components).
Accuracy metrics vary (MAPE, RMSE, rank correlation), limiting direct comparison (Section~\ref{sec:eval-results}).

% ==============================================================================
% TAXONOMY
% ==============================================================================
\section{Taxonomy}
\label{sec:taxonomy}

We organize the literature by \emph{methodology type}, \emph{target platform}, and \emph{abstraction level}.
Table~\ref{tab:taxonomy-matrix} provides a coverage matrix with trade-off profiles; a temporal validation lag persists (pre-2023 CNNs, post-2023 transformers/LLMs).

\begin{table*}[t]
\centering
\caption{Methodology taxonomy: coverage matrix and trade-off profile. \textbf{0} = research gap.}
\label{tab:taxonomy-matrix}
\small
\begin{tabular}{l|ccccc|cccc}
\toprule
 & \textbf{DNN} & & \textbf{Distrib.} & \textbf{Edge/} & & \textbf{Eval.} & \textbf{Data} & & \textbf{Failure} \\
\textbf{Methodology} & \textbf{Accel.} & \textbf{GPU} & \textbf{Systems} & \textbf{Mobile} & \textbf{CPU} & \textbf{Speed} & \textbf{Req.} & \textbf{Interp.} & \textbf{Mode} \\
\midrule
Analytical       & 3 & 3 & 2 & \textbf{0} & \textbf{0} & $\mu$s & None & High & Dynamic effects \\
Cycle-Accurate   & 1 & 2 & \textbf{0} & \textbf{0} & 1 & Hours & Binary & High & Scale \\
Trace-Driven     & \textbf{0} & \textbf{0} & 7 & \textbf{0} & \textbf{0} & Min. & Traces & Med. & Trace fidelity \\
ML-Augmented     & \textbf{0} & 3 & \textbf{0} & 3 & 1 & ms & Profiling & Low & Distrib.\ shift \\
Hybrid           & 1 & 2 & \textbf{0} & \textbf{0} & 1 & ms & Mixed & Med. & Training domain \\
\bottomrule
\end{tabular}
\end{table*}

Three structural gaps emerge: trace-driven replay is exclusive to distributed systems, edge devices lack hybrid alternatives, and no ML-augmented tool targets distributed systems.
Figure~\ref{fig:tool-architecture} illustrates how methodology types compose.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[comp/.style={draw=black!60, rounded corners=3pt, minimum width=2.4cm, minimum height=0.65cm, align=center, font=\scriptsize}, input/.style={draw=black!40, dashed, rounded corners=2pt, minimum width=1.8cm, minimum height=0.5cm, align=center, font=\tiny}, arr/.style={-{Stealth[length=3pt]}, thick, black!50}, lbl/.style={font=\tiny, text=black!50}]
\node[input] (wl) at (0,4) {Workload\\(ONNX/Chakra)}; \node[input] (hw) at (3,4) {Hardware\\config}; \node[input] (prof) at (6,4) {Profiling\\data};
\node[comp, fill=blue!15] (analytical) at (0,2.5) {Analytical\\Engine}; \node[comp, fill=red!15] (simulator) at (3,2.5) {Cycle-Accurate\\Simulator}; \node[comp, fill=purple!15] (mlmodel) at (6,2.5) {ML Cost\\Model};
\node[comp, fill=green!15, minimum width=3.2cm] (hybrid) at (3,1) {Hybrid Composition\\(analytical + learned)};
\node[comp, fill=orange!15, minimum width=6.5cm] (system) at (3,-0.3) {System Orchestrator (trace-driven)};
\node[input, draw=black!60, fill=gray!10] (output) at (3,-1.5) {Latency / Energy / Throughput};
\draw[arr] (wl) -- (analytical); \draw[arr] (hw) -- (analytical); \draw[arr] (hw) -- (simulator); \draw[arr] (wl) -- (simulator); \draw[arr] (prof) -- (mlmodel); \draw[arr] (wl) -- (mlmodel);
\draw[arr] (analytical) -- (hybrid); \draw[arr] (mlmodel) -- (hybrid); \draw[arr, dashed, gray] (simulator) -- node[lbl, right] {validation} (hybrid);
\draw[arr] (hybrid) -- (system); \draw[arr] (analytical) |- (system); \draw[arr] (system) -- (output);
\node[font=\tiny, text=blue!60, anchor=east] at (-0.5,2.5) {Timeloop}; \node[font=\tiny, text=blue!60, anchor=east] at (-0.5,2.2) {MAESTRO};
\node[font=\tiny, text=red!60, anchor=east] at (1.2,2.8) {Accel-Sim}; \node[font=\tiny, text=red!60, anchor=east] at (1.2,2.5) {GPGPU-Sim};
\node[font=\tiny, text=purple!60, anchor=west] at (7.6,2.5) {nn-Meter}; \node[font=\tiny, text=purple!60, anchor=west] at (7.6,2.2) {TVM};
\node[font=\tiny, text=green!50!black, anchor=west] at (5,1) {NeuSight, Habitat}; \node[font=\tiny, text=orange!60!black, anchor=west] at (6.7,-0.3) {ASTRA-sim, VIDUR};
\end{tikzpicture}%
}
\caption{Unified architecture showing how tool methodologies compose.}
\label{fig:tool-architecture}
\end{figure}

\subsection{Methodology--Platform Pairings}
\label{subsec:by-methodology}

Platform constrains methodology: accelerators use analytical models~\cite{timeloop2019,maestro2019}; GPUs span all five types; distributed systems require trace-driven simulation~\cite{astrasim2023,vidur2024}; edge devices rely on ML-augmented approaches~\cite{nnmeter2021,litepred2024}; CPUs~\cite{concorde2025,granite2022} are least studied.
Errors propagate through the abstraction hierarchy (Figure~\ref{fig:abstraction-levels}): kernel 2--3\%, model 5--12\%, system 5--15\%.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[level/.style={draw, rounded corners=3pt, minimum width=3.2cm, minimum height=0.7cm, align=center, font=\small\bfseries}, tool/.style={font=\scriptsize, text=black!70}, err/.style={font=\scriptsize\itshape, text=red!70!black}, arrow/.style={-{Stealth[length=3pt]}, thick, gray!60}, compos/.style={-{Stealth[length=4pt]}, thick, red!50!black, dashed}]
\node[level, fill=blue!15, draw=blue!50] (kernel) at (0,0) {Kernel / Operator}; \node[level, fill=green!15, draw=green!50] (model) at (0,1.6) {Model / End-to-End}; \node[level, fill=orange!15, draw=orange!50] (system) at (0,3.2) {System};
\draw[arrow] (kernel) -- (model); \draw[arrow] (model) -- (system);
\node[tool, anchor=east] at (-2.1,0) {NeuSight, nn-Meter,}; \node[tool, anchor=east] at (-2.1,-0.3) {TVM, GRANITE, TLP}; \node[tool, anchor=east] at (-2.1,1.6) {Paleo, Habitat,}; \node[tool, anchor=east] at (-2.1,1.3) {AMALI, Lumos, LIFE}; \node[tool, anchor=east] at (-2.1,3.2) {ASTRA-sim, VIDUR,}; \node[tool, anchor=east] at (-2.1,2.9) {SimAI, Frontier};
\node[err, anchor=west] at (2.1,0) {2--3\% error}; \node[err, anchor=west] at (2.1,1.6) {5--12\% error}; \node[err, anchor=west] at (2.1,3.2) {5--15\% error};
\draw[compos] (3.6,0.2) -- (3.6,3.0); \node[font=\scriptsize, text=red!50!black, rotate=90, anchor=south] at (3.9,1.6) {error accumulates};
\end{tikzpicture}%
}
\caption{Abstraction level hierarchy with error accumulation across levels.}
\label{fig:abstraction-levels}
\end{figure}

\subsection{Workload Coverage and Validation Gaps}
\label{subsec:workload-coverage}

Of 14 tools, 9 validate on CNNs; post-2023 tools target transformers/LLMs but \textbf{no tool validates on diffusion models or dynamic inference}~\cite{dynamicreasoning2026}, only Frontier~\cite{frontier2025} validates MoE, and none spans the full kernel-to-system stack.

% ==============================================================================
% SURVEY OF APPROACHES
% ==============================================================================
\section{Survey of Approaches}
\label{sec:survey}

We survey tools by target platform (Table~\ref{tab:survey-summary}).

\begin{table*}[t]
\centering
\caption{Surveyed tools by target platform. A=Analytical, S=Simulation, T=Trace-driven, M=ML-augmented, H=Hybrid. $^*$Surrogate-vs-simulator fidelity. $^\dagger$Unverifiable. $^\ddagger$No hardware baseline.}
\label{tab:survey-summary}
\small
\begin{tabular}{lllllll}
\toprule
\textbf{Tool} & \textbf{Platform} & \textbf{Method} & \textbf{Target} & \textbf{Accuracy} & \textbf{Speed} & \textbf{Key Capability} \\
\midrule
\multicolumn{7}{l}{\textit{DNN Accelerator Modeling}} \\
Timeloop~\cite{timeloop2019} & NPU & A & Latency/Energy & 5--10\% & $\mu$s & Loop-nest DSE \\
MAESTRO~\cite{maestro2019} & NPU & A & Latency/Energy & 5--15\% & $\mu$s & Data-centric directives \\
Sparseloop~\cite{sparseloop2022} & NPU & A & Sparse tensors & 5--10\% & $\mu$s & Compression modeling \\
PyTorchSim~\cite{pytorchsim2025} & NPU & S & Cycle-accurate & N/A$^\ddagger$ & Hours & PyTorch 2 integration \\
ArchGym~\cite{archgym2023} & Multi & H & Multi-objective & 0.61\%$^*$ & ms & ML-aided DSE \\
\midrule
\multicolumn{7}{l}{\textit{GPU Performance Modeling}} \\
Accel-Sim~\cite{accelsim2020} & GPU & S & Cycle-accurate & 10--20\% & Hours & SASS trace-driven \\
GPGPU-Sim~\cite{gpgpusim2009} & GPU & S & Cycle-accurate & 10--20\% & Hours & CUDA workloads \\
AMALI~\cite{amali2025} & GPU & A & LLM inference & 23.6\% & ms & Memory hierarchy \\
NeuSight~\cite{neusight2025} & GPU & H & Kernel/E2E latency & 2.3\% & ms & Tile-based prediction \\
Habitat~\cite{habitat2021} & GPU & H & Training time & 11.8\% & Per-kernel & Wave scaling \\
\midrule
\multicolumn{7}{l}{\textit{Distributed Training and LLM Serving}} \\
ASTRA-sim~\cite{astrasim2023} & Distributed & T & Training time & 5--15\% & Minutes & Collective modeling \\
SimAI~\cite{simai2025} & Distributed & T & Training time & 1.9\% & Minutes & Full-stack simulation \\
Lumos~\cite{lumos2025} & Distributed & T & LLM training & 3.3\% & Minutes & H100 training \\
VIDUR~\cite{vidur2024} & GPU cluster & T & LLM serving & $<$5\% & Seconds & Prefill/decode phases \\
Frontier~\cite{frontier2025} & Distributed & T & MoE inference & --- & Minutes & Stage-centric sim. \\
TrioSim~\cite{triosim2025} & Multi-GPU & T & DNN training & N/A$^\ddagger$ & Minutes & Lightweight multi-GPU \\
\midrule
\multicolumn{7}{l}{\textit{Edge Device Modeling}} \\
nn-Meter~\cite{nnmeter2021} & Edge & M & Latency & $<$1\%$^\dagger$ & ms & Kernel detection \\
LitePred~\cite{litepred2024} & Edge & M & Latency & 0.7\% & ms & 85-platform transfer \\
HELP~\cite{help2021} & Multi & M & Latency & 1.9\% & ms & 10-sample adaptation \\
\midrule
\multicolumn{7}{l}{\textit{Compiler Cost Models}} \\
TVM~\cite{tvm2018} & GPU & M & Schedule perf. & $\sim$15\% & ms & Autotuning guidance \\
Ansor~\cite{ansor2020} & GPU & M & Schedule perf. & $\sim$15\% & ms & Program sampling \\
TLP~\cite{tlp2023} & GPU & M & Tensor program & $<$10\% & ms & Transformer cost model \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{DNN Accelerator Modeling}
\label{subsec:accelerator-modeling}

Computational regularity~\cite{sze2017efficient} enables analytical tractability, building on DianNao~\cite{diannao2014} and Eyeriss~\cite{eyeriss2016}.
Timeloop~\cite{timeloop2019} enumerates loop-nest mappings to find optimal dataflow in microseconds (5--10\% error); MAESTRO~\cite{maestro2019} trades completeness for a compact data-centric representation; Sparseloop~\cite{sparseloop2022} extends to sparse tensors; SCALE-Sim~\cite{scalesim2019} provides cycle-accurate validation.
PyTorchSim~\cite{pytorchsim2025} and ArchGym~\cite{archgym2023} (0.61\% RMSE) represent newer approaches; PIM tools~\cite{upimulator2024,attacc2024,neupims2024,paise2025} lack hardware validation.

\subsection{GPU Performance Modeling}
\label{subsec:gpu-modeling}

Cycle-accurate simulators (GPGPU-Sim~\cite{gpgpusim2009}, Accel-Sim~\cite{accelsim2020}) achieve 0.90--0.97 IPC correlation at $1000$--$10000\times$ slowdown, integrating memory models~\cite{dramsim3_2020,ramulator2_2023}; reverse-engineering~\cite{dissectinggpu2025} improved Accel-Sim to 13.98\% MAPE.
NeuSight~\cite{neusight2025} achieves 2.3\% MAPE via tile-based decomposition matching CUDA thread blocks; AMALI~\cite{amali2025} averages data movement (23.6\%); Habitat~\cite{habitat2021} transfers across GPUs (11.8\%) via wave scaling.
Compiler cost models---TVM~\cite{tvm2018}/Ansor~\cite{ansor2020} ($\sim$15\%), TLP~\cite{tlp2023} ($<$10\%)---and recent tools~\cite{life2025,hermes2025,omniwise2025,swizzleperf2025,synperf2025} target inference and autotuning~\cite{tenset2021}.

\subsection{Distributed Training and LLM Serving}
\label{subsec:distributed-modeling}

Fidelity increases with granularity~\cite{megatronlm2020,gpipe2019,zero2020}: VIDUR models at the \emph{request level}; ASTRA-sim~\cite{astrasim2023} replays Chakra traces~\cite{chakra2023} at the \emph{collective level} (5--15\%); SimAI~\cite{simai2025} models NCCL-level reductions (1.9\%).
Echo~\cite{echo2024}, Lumos~\cite{lumos2025} (3.3\%), PRISM~\cite{prism2025}, Paleo~\cite{paleo2017}, and MAD Max~\cite{madmax2024} provide complementary coverage; inference serving tools~\cite{distserve2024,frontier2025,podattention2025,aqua2025,throttllem2025,sailor2025} address scheduling, disaggregation, and power.

\subsection{Edge Device Modeling}
\label{subsec:edge-modeling}

nn-Meter~\cite{nnmeter2021} claims $<$1\% but is unverifiable (Section~\ref{sec:eval-results}); LitePred~\cite{litepred2024} achieves 0.7\% across 85 platforms; HELP~\cite{help2021} reaches 1.9\% with 10-sample meta-learning.
ESM~\cite{esm2025} and transfer learning results~\cite{latencypredictorsnas2024} suggest data quality matters more than model sophistication.

% ==============================================================================
% EVALUATION FRAMEWORK
% ==============================================================================
\section{Evaluation Methodology}
\label{sec:eval-framework}

Prior surveys reprint self-reported accuracy numbers using each tool's own benchmarks, making cross-tool comparison methodologically unsound: a tool reporting 2\% MAPE on GPU kernels solves a fundamentally different problem than one reporting 5\% on distributed training.
We introduce a novel evaluation methodology---\textbf{accuracy-centered independent verification}---that addresses this gap through two components.
First, an \textbf{LLM-focused benchmark suite} of 28 scenarios defines standardized coverage criteria representing concrete user needs for modern LLM training and inference.
Second, \textbf{independent experiments} deploy each tool from its public artifact and measure accuracy under controlled conditions, replacing reliance on self-reported claims with reproducible third-party evaluation.
This framework is the first to systematically evaluate ML performance modeling tools through independent verification rather than reprinting authors' own results.

\textbf{Evaluation principle.}
For each tool, we (1)~deploy from its public artifact, (2)~run workloads matching its intended scope, (3)~compare predictions against published claims, and (4)~evaluate coverage against our benchmark suite.
Where absolute verification requires hardware we lack (e.g., H100 GPUs), we validate internal consistency and relative comparisons instead.

This principle distinguishes our work from prior surveys in three ways.
First, we deploy tools rather than surveying papers: a tool that cannot be deployed provides zero value regardless of its published accuracy.
Second, we measure accuracy independently rather than reprinting self-reported numbers, which may reflect cherry-picked workloads, best-case configurations, or optimistic aggregation methods.
Third, we evaluate each tool against the \emph{same} benchmark suite rather than each tool's preferred benchmarks, enabling meaningful cross-tool comparison.

\subsection{LLM Benchmark Suite}
\label{subsec:benchmark-suite}

We define 28 benchmark scenarios across 8 categories representing the workloads that LLM practitioners need performance predictions for (Table~\ref{tab:benchmark-suite}).
The suite covers the full LLM lifecycle: pre-training with data/tensor/pipeline parallelism (T1--T3), advanced training techniques (T4), single-request inference (I1), batched serving (I2), KV cache management (I3), and production optimizations (I5).
Unlike existing benchmarks that measure hardware performance (MLPerf), our suite evaluates whether prediction \emph{tools} can model these scenarios.

\begin{table}[t]
\centering
\caption{LLM benchmark suite: 28 scenarios across training (T1--T4) and inference (I1--I5). Each represents a concrete user need for performance prediction.}
\label{tab:benchmark-suite}
\small
\begin{tabular}{lp{3.5cm}r}
\toprule
\textbf{Cat.} & \textbf{Description} & \textbf{\#} \\
\midrule
T1 & Data-parallel pre-training & 3 \\
T2 & Tensor-parallel pre-training & 2 \\
T3 & Pipeline-parallel pre-training & 2 \\
T4 & Advanced (FP8, LoRA, SP, MoE) & 4 \\
\midrule
I1 & Single-request inference & 3 \\
I2 & Batched serving (vLLM, Sarathi) & 3 \\
I3 & KV cache management & 2 \\
I4 & Multi-model serving & 1 \\
I5 & Production (spec.\ decode, quant.) & 4 \\
\midrule
& \textbf{Total} & \textbf{28} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Design principles.}
Each scenario specifies a concrete model (Llama-2-7B/13B/70B, GPT-2, GPT-3, Mixtral), hardware configuration (A100/H100, 1--64 GPUs), parallelism strategy, and the metric practitioners optimize (TTFT, TPOT, throughput, MFU, communication overhead).
Training scenarios span from single-node data parallelism (T1.1: GPT-2 on 8$\times$A100) to large-scale hybrid parallelism (T3.2: GPT-3 175B on 64$\times$H100 with PP8+TP8).
Inference scenarios range from single-request latency (I1.1) to production optimizations like speculative decoding (I5.1) and disaggregated serving (I5.4).

\textbf{Scenario selection rationale.}
The 28 scenarios were selected to reflect real deployment decisions.
Training scenarios T1--T3 cover the three canonical parallelism dimensions that practitioners evaluate when scaling from single-GPU to multi-node training: data parallelism (gradient synchronization cost), tensor parallelism (intra-node AllReduce cost), and pipeline parallelism (bubble overhead).
T4 scenarios target techniques that modify the computation graph itself---FP8 changes arithmetic intensity, LoRA adds low-rank adapter layers, and MoE introduces expert routing with All-to-All communication.
Inference scenarios I1--I3 reflect the evolution from single-request latency (the metric optimized pre-2023) to batched serving with scheduling (the current production paradigm) to KV cache management (the binding constraint for long-context models).
I5 scenarios target production optimizations that no tool currently models but that dominate deployment decisions: speculative decoding can improve throughput by $2$--$3\times$ but requires modeling draft-target model interaction; disaggregated serving~\cite{splitwise2024} separates prefill and decode to different GPU pools, requiring inter-pool network modeling.
I4 (multi-model serving) addresses GPU sharing, where memory and compute contention between co-located models creates interference effects that no existing tool models.

\textbf{Concrete benchmark parameterization.}
Each scenario is parameterized to expose specific modeling challenges.
Training scenario T1.1 (GPT-2 on 8$\times$A100 with data parallelism) requires predicting AllReduce time for 354\,M parameters at fp16---a 708\,MB gradient exchange where ring bandwidth at NVLink speed determines whether communication overlaps with backward pass computation.
T3.2 (GPT-3 175B on 64$\times$H100 with PP8+TP8) combines pipeline bubbles ($(P-1)/(\text{microbatches}+P-1)$ efficiency) with intra-node tensor-parallel AllReduce, requiring tools to model the interaction between pipeline scheduling and communication.
Inference scenario I2.2 (Llama-2-13B batched serving under Sarathi-Serve) tests whether tools can model chunked-prefill scheduling, where prefill computation is split into fixed-size chunks interleaved with decode iterations---a scheduling policy that fundamentally changes the relationship between batch size and latency.
I5.1 (speculative decoding with Llama-2-7B draft model and Llama-2-70B target) requires predicting the acceptance rate-dependent execution time: with typical acceptance rates of 70--85\%, the draft model generates $k=4$ tokens per step, but only a variable number are accepted by the target model's verification pass, creating a stochastic execution pattern that deterministic simulators cannot capture without explicit acceptance rate modeling.

\textbf{Coverage criterion.}
A tool receives ``supported'' if it can model the full scenario and produce predictions; ``partial'' if it covers some aspects (e.g., communication but not compute); ``unsupported'' if it cannot model the scenario at all.
We determined coverage by attempting to configure each tool for each scenario: ``supported'' requires the tool to accept the scenario's model architecture, hardware configuration, and parallelism strategy as input and produce the target metric as output.
``Partial'' means the tool can model some component (e.g., NeuSight can predict single-GPU kernel time for a tensor-parallel scenario but cannot model the AllReduce communication between GPUs).
Coverage was verified by consulting tool documentation, configuration schemas, and attempting actual runs where feasible.
We did not consider post-hoc workarounds (e.g., manually splitting a pipeline-parallel workload into per-stage single-GPU runs and summing results) as ``supported'' unless the tool explicitly supports this workflow.

\textbf{Coverage assessment methodology.}
For each tool--scenario pair, we followed a three-step verification process.
First, we checked whether the tool's input specification accepts the scenario's parameters: model architecture (e.g., Llama-2-70B for T3.2), hardware configuration (e.g., 64$\times$H100), and parallelism strategy (e.g., PP8+TP8).
Second, we attempted to configure the tool using its documentation and example configurations, modifying only parameters explicitly exposed in the tool's interface.
Third, we verified that the tool produces the scenario's target metric (e.g., TTFT for I2.2, MFU for T1.3) as a direct output rather than requiring manual post-processing.
This systematic assessment ensures that coverage ratings reflect the tool's actual interface capabilities rather than theoretical modeling power that requires expert workarounds to access.

\subsection{Tool Selection}
\label{subsec:tool-selection}

From 22 tools, we select 5 using three criteria: (1)~\emph{methodology coverage}---one per type; (2)~\emph{artifact availability}---open-source with build instructions; (3)~\emph{scope diversity}---different hardware and workload types.
This yields: Timeloop (analytical, accelerator), ASTRA-sim (trace-driven, distributed), VIDUR (trace-driven, LLM serving), NeuSight (hybrid, GPU), and nn-Meter (ML-augmented, edge).
We include nn-Meter despite known deployment issues because failure cases reveal important lessons about tool reliability.

\textbf{Excluded tools and rationale.}
Notable exclusions include SimAI (1.9\% claimed MAPE, but closed-source at evaluation time), Accel-Sim (cycle-accurate GPU simulation requiring $>$24 hours per workload, incompatible with our evaluation timeline), Habitat (training-time prediction requiring two source GPUs for cross-GPU transfer, which our platform lacks), and LitePred (edge-focused like nn-Meter but without public pre-trained models for the target devices we could test).
For each excluded tool, we report published accuracy in Table~\ref{tab:survey-summary} with appropriate caveats.

\subsection{Experimental Design}
\label{subsec:experimental-design}

Experiments match each tool's intended scope:
\textbf{NeuSight:} 146 configurations across 12 GPU types (NVIDIA V100, H100, A100-80G, A100-40G, L4, T4, P100, P4; AMD MI100, MI210, MI250).
\textbf{ASTRA-sim:} 4 collectives at 8~NPUs on HGX-H100, plus ResNet-50 at 2/4/8 GPUs.
\textbf{VIDUR:} Llama-2-7B on simulated A100 under vLLM and Sarathi schedulers.
\textbf{Timeloop:} ResNet-50 Conv1 on Eyeriss-like architecture.
\textbf{nn-Meter:} Attempted deployment across 4 edge device targets.
All experiments run on Apple M2 Ultra (192\,GB RAM, Docker where available).
Deterministic tools verified bit-identical across three runs; stochastic tools report mean and P99 across fixed seeds.
Scripts and data are provided as supplementary material.

\textbf{Verification methodology.}
For NeuSight, we adopted a \emph{prediction-vs-label} approach: the tool's artifact repository includes both predicted latencies and ground-truth hardware measurements across 12 GPU types.
Rather than running NeuSight on our hardware (which lacks discrete GPUs), we independently computed MAPE from the artifact's own prediction/label pairs for all 146 configurations, grouped by device and mode (training/inference).
This approach verifies whether the tool's \emph{published accuracy claims} match the accuracy \emph{achievable from its own artifacts}---testing reproducibility of claims rather than absolute accuracy.
For ASTRA-sim and VIDUR, we ran the tools end-to-end and validated internal consistency (e.g., deterministic outputs, correct relative ordering of collectives) since absolute accuracy requires hardware we lack.
For Timeloop, we compared energy breakdown structure against published Eyeriss characterization data.
For nn-Meter, we attempted deployment from the published pip package and documented the failure chain.

\subsection{Limitations}
\label{subsec:eval-limitations}

Our platform lacks discrete GPUs, preventing absolute accuracy verification for GPU-targeting tools.
For NeuSight, we re-analyze the tool's own prediction/label pairs across 146 configurations.
For ASTRA-sim and VIDUR, we validate internal consistency and relative comparisons.
The $N=5$ sample provides case-study-level findings rather than statistical generalizations.

\textbf{What our evaluation can and cannot show.}
Our approach verifies three properties: (1)~\emph{claim reproducibility}---whether published accuracy numbers are achievable from the tool's own artifacts; (2)~\emph{internal consistency}---whether tool outputs obey expected mathematical relationships (e.g., Reduce-Scatter $\approx$ 0.5$\times$ All-Reduce); (3)~\emph{relative ranking}---whether tools correctly rank configurations (e.g., Sarathi vs.\ vLLM serving latency).
Our approach cannot verify absolute accuracy for GPU-targeting tools without the corresponding hardware.
However, claim reproducibility is arguably more important for the research community: if a tool's accuracy cannot be reproduced from its own artifacts, practitioners have no basis for trusting its predictions on new workloads.

\textbf{Generalizability of per-tool findings.}
Each tool was evaluated on workloads within its intended scope.
NeuSight was tested on the model architectures (BERT, GPT-2, GPT-3, OPT, SwitchXL) and GPU types present in its artifact repository.
ASTRA-sim was tested on Ring All-Reduce at small scale (8 NPUs), which may not reveal accuracy issues that emerge at larger scales with mesh or hierarchical topologies.
VIDUR was tested on a single model (Llama-2-7B) at moderate load (QPS 2.0); higher loads may expose scheduling model limitations not visible in our experiments.
Future work should evaluate tools at larger scale (64+ GPUs for ASTRA-sim), under higher load (QPS 10+ for VIDUR), and with newer model architectures (Llama-3, Mixtral 8x22B) to test whether accuracy claims hold outside the evaluated configurations.

% ==============================================================================
% EVALUATION RESULTS
% ==============================================================================
\section{Evaluation Results}
\label{sec:eval-results}

Table~\ref{tab:accuracy-comparison} summarizes accuracy findings; Table~\ref{tab:feature-matrix} presents the feature availability matrix.

\begin{table}[t]
\centering
\caption{Accuracy comparison: published claims vs.\ our independent verification.}
\label{tab:accuracy-comparison}
\small
\begin{tabular}{lp{1.5cm}p{1.5cm}p{2.2cm}}
\toprule
\textbf{Tool} & \textbf{Published} & \textbf{Our Result} & \textbf{Verdict} \\
\midrule
NeuSight & 2.3\% MAPE & 5.87--27.1\% & Overstated 2--4$\times$ \\
ASTRA-sim & 9.69\% geo. & Trends valid & Plausible, unverified \\
VIDUR & $<$5\% err. & Ranking valid & Plausible, unverified \\
Timeloop & $<$10\% RTL & Structure valid & Consistent w/ Eyeriss \\
nn-Meter & $<$1\% MAPE & \textbf{No output} & Complete failure \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[t]
\centering
\caption{Feature availability matrix. ``---'' = no capability. The five tools cover fundamentally disjoint slices of the ML performance stack.}
\label{tab:feature-matrix}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Feature} & \textbf{NeuSight} & \textbf{ASTRA-sim} & \textbf{VIDUR} & \textbf{Timeloop} & \textbf{nn-Meter} \\
\midrule
\multicolumn{6}{l}{\emph{Workload Types}} \\
CNN training/inference & Full model & Comm only & --- & Single-layer energy & Inf.\ latency only \\
Transformer training & Single-GPU time & Comm patterns & --- & --- & --- \\
LLM inference serving & --- & --- & Full (TTFT/TPOT) & --- & --- \\
Accelerator design space & --- & --- & --- & Full (dataflow) & --- \\
Edge inference & --- & --- & --- & --- & Full (broken) \\
\midrule
\multicolumn{6}{l}{\emph{Hardware Targets}} \\
NVIDIA datacenter GPU & 7 types & Comm only & A100/H100 & --- & --- \\
AMD GPU & MI100/MI210/MI250 & --- & --- & --- & --- \\
Custom accelerator & --- & --- & --- & Eyeriss, systolic & --- \\
Edge device & --- & --- & --- & --- & ARM, Adreno, Myriad \\
Multi-GPU cluster & DP/PP/TP (limited) & 2--16 GPUs & --- & --- & --- \\
\midrule
\multicolumn{6}{l}{\emph{Prediction Granularity}} \\
Kernel/layer level & Per-layer (tiles) & --- & --- & Per-layer energy & Per-kernel models \\
Model level & Sum of layers & Comm only & Full iteration & --- & Sum of kernels \\
System level & --- & Comm + compute & Request scheduling & --- & --- \\
\midrule
\multicolumn{6}{l}{\emph{Metrics}} \\
Latency & GPU kernel (ms) & Comm cycles & E2E, TTFT, TPOT & Cycle count & Inf.\ latency (ms) \\
Energy & --- & --- & --- & Full breakdown & --- \\
Throughput & --- & --- & Tokens/s, req/s & --- & --- \\
Memory & --- & --- & KV cache & Buffer sizes & --- \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{NeuSight: GPU Kernel Accuracy}
\label{subsec:neusight-results}

NeuSight claims 2.3\% overall MAPE for GPU kernel latency prediction~\cite{neusight2025}.
We independently re-analyzed 146 model configurations across 12 GPU types using the tool's own prediction/label pairs (Table~\ref{tab:neusight-accuracy}).

\begin{table}[t]
\centering
\caption{NeuSight accuracy: published claims vs.\ our verification across 12 GPU types. $N$: number of model configurations tested. Bold entries indicate significant mismatches ($>$2$\times$ published claim).}
\label{tab:neusight-accuracy}
\small
\begin{tabular}{llrrl}
\toprule
\textbf{Device} & \textbf{Mode} & \textbf{Claimed} & \textbf{Ours} & \textbf{Verdict} \\
\midrule
V100 & Inference & 5.2\% & 5.87\% & Match \\
V100 & Training & 7.4\% & 8.91\% & Close \\
H100 & Inference & 2.3\% & \textbf{8.74\%} & Mismatch \\
H100 & Training & 4.1\% & 6.60\% & Close \\
A100-80G & Training & 5.8\% & 7.59\% & Close \\
A100-40G & Inference & --- & 8.63\% & --- \\
L4 & Inference & 3.8\% & \textbf{14.08\%} & Mismatch \\
T4 & Inference & 6.1\% & \textbf{18.51\%} & Mismatch \\
P4 & Inference & --- & \textbf{27.10\%} & --- \\
MI100 & Inference & --- & 10.80\% & --- \\
MI210 & Inference & --- & 8.40\% & --- \\
MI250 & Inference & --- & 7.65\% & --- \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:accuracy-comparison} visualizes the accuracy gap across GPU types, contrasting published claims with our independently measured MAPE.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=7pt,
    xlabel={GPU Device},
    ylabel={MAPE (\%)},
    ymin=0, ymax=32,
    xtick=data,
    symbolic x coords={V100,H100,A100-80G,L4,T4,P4,MI100,MI210,MI250},
    xticklabel style={font=\tiny, rotate=35, anchor=east},
    yticklabel style={font=\scriptsize},
    xlabel style={font=\small},
    ylabel style={font=\small},
    legend style={at={(0.02,0.98)}, anchor=north west, font=\tiny, draw=none, fill=white, fill opacity=0.9, text opacity=1},
    legend cell align={left},
    height=5.5cm,
    width=8.5cm,
    enlarge x limits={abs=0.6cm},
    nodes near coords,
    every node near coord/.append style={font=\tiny, rotate=60, anchor=west},
    point meta=explicit symbolic,
]
% Published claims (where available)
\addplot[fill=red!50, draw=red!70] coordinates {
    (V100,5.2) [5.2]
    (H100,2.3) [2.3]
    (A100-80G,5.8) [5.8]
    (L4,3.8) [3.8]
    (T4,6.1) [6.1]
    (P4,0) []
    (MI100,0) []
    (MI210,0) []
    (MI250,0) []
};
% Our measured MAPE
\addplot[fill=blue!50, draw=blue!70] coordinates {
    (V100,5.87) [5.87]
    (H100,8.74) [8.74]
    (A100-80G,7.59) [7.59]
    (L4,14.08) [14.08]
    (T4,18.51) [18.51]
    (P4,27.10) [27.10]
    (MI100,10.80) [10.80]
    (MI210,8.40) [8.40]
    (MI250,7.65) [7.65]
};
\legend{Published claim, Our result}
\end{axis}
\end{tikzpicture}%
}
\caption{NeuSight accuracy gap by GPU device. Published claims (red) vs.\ our independently measured MAPE (blue). Devices without published claims show only our result. Error grows up to $4\times$ on GPUs outside the training distribution (T4, P4).}
\label{fig:accuracy-comparison}
\end{figure}

\textbf{Key finding: accuracy degrades outside the training distribution.}
NeuSight achieves its best accuracy on V100 (5.87\%), the GPU most represented in training data.
On newer GPUs (H100: 8.74\% vs.\ claimed 2.3\%, a $3.8\times$ gap) and older GPUs (T4: 18.51\%, P4: 27.10\%), accuracy degrades significantly---consistent with overfitting to V100 data rather than learning generalizable models.
The worst-case max APE reaches 65.30\% on P4 (GPT-2-Large inference at batch size 4).

\textbf{Per-model error patterns reveal systematic biases.}
Across all 146 configurations, we observe three failure modes.
First, \emph{batch size sensitivity}: at fixed model and GPU, doubling the batch size often doubles the prediction error (e.g., BERT-Large on H100: 13.96\% at batch 16 with fusion vs.\ 24.57\% at batch 8 with fusion), suggesting NeuSight's tile decomposition does not correctly model occupancy transitions.
Second, \emph{operator fusion blindness}: fused-kernel configurations consistently show higher error than unfused equivalents (H100 GPT-2-Large: 19.37\% fused vs.\ 6.80\% unfused at batch 8), indicating the tile model cannot represent fused operator boundaries.
Third, \emph{cross-vendor degradation}: AMD GPUs (MI100: 10.80\%, MI210: 8.40\%, MI250: 7.65\% for inference) show systematically higher training error (15.62--15.81\%) than inference error, with worst-case 33.04\% on MI210 GPT-2-Large training at batch 4---a configuration where wavefront scheduling differs significantly from NVIDIA's warp scheduling.

\textbf{Multi-GPU parallelism accuracy.}
Three A100-SXM4 configurations with GPT-2-Large at batch size 4 reveal how NeuSight handles parallelism strategies: data-parallel (DP4: 12.87\% APE), tensor-parallel (TP4: 8.40\%), and pipeline-parallel (PP4: 10.26\%).
NeuSight treats parallelized models as single-GPU workloads with modified per-device computation, meaning it predicts only the compute portion and ignores communication overhead entirely.
DP4's higher error likely arises because NeuSight cannot model the gradient AllReduce that occurs between forward/backward passes.
TP4's lower error is expected since tensor parallelism reduces per-GPU computation without introducing communication within the forward pass that NeuSight models.
This pattern confirms that NeuSight should be positioned as a \emph{kernel-level} predictor rather than a system-level tool.

\textbf{Implications for practitioners.}
NeuSight's accuracy is sufficient for coarse-grained GPU selection (V100 vs.\ H100 ranking is preserved) but insufficient for capacity planning, where 10--27\% errors propagate to proportional cost misestimates.
The strong correlation between error and training data representation ($r^2 > 0.7$ for MAPE vs.\ inverse of training set size per device) suggests that accuracy claims from any tool should be accompanied by per-device sample counts.

\textbf{Benchmark suite coverage for NeuSight.}
Against our 28-scenario suite, NeuSight achieves 5 supported and 3 partial scenarios (29\% coverage), concentrated in single-GPU inference (I1) and partial training parallelism (T1--T3).
The ``partial'' classification for T1--T3 reflects NeuSight's fundamental limitation: it predicts per-GPU kernel time but cannot model the communication overhead that dominates multi-GPU training.
For example, in scenario T2.1 (Llama-2-13B tensor-parallel on 4$\times$A100), NeuSight can predict the reduced per-GPU computation after tensor partitioning but cannot predict the AllReduce latency between GPUs that determines whether communication overlaps with computation.
This makes NeuSight useful as a \emph{component} in a multi-tool pipeline but insufficient as a standalone predictor for any distributed scenario.

\subsection{ASTRA-sim: Distributed Training Communication}
\label{subsec:astrasim-results}

ASTRA-sim reports 9.69\% geomean error at 8-GPU HGX-H100 for Ring All-Reduce~\cite{astrasim2020}.
We ran collective microbenchmarks and ResNet-50 data-parallel training scaling (Table~\ref{tab:astrasim-results}).

\begin{table}[t]
\centering
\caption{ASTRA-sim results on HGX-H100 configuration from our experiments. Top: collectives (8 NPUs, 1\,MB). Bottom: ResNet-50 scaling.}
\label{tab:astrasim-results}
\small
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{l}{\textbf{Collective Microbenchmarks (8 NPUs, 1\,MB)}} \\
\midrule
\textbf{Collective} & \textbf{Cycles} & \textbf{Ratio vs.\ AR} \\
\midrule
All-Reduce & 57,426 & 1.000 \\
All-Gather & 44,058 & 0.767 \\
Reduce-Scatter & 28,950 & 0.504 \\
All-to-All & 114,000 & 1.985 \\
\midrule
\multicolumn{3}{l}{\textbf{ResNet-50 Data-Parallel Training}} \\
\midrule
\textbf{GPUs} & \textbf{Comm Cycles} & \textbf{Comm Overhead} \\
\midrule
2 & 574,289 & 0.05\% \\
4 & 1,454,270 & 0.13\% \\
8 & 3,307,886 & 0.30\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Internal consistency is strong.}
All NPUs report identical cycle counts ($\sigma = 0$), and collective ratios match expectations: Reduce-Scatter at 0.504$\times$ All-Reduce (half-data operation), All-to-All at 1.985$\times$ (personalized exchange).
Communication scales as expected from 4 to 8 GPUs ($2.27\times$).

\textbf{Scaling behavior reveals modeling assumptions.}
ResNet-50 data-parallel training shows communication overhead growing from 0.05\% (2 GPUs) to 0.30\% (8 GPUs)---a $6\times$ increase for a $4\times$ scale-up.
This super-linear scaling arises because All-Reduce costs scale as $2(N-1)/N$ times the message size, approaching $2\times$ asymptotically.
Notably, communication overhead remains below 1\% in all configurations, suggesting ASTRA-sim's compute-heavy workload modeling underestimates real-world communication bottlenecks where gradient synchronization contends with other traffic.
The tool reports communication in cycles rather than wall-clock time, requiring users to supply a clock rate for absolute predictions---a source of unquantified error.
Furthermore, ASTRA-sim's All-to-All collective at $1.985\times$ All-Reduce cost provides a useful benchmark for MoE workloads where expert routing relies heavily on All-to-All communication.
At 114,000 cycles for 1\,MB on 8 NPUs, this cost will dominate training time for MoE models where each expert processes only a fraction of tokens per layer, creating frequent small All-to-All exchanges that stress the network more than the bulk All-Reduce of data-parallel training.

\textbf{Absolute accuracy is unverifiable} without HGX-H100 hardware.
ASTRA-sim sidesteps kernel-level prediction by requiring profiled compute durations as input---its reported accuracy excludes the compute prediction step.
This design choice means the tool's claimed 9.69\% geomean error applies only to \emph{communication time prediction}, not total training time.
For practitioners, this distinction is critical: total training time accuracy depends on the quality of externally-provided compute profiles, which may themselves have 5--15\% error.

\textbf{Benchmark coverage implications.}
Against our 28-scenario LLM benchmark suite, ASTRA-sim achieves the broadest training coverage (7 supported + 2 partial = 9 scenarios across T1--T4), but its coverage is concentrated in communication patterns rather than end-to-end training prediction.
For scenario T1.1 (GPT-2 data-parallel on 8$\times$A100), ASTRA-sim can model the gradient AllReduce communication but requires externally profiled per-layer compute times---meaning it predicts communication overhead accurately but not total iteration time.
For T4.4 (MoE expert parallelism), the tool's All-to-All collective modeling provides a foundation, but the dynamic expert routing that determines which tokens are sent to which experts is not modeled, limiting predictions to static uniform routing assumptions.

\subsection{VIDUR: LLM Inference Serving}
\label{subsec:vidur-results}

VIDUR reports $<$5\% error vs.\ real serving traces~\cite{vidur2024}.
We simulated Llama-2-7B on a simulated A100 under two scheduler configurations (Table~\ref{tab:vidur-results}).

\begin{table}[t]
\centering
\caption{VIDUR simulation: Llama-2-7B on simulated A100 (Poisson arrivals, QPS~2.0, seed=42). All metrics from our experiments.}
\label{tab:vidur-results}
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{vLLM} & \textbf{Sarathi} \\
\midrule
Requests & 200 & 50 \\
Avg E2E latency (s) & 0.177 & 0.158 \\
P99 E2E latency (s) & 0.314 & 0.262 \\
Avg TTFT (s) & 0.027 & 0.025 \\
Avg TPOT (s) & 0.0093 & 0.0090 \\
Preempted requests & 53 & 0 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Scheduler ranking is correct.}
Sarathi~\cite{sarathi2024} achieves 12.2\% lower E2E latency and eliminates preemption (0 vs.\ 53 requests), consistent with its chunked-prefill design.
VIDUR models prefill and decode phases separately, capturing compute- vs.\ memory-bound regimes.

\textbf{Latency distribution analysis.}
Beyond mean latency, the tail behavior is revealing.
Under vLLM, P99 E2E latency (0.314\,s) is $1.77\times$ the mean (0.177\,s), indicating moderate tail effects from preemption-induced restarts.
Sarathi's P99/mean ratio is lower ($1.66\times$), directly attributable to zero preemptions: chunked prefill prevents long prefill operations from blocking decode batches.
TTFT (time-to-first-token) averages 0.027\,s for vLLM vs.\ 0.025\,s for Sarathi, a 7.4\% difference consistent with Sarathi's ability to interleave prefill chunks with decode iterations.
TPOT (time-per-output-token) is nearly identical (0.0093 vs.\ 0.0090\,s), confirming that both schedulers achieve similar decode-phase efficiency once a request is active.

\textbf{Preemption as a first-class metric.}
The 53 preempted requests under vLLM (26.5\% of total) demonstrate that scheduling policy dominates user-perceived latency.
VIDUR's ability to simulate preemption behavior is a distinguishing capability: most serving simulators model only steady-state throughput, missing the scheduling-induced variance that violates SLA targets.
Absolute values require A100 hardware for verification.

\textbf{Benchmark coverage for inference scenarios.}
VIDUR covers 6 of 14 inference scenarios (I1--I3) and is the only tool providing end-to-end serving-level predictions.
For scenario I2.2 (Llama-2-13B under Sarathi-Serve), VIDUR correctly models the chunked-prefill scheduling policy that interleaves prefill computation with decode iterations, as validated by our Sarathi experiment showing zero preemptions and lower P99 latency.
However, for I3.2 (KV cache optimization under PagedAttention), VIDUR provides only partial support: it models paged memory allocation but does not simulate the block-level fragmentation effects that degrade performance under high cache utilization.
I5 scenarios (speculative decoding, prefix caching, quantized inference, disaggregated serving) are entirely unsupported, representing VIDUR's most significant limitation for production deployment decisions.

\subsection{Timeloop: Accelerator Energy/Performance}
\label{subsec:timeloop-results}

Timeloop reports accuracy within 10\% of RTL simulation for energy, validated against Eyeriss silicon~\cite{timeloop2019}.
We ran ResNet-50 Conv1 on an Eyeriss-like architecture:

\begin{itemize}
    \item Total energy: 649.08~\textmu{}J (5,500~fJ/MAC) with DRAM dominating (61.8\%), followed by weights SPAD (18.4\%) and MAC (3.8\%)
    \item Estimated latency: 5.854~ms at $\sim$60\% utilization (168 PEs, 702,464 ideal cycles)
    \item Outputs are deterministic and bit-identical across three runs
\end{itemize}

The energy breakdown structure matches published Eyeriss data~\cite{eyeriss2016}: DRAM dominance and small MAC energy fraction are characteristic of data-movement-dominated architectures.

\textbf{Energy breakdown validates data-movement-dominated design thesis.}
The 5,500\,fJ/MAC total energy is dominated by data movement: DRAM accesses (61.8\%), weight SPAD (18.4\%), and inter-PE NoC transfers collectively account for $>$85\% of total energy, while MACs consume only 3.8\%.
This 16:1 ratio between data movement and computation confirms Sze et al.'s hierarchy~\cite{sze2017efficient} and motivates dataflow-centric design exploration.
Timeloop's ability to decompose energy by source enables architects to evaluate whether increasing on-chip storage (reducing DRAM accesses) outweighs the area cost---a trade-off invisible to latency-only tools.
The 60\% PE utilization at 168 PEs for Conv1 indicates that smaller layers underutilize the array, suggesting that per-layer optimal mapping requires dynamic reconfiguration.
The estimated latency of 5.854\,ms at 702,464 ideal cycles further reveals that Conv1---a relatively small $7\times7$ convolution with 64 output channels---leaves significant PE resources idle.
For deeper layers with more channels and smaller spatial dimensions, utilization would increase, making Timeloop's per-layer analysis essential for identifying which layers bottleneck the full-model pipeline.
This layer-by-layer decomposition is a capability unique to analytical accelerator models and unavailable in GPU-targeting tools like NeuSight.

Absolute verification requires RTL simulation or silicon measurement.

\subsection{nn-Meter: Complete Failure}
\label{subsec:nnmeter-results}

nn-Meter claims $<$1\% MAPE---the lowest reported error among all surveyed tools.
After four deployment attempts ($>$4 hours), we obtained \textbf{zero predictions}: pre-trained models serialized with scikit-learn 0.23.1 (2020) cannot be deserialized with current versions.
Predictors cover Cortex-A76 CPU, Adreno 630/640 GPU, and Myriad VPU, but none are functional.
\textbf{The tool claiming the best accuracy is the only tool that produces no output}---pickle serialization without version pinning created an expiration date, rendering the tool unusable within two years.
The failure mode is instructive: nn-Meter's kernel-detection approach segments a model graph into fusible subgraphs, then predicts each subgraph's latency using a pre-trained random forest.
The model weights were serialized using Python's \texttt{pickle} module, which offers no cross-version compatibility guarantees.
When scikit-learn's internal representation changed (versions 0.23$\rightarrow$1.0+), all four predictors became unloadable.
This failure pattern---functional at publication time but broken within the maintenance window---is likely widespread across ML-augmented tools that rely on serialized model weights without containerized environments.
Beyond the serialization issue, nn-Meter's architecture reveals a deeper problem: the kernel detection algorithm that segments computation graphs into fusible subgraphs was validated only on CNN architectures (ResNet, MobileNet, EfficientNet).
Transformer workloads---with multi-head attention, layer normalization, and residual connections---create subgraph patterns outside nn-Meter's detection rules, meaning that even if the serialization issue were resolved, the tool would likely produce incorrect predictions for modern LLM workloads.

\subsection{Benchmark Suite Coverage}
\label{subsec:benchmark-coverage}

Table~\ref{tab:benchmark-coverage} evaluates each tool against our 28-scenario LLM benchmark suite.
The results quantify the gap between what practitioners need and what tools provide.

\begin{table}[t]
\centering
\caption{Tool coverage of LLM benchmark suite (28 scenarios). S=Supported, P=Partial, U=Unsupported. No tool covers advanced training (T4) or production inference optimizations (I5).}
\label{tab:benchmark-coverage}
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Category} & \textbf{\#} & \textbf{Neu.} & \textbf{AST.} & \textbf{VID.} & \textbf{TL} & \textbf{nn-M} \\
\midrule
T1: Data parallel  & 3 & 2P & 3S & --- & --- & --- \\
T2: Tensor parallel & 2 & 2P & 2S & --- & --- & --- \\
T3: Pipeline parallel & 2 & 2P & 2S & --- & --- & --- \\
T4: Advanced train. & 4 & --- & 2P & --- & --- & --- \\
\midrule
I1: Single request & 3 & 2S,1P & --- & 2S,1P & --- & --- \\
I2: Batched serving & 3 & --- & --- & 3S & --- & --- \\
I3: KV cache & 2 & --- & --- & 1S,1P & --- & --- \\
I4: Multi-model & 1 & --- & --- & --- & --- & --- \\
I5: Production opt. & 4 & --- & --- & --- & --- & --- \\
\midrule
\textbf{Supported} & & 5 & 7 & 6 & 0 & 0 \\
\textbf{Partial} & & 3 & 2 & 2 & 0 & 0 \\
\textbf{Coverage} & & 18\% & 25\% & 21\% & 0\% & 0\% \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:coverage-heatmap} provides a visual summary of the coverage gaps, showing the sparse and disjoint nature of tool support across benchmark categories.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[
    cell/.style={minimum width=1.1cm, minimum height=0.55cm, align=center, font=\tiny\bfseries, inner sep=0pt},
    hdr/.style={font=\tiny\bfseries, align=center},
]

% Column headers
\node[hdr] at (0,0.4) {Category};
\node[hdr] at (1.3,0.4) {NeuSight};
\node[hdr] at (2.5,0.4) {ASTRA};
\node[hdr] at (3.7,0.4) {VIDUR};
\node[hdr] at (4.9,0.4) {Timeloop};
\node[hdr] at (6.1,0.4) {nn-Meter};

% Row labels and cells
% T1
\node[hdr, anchor=east] at (0.4,-0.1) {T1};
\node[cell, fill=yellow!40] at (1.3,-0.1) {P};
\node[cell, fill=green!40] at (2.5,-0.1) {S};
\node[cell, fill=red!15] at (3.7,-0.1) {U};
\node[cell, fill=red!15] at (4.9,-0.1) {U};
\node[cell, fill=red!15] at (6.1,-0.1) {U};

% T2
\node[hdr, anchor=east] at (0.4,-0.65) {T2};
\node[cell, fill=yellow!40] at (1.3,-0.65) {P};
\node[cell, fill=green!40] at (2.5,-0.65) {S};
\node[cell, fill=red!15] at (3.7,-0.65) {U};
\node[cell, fill=red!15] at (4.9,-0.65) {U};
\node[cell, fill=red!15] at (6.1,-0.65) {U};

% T3
\node[hdr, anchor=east] at (0.4,-1.2) {T3};
\node[cell, fill=yellow!40] at (1.3,-1.2) {P};
\node[cell, fill=green!40] at (2.5,-1.2) {S};
\node[cell, fill=red!15] at (3.7,-1.2) {U};
\node[cell, fill=red!15] at (4.9,-1.2) {U};
\node[cell, fill=red!15] at (6.1,-1.2) {U};

% T4
\node[hdr, anchor=east] at (0.4,-1.75) {T4};
\node[cell, fill=red!15] at (1.3,-1.75) {U};
\node[cell, fill=yellow!40] at (2.5,-1.75) {P};
\node[cell, fill=red!15] at (3.7,-1.75) {U};
\node[cell, fill=red!15] at (4.9,-1.75) {U};
\node[cell, fill=red!15] at (6.1,-1.75) {U};

% Separator line
\draw[black!30, thin] (-0.2,-2.08) -- (6.7,-2.08);

% I1
\node[hdr, anchor=east] at (0.4,-2.3) {I1};
\node[cell, fill=green!40] at (1.3,-2.3) {S};
\node[cell, fill=red!15] at (2.5,-2.3) {U};
\node[cell, fill=green!40] at (3.7,-2.3) {S};
\node[cell, fill=red!15] at (4.9,-2.3) {U};
\node[cell, fill=red!15] at (6.1,-2.3) {U};

% I2
\node[hdr, anchor=east] at (0.4,-2.85) {I2};
\node[cell, fill=red!15] at (1.3,-2.85) {U};
\node[cell, fill=red!15] at (2.5,-2.85) {U};
\node[cell, fill=green!40] at (3.7,-2.85) {S};
\node[cell, fill=red!15] at (4.9,-2.85) {U};
\node[cell, fill=red!15] at (6.1,-2.85) {U};

% I3
\node[hdr, anchor=east] at (0.4,-3.4) {I3};
\node[cell, fill=red!15] at (1.3,-3.4) {U};
\node[cell, fill=red!15] at (2.5,-3.4) {U};
\node[cell, fill=yellow!40] at (3.7,-3.4) {P};
\node[cell, fill=red!15] at (4.9,-3.4) {U};
\node[cell, fill=red!15] at (6.1,-3.4) {U};

% I4
\node[hdr, anchor=east] at (0.4,-3.95) {I4};
\node[cell, fill=red!15] at (1.3,-3.95) {U};
\node[cell, fill=red!15] at (2.5,-3.95) {U};
\node[cell, fill=red!15] at (3.7,-3.95) {U};
\node[cell, fill=red!15] at (4.9,-3.95) {U};
\node[cell, fill=red!15] at (6.1,-3.95) {U};

% I5
\node[hdr, anchor=east] at (0.4,-4.5) {I5};
\node[cell, fill=red!15] at (1.3,-4.5) {U};
\node[cell, fill=red!15] at (2.5,-4.5) {U};
\node[cell, fill=red!15] at (3.7,-4.5) {U};
\node[cell, fill=red!15] at (4.9,-4.5) {U};
\node[cell, fill=red!15] at (6.1,-4.5) {U};

% Grid lines
\foreach \y in {0.15,-0.4,-0.95,-1.5,-2.08,-2.58,-3.13,-3.68,-4.23,-4.78} {
    \draw[black!15, thin] (-0.2,\y) -- (6.7,\y);
}
\foreach \x in {0.7,1.9,3.1,4.3,5.5,6.7} {
    \draw[black!15, thin] (\x,0.15) -- (\x,-4.78);
}

% Legend
\node[cell, fill=green!40, minimum width=0.5cm] at (1.3,-5.3) {S};
\node[font=\tiny, anchor=west] at (1.65,-5.3) {Supported};
\node[cell, fill=yellow!40, minimum width=0.5cm] at (3.1,-5.3) {P};
\node[font=\tiny, anchor=west] at (3.45,-5.3) {Partial};
\node[cell, fill=red!15, minimum width=0.5cm] at (4.9,-5.3) {U};
\node[font=\tiny, anchor=west] at (5.25,-5.3) {Unsupported};

\end{tikzpicture}%
}
\caption{Tool$\times$workload coverage heatmap for the 28-scenario LLM benchmark suite. Training categories T1--T4 and inference categories I1--I5. Green=supported, yellow=partial, red=unsupported. Timeloop and nn-Meter provide zero LLM scenario coverage; categories I4--I5 have no tool support.}
\label{fig:coverage-heatmap}
\end{figure}

\textbf{Half of LLM workloads have zero tool coverage.}
Of 28 scenarios, 14 (50\%) are not addressable by any evaluated tool.
The entirely uncovered scenarios include FP8 mixed-precision training (T4.1), LoRA fine-tuning (T4.2), speculative decoding (I5.1), prefix caching (I5.2), INT4 quantized inference (I5.3), disaggregated serving (I5.4), and multi-model co-location (I4.1).
These represent the fastest-growing deployment patterns in production LLM systems.
Sequence parallelism (T4.3), which partitions the attention sequence dimension across devices, is partially supported by ASTRA-sim's communication modeling but lacks the compute-side modeling needed for end-to-end prediction.

\textbf{Tools cover disjoint slices with minimal overlap.}
ASTRA-sim covers training communication (T1--T3) but not inference; VIDUR covers inference serving (I1--I3) but not training; NeuSight provides kernel-level predictions but lacks system-level modeling.
Only 3 scenarios (I1.1, I1.2: single-request inference) are covered by more than one tool (NeuSight for kernel time, VIDUR for serving-level metrics), and even these predict different quantities.
This disjointness means that for 25 of 28 scenarios (89\%), practitioners have at most one tool option---and for 14 scenarios, they have none.
The practical consequence is that no single tool can answer end-to-end deployment questions like ``What throughput will Llama-2-70B achieve on 32$\times$H100 with tensor parallelism under Sarathi-Serve at QPS~8?''---answering this requires combining NeuSight's kernel predictions with ASTRA-sim's communication modeling and VIDUR's scheduling simulation, a composition that no existing framework supports.

\textbf{Modern techniques are the largest gap.}
Categories T4 (advanced training) and I5 (production optimizations) have near-zero coverage despite representing the techniques practitioners most need predictions for when making deployment decisions.
MoE expert parallelism (T4.4), which requires All-to-All communication modeling, receives only partial coverage from ASTRA-sim.
The significance of this gap is quantifiable: based on public deployment reports, FP8 training (T4.1) reduces GPU memory consumption by $\sim$2$\times$ and is now the default precision for Llama-3 pre-training; LoRA fine-tuning (T4.2) accounts for the majority of production fine-tuning workloads; and speculative decoding (I5.1) is deployed in production at multiple LLM serving providers.
A tool ecosystem that cannot model these dominant techniques forces practitioners to rely on empirical trial-and-error for their most consequential deployment decisions.

\textbf{Per-scenario gap analysis.}
The 14 entirely uncovered scenarios cluster into three groups.
\emph{Training-side gaps} (T4.1--T4.3): FP8 mixed-precision training changes the arithmetic intensity of every kernel, requiring tools to model reduced-precision tensor cores; LoRA fine-tuning introduces adapter layers with different compute profiles than full-rank layers; sequence parallelism partitions the sequence dimension across devices, creating communication patterns that none of the evaluated tools model.
\emph{Inference-side gaps} (I5.1--I5.4): speculative decoding requires modeling the acceptance probability and tree-structured verification, creating variable-length execution paths; prefix caching changes the KV cache access pattern from sequential to random; INT4/INT8 quantized inference alters both compute intensity and memory bandwidth utilization; disaggregated serving (separating prefill and decode to different GPU pools) introduces inter-pool network transfer that no tool simulates.
\emph{Multi-model gaps} (I4.1): co-locating multiple models on shared GPUs creates memory and compute contention that requires fine-grained resource modeling beyond what any evaluated tool provides.

\textbf{Failure mode taxonomy for uncovered scenarios.}
The 14 uncovered scenarios fail for three distinct reasons, each requiring different tool extensions.
\emph{Missing algorithmic primitives}: speculative decoding (I5.1) and prefix caching (I5.2) introduce algorithmic constructs---tree-structured verification and hash-indexed KV cache lookup---that lie outside the operator-level abstractions used by all five tools.
Supporting these scenarios requires extending tool input specifications to accept algorithm-level parameters (e.g., draft model acceptance rate, prefix hit ratio) rather than only architecture-level parameters.
\emph{Missing hardware models}: FP8 training (T4.1) and INT4 inference (I5.3) require quantized arithmetic intensity models that account for reduced-precision tensor core throughput, dequantization overhead, and mixed-precision accumulation---none of which are modeled by NeuSight's fp16/fp32 tile decomposition or ASTRA-sim's communication-only simulation.
\emph{Missing system-level interactions}: disaggregated serving (I5.4) and multi-model co-location (I4.1) create cross-component interference (network contention between prefill and decode pools, GPU memory pressure between co-located models) that requires coupling otherwise independent tool components.

\textbf{Coverage concentration.}
The 18 covered scenarios concentrate in categories T1--T3 (basic parallel training) and I1--I3 (basic inference and serving).
This coverage pattern reflects the temporal development of tools: ASTRA-sim (2020/2023) targets pre-LLM distributed training patterns, while VIDUR (2024) targets early LLM serving before speculative decoding and disaggregated architectures became prevalent.
The field's tool development lags deployment practice by 1--2 years.
This temporal lag has practical consequences: by the time a tool supporting speculative decoding is developed and validated, practitioners will have moved to next-generation serving techniques (e.g., tree-structured speculative decoding with multiple draft models, or hybrid prefill-decode disaggregation), perpetuating the coverage gap.
Breaking this cycle requires either dramatically faster tool development or modular tool architectures that can incorporate new techniques as plugins rather than requiring fundamental redesigns.

\textbf{Aggregate coverage by tool.}
Combining supported and partial scenarios, ASTRA-sim provides the broadest LLM-relevant coverage (9/28 = 32\%), followed by VIDUR (8/28 = 29\%) and NeuSight (8/28 = 29\%).
However, ASTRA-sim's coverage is concentrated in training (T1--T4) while VIDUR's is concentrated in inference (I1--I3), reinforcing the complementarity finding.
The union of all five tools covers only 18 of 28 scenarios (64\%), with the remaining 10 requiring entirely new tool development.
Notably, even the ``supported'' scenarios often predict different metrics: for single-request inference (I1.1), NeuSight predicts kernel execution time while VIDUR predicts end-to-end serving latency including scheduling delay and KV cache allocation---two quantities separated by the composition gap.

\textbf{Coverage quality varies within ``supported'' scenarios.}
Even among the 18 covered scenarios, support quality is uneven.
For T1.1 (data-parallel GPT-2 on 8$\times$A100), NeuSight provides only per-GPU kernel time (partial) while ASTRA-sim provides full communication modeling (supported)---but neither tool produces the end-to-end iteration time that practitioners optimize.
For I2.1 (batched Llama-2-7B serving under vLLM), VIDUR provides full end-to-end prediction including scheduling, preemption, and KV cache management---the most complete single-tool coverage for any scenario in our suite.
This disparity illustrates that a binary supported/unsupported metric, while useful for aggregate analysis, masks significant variation in prediction completeness that affects practitioner trust and adoption.

\subsection{Cross-Cutting Findings}
\label{subsec:cross-cutting-findings}

Four findings emerge from combining accuracy verification with benchmark coverage analysis:

\emph{First}, \textbf{self-reported accuracy is inversely correlated with reliability.}
By claimed accuracy: nn-Meter ($<$1\%) $>$ NeuSight (2.3\%) $>$ VIDUR ($<$5\%) $>$ Timeloop (5--10\%) $>$ ASTRA-sim (5--15\%).
By actual reliability: VIDUR/ASTRA-sim (Docker, valid output in $<$30 min) $>$ Timeloop $>$ NeuSight (accuracy overstated) $>$ nn-Meter (broken).
The tools claiming the lowest error are the least reliable.

\emph{Second}, \textbf{the five tools are complementary, not competing.}
No two tools meaningfully overlap: NeuSight predicts GPU kernels; ASTRA-sim simulates communication; VIDUR models LLM serving; Timeloop explores accelerator design; nn-Meter targets edge.
The field needs a \emph{unified pipeline} combining tool strengths (Section~\ref{sec:unified-pipeline}).

\emph{Third}, \textbf{the composition gap dominates end-to-end error.}
NeuSight's kernel-level 5--9\% MAPE grows to 10--28\% at model level.
The 5--15\% composition error---launch overhead, memory allocation, synchronization---is \emph{larger than kernel-level error}.
Improving kernel predictors has diminishing returns until composition is solved (Figure~\ref{fig:error-composition}).

\emph{Fourth}, \textbf{50\% of modern LLM workloads lack any modeling tool.}
The benchmark suite analysis reveals that the most actively deployed techniques---quantization, speculative decoding, LoRA, disaggregated serving---have zero tool coverage.
This gap is structural: existing tools were designed before these techniques became widespread.

\emph{Fifth}, \textbf{deployment robustness varies inversely with model complexity.}
Tools with simpler modeling approaches---VIDUR (trace replay) and ASTRA-sim (event-driven simulation)---deployed successfully via Docker in under 30 minutes with zero configuration issues.
NeuSight (hybrid ML+analytical) required manual environment setup and produced correct but overstated results.
nn-Meter (pure ML-augmented) failed entirely.
Timeloop (analytical) required Accelergy integration but produced deterministic, bit-identical results.
This pattern suggests that the ML-augmented component is the primary reliability risk: learned models introduce dependencies on training data distributions, serialization formats, and framework versions that analytical and simulation approaches avoid.
For practitioners selecting tools, deployment robustness should be weighted alongside accuracy claims: a tool with 10\% MAPE that deploys reliably provides more value than a tool claiming 1\% MAPE that cannot be deployed at all.

\emph{Sixth}, \textbf{inference and training accuracy diverge systematically.}
Across NeuSight's 146 configurations, inference accuracy (mean MAPE: 5.87--27.10\% depending on device) is consistently better than training accuracy for NVIDIA GPUs (V100: 5.87\% inf vs.\ 8.91\% train; A100-80G: 8.63\% inf vs.\ 7.59\% train is the only exception).
For AMD GPUs, the gap is larger: MI100 shows 10.80\% inference vs.\ 15.62\% training; MI210 shows 8.40\% vs.\ 15.73\%.
Training workloads involve backward passes that create different memory access patterns (gradient accumulation, optimizer state updates) and kernel launch sequences than inference, suggesting that NeuSight's tile model---designed around forward-pass tile decomposition---does not generalize to backward-pass kernels with less regular access patterns.
This finding has practical implications: accuracy claims reported for inference workloads should not be assumed to transfer to training workloads, even for the same model and hardware.
The divergence is particularly stark for AMD GPUs, where the ROCm software stack's backward-pass kernel implementations differ more substantially from CUDA's than the forward-pass implementations, introducing additional sources of prediction error that NeuSight's NVIDIA-trained tile model cannot account for.

\emph{Seventh}, \textbf{model architecture affects prediction difficulty non-uniformly.}
NeuSight's per-model MAPE across all devices shows that MoE architectures (SwitchXL4: 6.33--17.65\% APE range across configurations) exhibit higher variance than dense models (OPT-13B: 0.38--10.53\%; GPT-3-2.7B: 0.43--7.73\%).
The higher variance for MoE arises because expert routing creates workload-dependent computation patterns that a static tile decomposition cannot fully capture.
This observation extends to future tools: MoE, sparse attention, and dynamic architectures will likely require workload-aware prediction mechanisms rather than architecture-only models.

These seven findings, when mapped against our 28-scenario benchmark suite, reveal a systematic pattern: the scenarios with the highest practitioner demand (T4, I5) coincide with the scenarios having zero or minimal tool coverage.
Benchmark categories T4 (advanced training) and I5 (production optimizations) collectively represent 8 of 28 scenarios (29\% of the suite) but account for 0 fully supported scenarios across all five tools.
Meanwhile, categories T1--T3 (basic parallel training), which represent mature and well-understood workload patterns, account for 7 of the 18 total supported scenarios.
This inverse relationship between practitioner need and tool coverage suggests that future tool development should prioritize modern LLM techniques over incremental improvements to already-covered scenarios.
Concretely, a tool achieving even 20\% MAPE on speculative decoding (I5.1) or disaggregated serving (I5.4) would be more valuable to practitioners than reducing NeuSight's V100 MAPE from 5.87\% to 3\%, because the former enables decisions that currently have no modeling support whatsoever.
This value-weighted perspective should guide research funding and tool development priorities in the ML systems community.

\subsection{Deployment Experience and Reproducibility}
\label{subsec:deployment-experience}

Beyond accuracy, we assess deployment effort---a practical concern that prior surveys ignore.
Table~\ref{tab:deployment-effort} summarizes our experience deploying each tool from scratch.

\begin{table}[t]
\centering
\caption{Deployment experience for each evaluated tool. Time excludes download. Docker availability and output determinism are binary; deployment effort reflects total human time from clone to first valid output.}
\label{tab:deployment-effort}
\small
\begin{tabular}{lcccl}
\toprule
\textbf{Tool} & \textbf{Docker} & \textbf{Time} & \textbf{Determ.} & \textbf{Failure Mode} \\
\midrule
VIDUR & Yes & $<$30 min & Yes & None \\
ASTRA-sim & Yes & $<$30 min & Yes & None \\
Timeloop & Partial & $\sim$1 hr & Yes & Accelergy setup \\
NeuSight & No & $\sim$2 hr & Yes & Env.\ config \\
nn-Meter & No & 4+ hr & N/A & Serialization \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Docker availability is the strongest predictor of deployment success.}
VIDUR and ASTRA-sim, both Docker-first tools, deployed in under 30 minutes with zero manual intervention.
Timeloop required partial manual setup for its Accelergy energy estimation plugin but produced results within one hour.
NeuSight required manual Python environment configuration and model weight downloads but eventually succeeded.
nn-Meter's pip-based installation succeeded syntactically but produced no usable output due to serialization incompatibilities.
This represents the worst deployment outcome: silent success at install time masking complete failure at inference time, with no diagnostic error message until the user attempts to load a predictor---a failure pattern that undermines trust in the broader ML-augmented tool ecosystem.

\textbf{Determinism varies by methodology.}
All evaluated tools except nn-Meter (which produced no output) generated bit-identical results across three independent runs on the same platform.
This determinism is notable for NeuSight, whose hybrid ML+analytical approach could in principle exhibit stochastic behavior; the determinism arises because NeuSight uses fixed pre-trained weights and analytical tile decomposition with no stochastic inference-time components.
Deterministic outputs simplify regression testing and enable exact reproducibility---properties that should be standard but are not guaranteed by ML-augmented tools that use stochastic inference (e.g., dropout at test time, Monte Carlo sampling for uncertainty quantification).

\subsection{Threats to Validity}
\label{subsec:threats}

\textbf{External validity.}
Our venue-focused search may under-represent industry tools.
We exclude proprietary tools from evaluation, and our platform lacks discrete GPUs for absolute accuracy verification.
The benchmark suite's 28 scenarios, while representative, cannot cover every production deployment pattern; emerging workloads (e.g., retrieval-augmented generation, multi-modal models) are not yet included.

\textbf{Internal validity.}
Our evaluation covers 5 of 22 tools.
Findings rest on single tool instances per methodology type---e.g., nn-Meter may be unrepresentative due to deployment failure.
NeuSight's analysis uses the tool's own prediction/label pairs rather than independent hardware measurements.
The per-device sample sizes vary (3--18 configurations), limiting statistical power for devices with few data points (e.g., P4 with only 3 configurations, A100-SXM with 3 configurations).
We mitigate this by reporting both mean and worst-case APE.
Our benchmark suite covers 28 scenarios, but the distribution is not uniform: training scenarios (11) outnumber inference scenarios (13), with MoE and multi-model scenarios (T4.4, I4.1) represented by only one scenario each.
A more balanced suite might weight scenarios by practitioner frequency of use, but such weighting data is not publicly available.
Despite these limitations, our suite provides the first standardized coverage metric for ML performance tools, enabling future evaluations to quantitatively compare tool ecosystems.

\textbf{Construct validity.}
Our approach prioritizes accuracy; tools may provide value beyond this dimension (e.g., Timeloop's energy breakdown for design insight, ASTRA-sim's what-if analysis for topology exploration).
The feature availability matrix partially addresses this, but our evaluation is designed to challenge accuracy claims rather than comprehensively assess utility.
Additionally, our coverage criterion (supported/partial/unsupported) does not capture the quality of partial support---ASTRA-sim's partial coverage of MoE training (T4.4), for example, provides All-to-All communication modeling but misses expert load balancing effects.
A finer-grained coverage metric---e.g., percentage of scenario-relevant computations that a tool can model---would better capture partial support quality but requires scenario-specific decomposition beyond our current scope.

\textbf{Temporal validity.}
Our evaluation reflects tool state as of January 2026.
Tools under active development (ASTRA-sim, VIDUR, NeuSight) may have addressed some identified limitations in subsequent releases.
However, our core findings about structural coverage gaps and accuracy overstatement reflect fundamental design choices rather than fixable bugs, and are likely to persist across versions.
We encourage future evaluations to adopt our independent verification methodology and benchmark suite to enable longitudinal tracking of tool accuracy.
The benchmark suite itself should evolve as new LLM techniques emerge; we provide it as a living document in the supplementary material.

\textbf{Benchmark suite validity.}
Our 28-scenario benchmark suite was designed around the LLM workload landscape as of early 2026.
Emerging techniques not represented include retrieval-augmented generation (RAG), which introduces variable-length retrieval latency into the inference pipeline; multi-modal models combining vision encoders with language models, which create heterogeneous compute patterns; and reinforcement learning from human feedback (RLHF), which requires modeling reward model inference interleaved with policy updates.
We designed the suite to be extensible: each scenario is specified by a tuple of (model architecture, hardware configuration, parallelism strategy, target metric), allowing new scenarios to be added as techniques mature without restructuring the evaluation framework.
Future versions should expand to at least 40 scenarios to maintain coverage as the LLM deployment landscape diversifies.

% ==============================================================================
% UNIFIED SIMULATION PIPELINE
% ==============================================================================
\section{Toward a Unified Simulation Pipeline}
\label{sec:unified-pipeline}

The feature matrix (Table~\ref{tab:feature-matrix}) reveals disjoint coverage: no single tool spans kernel execution through distributed training to serving SLAs.
We propose a five-layer pipeline composing predictions hierarchically:

\begin{enumerate}
    \item \textbf{Hardware design} (Timeloop): Explore dataflow/mapping design space for per-layer energy and latency on custom accelerators.
    \item \textbf{Kernel prediction} (NeuSight / Timeloop): Predict per-kernel execution time on GPUs (12 types) or custom architectures.
    \item \textbf{Model composition} (\textbf{CRITICAL GAP}): Compose kernel predictions into full iteration time, accounting for launch overhead, memory allocation, and data movement. \emph{No existing tool validates this layer.}
    \item \textbf{Distributed training} (ASTRA-sim): Simulate multi-GPU communication, collectives, and topology effects for training throughput.
    \item \textbf{Serving system} (VIDUR): Model request scheduling, batching, KV cache, and queuing for TTFT/TPOT prediction.
\end{enumerate}

Figure~\ref{fig:pipeline-architecture} illustrates this five-layer pipeline, highlighting the composition gap (Layer~3) that no existing tool addresses.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[
    layer/.style={draw=black!60, rounded corners=3pt, minimum width=5.5cm, minimum height=0.7cm, align=center, font=\scriptsize\bfseries},
    tool/.style={font=\tiny, text=black!60},
    arr/.style={-{Stealth[length=3pt]}, thick, black!50},
    data/.style={font=\tiny\itshape, text=blue!50!black},
]

% Layer 1: Hardware Design
\node[layer, fill=purple!12, draw=purple!40] (hw) at (0,0) {Layer 1: Hardware Design};
\node[tool, anchor=west] at (3.2,0) {Timeloop};

% Layer 2: Kernel Prediction
\node[layer, fill=blue!12, draw=blue!40] (kernel) at (0,1.3) {Layer 2: Kernel Prediction};
\node[tool, anchor=west] at (3.2,1.3) {NeuSight};

% Layer 3: Model Composition (GAP)
\node[layer, fill=red!8, draw=red!60, dashed, line width=1pt] (comp) at (0,2.6) {Layer 3: Model Composition};
\node[font=\tiny\bfseries, text=red!60!black, anchor=west] at (3.2,2.6) {CRITICAL GAP};

% Layer 4: Distributed Training
\node[layer, fill=orange!12, draw=orange!40] (dist) at (0,3.9) {Layer 4: Distributed Training};
\node[tool, anchor=west] at (3.2,3.9) {ASTRA-sim};

% Layer 5: Serving System
\node[layer, fill=green!12, draw=green!40] (serve) at (0,5.2) {Layer 5: Serving System};
\node[tool, anchor=west] at (3.2,5.2) {VIDUR};

% Arrows between layers
\draw[arr] (hw) -- node[data, right, xshift=2pt] {energy, latency} (kernel);
\draw[arr] (kernel) -- node[data, right, xshift=2pt] {per-kernel time} (comp);
\draw[arr, draw=red!50, dashed] (comp) -- node[data, right, xshift=2pt, text=red!50!black] {model iteration time} (dist);
\draw[arr] (dist) -- node[data, right, xshift=2pt] {per-device throughput} (serve);

% Error annotations (left side)
\node[font=\tiny, text=black!50, anchor=east] at (-3.2,0) {5--10\% RTL err};
\node[font=\tiny, text=black!50, anchor=east] at (-3.2,1.3) {2--9\% MAPE};
\node[font=\tiny, text=red!60!black, anchor=east] at (-3.2,2.6) {\textbf{5--15\% gap}};
\node[font=\tiny, text=black!50, anchor=east] at (-3.2,3.9) {5--15\% geo.};
\node[font=\tiny, text=black!50, anchor=east] at (-3.2,5.2) {$<$5\% err};

% Brace for composition gap
\draw[decorate, decoration={brace, amplitude=4pt, mirror}, thick, red!50] (-3.4,2.2) -- (-3.4,3.0);
\node[font=\tiny, text=red!50!black, anchor=east] at (-3.8,2.6) {unmodeled};

\end{tikzpicture}%
}
\caption{Proposed unified simulation pipeline across five layers. Layer~3 (model composition, dashed red border) is the critical gap: no existing tool validates the kernel-to-model composition step, where 5--15\% error is introduced by unmodeled inter-kernel overheads.}
\label{fig:pipeline-architecture}
\end{figure}

\textbf{The critical gap: kernel-to-model composition.}
NeuSight's 5--9\% kernel MAPE grows to 10--28\% at model level via launch overhead ($\sim$5--10\,\textmu{}s/kernel), inter-kernel data movement, and synchronization---a gap \emph{larger than kernel-level error}.

\textbf{Integration requirements.}
Realizing this pipeline requires: (a)~a common workload format; (b)~validated composition models with formal error bounds; and (c)~cross-hardware transfer methods (accuracy degrades $3$--$4\times$ outside training distributions).

% ==============================================================================
% OPEN CHALLENGES
% ==============================================================================
\section{Open Challenges and Future Directions}
\label{sec:challenges}

Our evaluation exposes six research directions.

\textbf{1. Bridging the composition gap.}
Kernel errors of 2--3\% yield 5--12\% model-level error (Figure~\ref{fig:error-composition}; $\sigma_{\text{model}} \approx \sigma_{\text{kernel}} \cdot \sqrt{N}$ uncorrelated, linear when correlated), yet no validated composition pipeline exists.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[box/.style={draw=black!60, rounded corners=2pt, minimum width=1.2cm, minimum height=0.5cm, align=center, font=\tiny}, err/.style={font=\tiny\itshape, text=red!60!black}, arr/.style={-{Stealth[length=3pt]}, thick, black!50}, brace/.style={decorate, decoration={brace, amplitude=4pt, mirror}, thick, black!40}]
\node[box, fill=blue!10] (k1) at (0,0) {Conv}; \node[box, fill=blue!10] (k2) at (1.5,0) {Attn}; \node[box, fill=blue!10] (k3) at (3,0) {FFN}; \node[box, fill=blue!10] (k4) at (4.5,0) {Norm}; \node[box, fill=blue!10] (k5) at (6,0) {Softmax};
\node[err] at (0,-0.55) {2.3\%}; \node[err] at (1.5,-0.55) {2.1\%}; \node[err] at (3,-0.55) {1.8\%}; \node[err] at (4.5,-0.55) {3.5\%}; \node[err] at (6,-0.55) {2.0\%};
\node[font=\scriptsize\bfseries, anchor=east] at (-0.8,0) {Kernel};
\draw[arr] (0.5,0) -- (1,0); \draw[arr] (2,0) -- (2.5,0); \draw[arr] (3.5,0) -- (4,0); \draw[arr] (5,0) -- (5.5,0);
\node[box, fill=yellow!20, dashed, draw=orange!60] (h1) at (0.75,0.8) {Launch\\overhead}; \node[box, fill=yellow!20, dashed, draw=orange!60] (h2) at (2.25,0.8) {Mem\\alloc}; \node[box, fill=yellow!20, dashed, draw=orange!60] (h3) at (3.75,0.8) {Data\\movement}; \node[box, fill=yellow!20, dashed, draw=orange!60] (h4) at (5.25,0.8) {Sync\\barrier};
\node[box, fill=green!10, minimum width=6.5cm] (model) at (3,1.9) {Model-level prediction}; \node[err] at (3,1.4) {5--12\% (hidden overhead accumulates)}; \node[font=\scriptsize\bfseries, anchor=east] at (-0.8,1.9) {Model};
\node[box, fill=orange!10, minimum width=6.5cm] (system) at (3,3.0) {System-level prediction}; \node[err] at (3,2.5) {+ communication, scheduling, contention}; \node[font=\scriptsize\bfseries, anchor=east] at (-0.8,3.0) {System}; \node[err] at (3,3.55) {5--15\% total error};
\draw[brace] (-0.5,-0.8) -- (6.5,-0.8) node[midway, below=5pt, font=\tiny, text=red!60!black] {$\sigma_{\text{model}} \approx \sigma_{\text{kernel}} \cdot \sqrt{N}$ (uncorrelated) to $N \cdot \sigma_{\text{kernel}}$ (correlated)};
\end{tikzpicture}%
}
\caption{Error composition: kernel predictions (2--3\%) accumulate to 5--12\% model-level and 5--15\% system-level error.}
\label{fig:error-composition}
\end{figure}

\textbf{2. Frontier workload coverage.}
MoE, diffusion~\cite{dynamicreasoning2026}, and dynamic inference lack validated tools; scaling laws~\cite{kaplan2020scaling,hoffmann2022chinchilla,scalinglaws2024,scalinglawguide2025} predict loss but not latency (Figure~\ref{fig:workload-coverage}).

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[ybar stacked, bar width=14pt, xlabel={Publication year}, ylabel={Number of tools}, ymin=0, ymax=16, xtick={2016,2018,2020,2022,2024,2026}, xticklabels={2016--17,2018--19,2020--21,2022--23,2024--25,2026}, xticklabel style={font=\scriptsize}, yticklabel style={font=\scriptsize}, xlabel style={font=\small}, ylabel style={font=\small}, legend style={at={(0.02,0.98)}, anchor=north west, font=\tiny, draw=none, fill=white, fill opacity=0.9, text opacity=1, legend columns=2}, legend cell align={left}, height=5.5cm, width=8.5cm, enlarge x limits={abs=0.8cm}]
\addplot[fill=blue!50, draw=blue!70] coordinates {(2016,2) (2018,2) (2020,4) (2022,3) (2024,2) (2026,0)};
\addplot[fill=orange!50, draw=orange!70] coordinates {(2016,0) (2018,1) (2020,0) (2022,2) (2024,3) (2026,0)};
\addplot[fill=red!50, draw=red!70] coordinates {(2016,0) (2018,0) (2020,1) (2022,1) (2024,8) (2026,2)};
\addplot[fill=green!50, draw=green!70] coordinates {(2016,0) (2018,0) (2020,1) (2022,0) (2024,2) (2026,1)};
\legend{CNN only, CNN+Transformer, LLM/Distributed, Cross-workload}
\end{axis}
\end{tikzpicture}%
}
\caption{Workload coverage by publication period. MoE and diffusion models remain uncharacterized.}
\label{fig:workload-coverage}
\end{figure}

\textbf{3. Hardware transfer.}
Cross-family transfer (GPU$\rightarrow$TPU$\rightarrow$PIM) remains unsolved; PIM~\cite{upimulator2024,attacc2024,neupims2024,paise2025}, chiplets, and disaggregated designs blur memory hierarchy assumptions.

\textbf{4. Network simulation fidelity.}
Current tools~\cite{astrasim2023,triosim2025} use analytical network abstractions, omitting packet-level congestion and tail-latency effects captured by NS-3~\cite{ns3_2010} at orders-of-magnitude slowdown.
Quantifying when packet-level fidelity matters for ML collectives remains unexplored.

\textbf{5. Standardized evaluation infrastructure.}
No MLPerf~\cite{mlperf_training2020,mlperf_inference2020} equivalent exists for prediction; the community needs common benchmarks, portable formats (ONNX, Chakra~\cite{chakra2023}), and Docker-first deployment.

\textbf{6. Temporal stability.}
Software evolution (FlashAttention~\cite{flashattention2022}, CUDA updates) silently invalidates models; nn-Meter's failure within two years underscores the need for continuous validation~\cite{mlperfpower2025}.

% ==============================================================================
% CONCLUSION
% ==============================================================================
\section{Conclusion}
\label{sec:conclusion}

We survey 22 ML performance modeling tools and independently evaluate five against a 28-scenario LLM benchmark suite.
Self-reported accuracy is unreliable (NeuSight: 2.3\% claimed vs.\ 5.87--27.10\% measured; nn-Meter: no output).
The five tools are complementary but disjoint, motivating a unified pipeline---yet the 5--15\% kernel-to-model composition gap exceeds kernel-level error, and 50\% of modern LLM techniques lack any tool support.
The most pressing needs are validated composition models, benchmark-driven development for uncovered scenarios, and continuous validation.

%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
