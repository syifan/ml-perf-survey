% MICRO 2026 Survey Paper - ML Performance Models
% Template based on IEEE conference format

\documentclass[conference]{IEEEtran}

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}

% Custom commands
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\begin{document}

\title{A Survey of Machine Learning Approaches for\\Computer Architecture Performance Modeling}

\author{
\IEEEauthorblockN{Authors TBD}
\IEEEauthorblockA{Affiliations TBD}
}

\maketitle

% ==============================================================================
% ABSTRACT
% ==============================================================================
\begin{abstract}
\todo{Write abstract summarizing the survey scope, methodology, key findings, and contributions. Target: 150-200 words.}
\end{abstract}

\begin{IEEEkeywords}
machine learning, performance modeling, computer architecture, neural networks, survey
\end{IEEEkeywords}

% ==============================================================================
% INTRODUCTION
% ==============================================================================
\section{Introduction}
\label{sec:introduction}

Performance modeling is fundamental to computer architecture research and development.
Architects rely on accurate performance predictions to navigate vast design spaces, optimize hardware-software co-design, and make informed decisions about resource allocation.
Traditional approaches---analytical models~\cite{williams2009roofline} and cycle-accurate simulators~\cite{binkert2011gem5}---have served the community well, but face growing challenges as workloads and hardware become increasingly complex.
Analytical models often oversimplify system behavior, while simulators can require hours or days to evaluate a single design point, making exhaustive exploration impractical.

The rise of deep learning workloads has intensified these challenges.
Modern neural networks exhibit diverse computational patterns---from dense matrix operations in transformers to sparse irregular accesses in graph neural networks---that stress traditional modeling assumptions.
Simultaneously, hardware diversity has exploded: GPUs, TPUs, custom accelerators, and multi-device distributed systems each present unique performance characteristics that resist unified analytical treatment.
This complexity has motivated a new generation of \emph{machine learning-based} performance models that learn predictive functions directly from profiling data.

ML-based performance modeling has emerged as a compelling alternative.
Learned models can capture complex, non-linear relationships between workload characteristics and hardware behavior that elude closed-form analysis.
Recent work demonstrates remarkable accuracy: NeuSight~\cite{neusight2025} achieves 2.3\% error predicting GPT-3 latency on H100 GPUs, while nn-Meter~\cite{nnmeter2021} reaches 99\% accuracy for edge device latency prediction.
Beyond accuracy, these approaches offer practical benefits: models trained on one platform can transfer to new hardware with minimal adaptation~\cite{litepred2024}, and inference-time predictions complete in milliseconds rather than hours.

This survey provides a comprehensive analysis of ML-based performance modeling techniques for computer architecture.
We make the following contributions:
\begin{itemize}
    \item A \textbf{taxonomy} organizing approaches along eight dimensions: modeling technique, target hardware, workload types, prediction targets, accuracy metrics, input requirements, evaluation scope, and reproducibility.
    \item A \textbf{systematic survey} of over 60 papers from architecture venues (MICRO, ISCA, HPCA, ASPLOS) and ML venues (MLSys, NeurIPS, ICML) published between 2016--2025.
    \item A \textbf{comparative analysis} examining trade-offs between accuracy, training cost, generalization, and interpretability across approaches.
    \item An identification of \textbf{open challenges} including data scarcity, cross-platform generalization, and integration with design automation flows.
\end{itemize}

The remainder of this paper is organized as follows.
Section~\ref{sec:background} provides background on traditional performance modeling and relevant ML techniques.
Section~\ref{sec:taxonomy} presents our classification taxonomy.
Section~\ref{sec:survey} surveys approaches organized by target hardware platform.
Section~\ref{sec:comparison} offers comparative analysis across key dimensions.
Section~\ref{sec:challenges} discusses open challenges and future directions.
Section~\ref{sec:conclusion} concludes.

% ==============================================================================
% BACKGROUND
% ==============================================================================
\section{Background}
\label{sec:background}

\subsection{Traditional Performance Modeling}
\label{subsec:traditional-modeling}

Performance modeling has traditionally relied on two complementary approaches: analytical models and cycle-accurate simulation.
This section reviews both paradigms and their limitations, motivating the emergence of ML-based alternatives.

\subsubsection{Analytical Models}

Analytical models express performance as closed-form functions of hardware and workload parameters.
The roofline model~\cite{williams2009roofline} exemplifies this approach, bounding attainable performance by peak compute throughput and memory bandwidth.
Given operational intensity $I$ (FLOP/byte), the roofline predicts performance as $P = \min(\pi, \beta \cdot I)$, where $\pi$ is peak FLOPS and $\beta$ is memory bandwidth.
Despite its simplicity, roofline reasoning guides optimization by revealing compute-bound versus memory-bound regimes.

For DNN accelerators, analytical cost models have become standard practice.
Timeloop~\cite{timeloop2019} models data movement across memory hierarchies for any given mapping (loop order and tiling), computing access counts and energy from architectural parameters.
MAESTRO~\cite{maestro2019} provides a data-centric framework that derives performance from dataflow descriptions.
Sparseloop~\cite{sparseloop2022} extends this methodology to sparse tensor operations, achieving 2000$\times$ speedup over RTL simulation while maintaining accuracy.

Analytical models offer several advantages: fast evaluation (microseconds per design point), interpretability (designers can trace predictions to specific terms), and extrapolation to unseen configurations.
However, they require manual derivation for each target architecture, struggle to capture complex microarchitectural effects (contention, pipeline stalls, caching behavior), and may oversimplify non-linear interactions.

\subsubsection{Cycle-Accurate Simulation}

Cycle-accurate simulators model hardware at register-transfer level, faithfully reproducing timing behavior.
General-purpose simulators like gem5~\cite{binkert2011gem5} support flexible configuration of CPU cores, caches, memory controllers, and interconnects.
For GPUs, simulators such as GPGPU-Sim~\cite{gpgpusim2009} and Accel-Sim~\cite{accelsim2020} model SIMT execution, warp scheduling, and memory coalescing.

Cycle-accurate simulation achieves high fidelity---typically within 5--15\% of real hardware~\cite{binkert2011gem5}---and supports detailed microarchitectural studies.
However, simulation speed presents a fundamental limitation: evaluating a single ResNet-50 inference may require hours, making design space exploration impractical.
ASTRA-sim~\cite{astrasim2023} addresses distributed training at scale through analytical abstractions, but even coarse-grained simulation struggles with the combinatorial explosion of modern ML workloads and hardware configurations.

\subsubsection{The Modeling Gap}

Neither approach fully addresses modern performance modeling needs.
Analytical models are fast but imprecise for complex microarchitectures.
Simulators are accurate but too slow for iterative design.
This tension has intensified as ML workloads diversify (from CNNs to transformers to mixture-of-experts models) and hardware specializes (GPUs, TPUs, custom accelerators).
ML-based performance models offer a middle path: learning complex relationships from profiling data while enabling millisecond-scale inference.

\subsection{Machine Learning Fundamentals}
\label{subsec:ml-fundamentals}

This section provides a brief primer on ML techniques frequently employed in performance modeling, establishing terminology used throughout the survey.

\subsubsection{Classical Machine Learning}

Linear regression and its regularized variants (ridge, LASSO) remain widely used for performance prediction due to their simplicity and interpretability.
Given feature vector $\mathbf{x}$ (e.g., operator parameters, hardware counters), linear models predict $\hat{y} = \mathbf{w}^\top \mathbf{x} + b$.
While unable to capture non-linear relationships, linear models provide baselines and feature importance rankings.

Tree-based ensembles---random forests and gradient boosted trees (XGBoost, LightGBM)---handle non-linearities through recursive partitioning.
These methods dominate when training data is limited ($<$10K samples) and features are well-engineered, often outperforming deep learning in low-data regimes~\cite{nnmeter2021}.

\subsubsection{Deep Learning}

Multi-layer perceptrons (MLPs) learn hierarchical feature representations through stacked non-linear transformations: $\mathbf{h}_{i+1} = \sigma(\mathbf{W}_i \mathbf{h}_i + \mathbf{b}_i)$.
MLPs require minimal feature engineering but need sufficient training data and careful regularization to avoid overfitting.

Recurrent neural networks (RNNs) and their gated variants (LSTM, GRU) process sequential inputs, making them suitable for modeling operator sequences in neural network execution graphs.
However, sequential processing limits parallelization and can miss long-range dependencies.

\subsubsection{Graph Neural Networks}

Graph neural networks (GNNs) operate on graph-structured data through message passing.
For a node $v$ with features $\mathbf{h}_v$, GNNs iteratively update representations by aggregating information from neighbors $\mathcal{N}(v)$:
\begin{equation}
\mathbf{h}_v^{(k+1)} = \phi\left(\mathbf{h}_v^{(k)}, \bigoplus_{u \in \mathcal{N}(v)} \psi(\mathbf{h}_u^{(k)}, \mathbf{e}_{uv})\right)
\end{equation}
where $\phi$ and $\psi$ are learnable functions and $\oplus$ is a permutation-invariant aggregation (sum, mean, max).

GNNs are particularly appealing for performance modeling because DNN computation graphs have natural graph structure.
Nodes represent operators with features (type, parameters), edges represent data dependencies with features (tensor shapes, datatypes).
GNNs can learn to propagate performance-relevant information along these dependencies~\cite{granite2022}.

\subsubsection{Attention and Transformers}

Attention mechanisms compute weighted combinations over input elements, with weights determined by learned compatibility functions.
Self-attention allows each position to attend to all other positions:
\begin{equation}
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V}
\end{equation}

Transformers stack self-attention with feedforward networks, enabling long-range dependency modeling without sequential processing.
Recent performance models leverage transformer architectures to capture complex inter-operator interactions across entire computation graphs.

\subsubsection{Transfer Learning}

Transfer learning adapts models trained on one domain (source) to perform well on another (target).
In performance modeling, this enables training on easily-profiled hardware and transferring to new platforms with limited data.
Common approaches include fine-tuning (adjusting pre-trained weights with target data), domain adaptation (learning domain-invariant representations), and meta-learning (learning to adapt quickly from few examples)~\cite{litepred2024}.

\subsection{Problem Formulation}
\label{subsec:problem-formulation}

We now formally define the performance modeling problem and establish the evaluation framework used throughout this survey.

\subsubsection{Inputs and Outputs}

Performance modeling maps workload and hardware descriptions to performance metrics.
Formally, given workload specification $\mathcal{W}$ and hardware configuration $\mathcal{H}$, a performance model $f$ predicts metric $y$:
\begin{equation}
\hat{y} = f(\mathcal{W}, \mathcal{H}; \theta)
\end{equation}
where $\theta$ represents model parameters (weights for ML models, equations for analytical models).

\textbf{Workload representations} vary by granularity and abstraction:
\begin{itemize}
    \item \emph{Operator-level}: Individual layer parameters (kernel size, channels, batch size)
    \item \emph{Graph-level}: Full computation graph with node and edge features
    \item \emph{IR-level}: Intermediate representations from compilers (TVM~\cite{tvm2018}, XLA)
    \item \emph{Trace-level}: Execution traces capturing runtime behavior
\end{itemize}

\textbf{Hardware representations} similarly span multiple levels:
\begin{itemize}
    \item \emph{Specification}: Static parameters (core count, memory size, bandwidth)
    \item \emph{Counter-based}: Runtime performance counters (cache misses, stalls)
    \item \emph{Embedding}: Learned dense representations of hardware platforms
\end{itemize}

\subsubsection{Prediction Targets}

Performance models target various metrics depending on application requirements:

\textbf{Latency} measures execution time, typically end-to-end inference time or per-layer latency.
Latency prediction is critical for real-time applications with strict deadlines and for optimizing user-facing services.

\textbf{Throughput} captures sustained processing rate: samples per second for inference, tokens per second for language models, or images per second for training.
Throughput optimization maximizes hardware utilization for batch processing.

\textbf{Energy} encompasses power consumption (Watts) and energy per operation (Joules/inference).
Energy prediction is essential for mobile deployment, data center cost optimization, and sustainability considerations.

\textbf{Memory} includes peak memory footprint (for feasibility checking), memory bandwidth utilization, and memory access patterns.

\textbf{Multi-objective} formulations jointly predict multiple metrics, enabling Pareto-optimal design selection balancing latency, energy, and accuracy.

\subsubsection{Accuracy Metrics}

The field employs several accuracy metrics, each with distinct interpretations:

\textbf{Mean Absolute Percentage Error (MAPE)} measures average relative deviation:
\begin{equation}
\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
\end{equation}
MAPE is scale-invariant and interpretable (5\% MAPE means predictions typically differ by 5\% from ground truth).

\textbf{Root Mean Square Error (RMSE)} penalizes large errors more heavily:
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}

\textbf{Correlation coefficients} (Pearson, Spearman) measure how well predictions track relative ordering---important when models guide design space exploration.

\textbf{Ranking accuracy} directly evaluates whether models correctly order configurations, often measured via Kendall's $\tau$ or top-$k$ accuracy.

\subsubsection{Hardware Targets}

Modern performance modeling spans diverse hardware platforms:

\textbf{CPUs} remain important for general-purpose inference and training of smaller models.
CPU modeling must account for complex cache hierarchies, branch prediction, out-of-order execution, and SIMD vectorization.

\textbf{GPUs} dominate ML training and large-scale inference.
GPU modeling addresses SIMT execution, warp scheduling, memory coalescing, and multi-GPU scaling.

\textbf{TPUs and custom accelerators} employ specialized dataflows for matrix operations.
Modeling these devices requires understanding systolic arrays, on-chip memory hierarchies, and dataflow mappings.

\textbf{Edge devices} (mobile SoCs, embedded NPUs) impose strict power and memory constraints.
Edge modeling emphasizes latency under thermal throttling and memory-limited execution.

\textbf{Distributed systems} scale training across multiple devices and nodes.
Distributed modeling must capture communication overhead, synchronization barriers, and pipeline parallelism.

This diversity of targets, workloads, and metrics motivates our comprehensive taxonomy in Section~\ref{sec:taxonomy}.

% ==============================================================================
% TAXONOMY
% ==============================================================================
\section{Taxonomy}
\label{sec:taxonomy}

We organize the surveyed literature along three primary dimensions: the hardware target being modeled, the machine learning techniques employed, and the input representations used.
Figure~\ref{fig:taxonomy-overview} illustrates how these dimensions intersect to characterize different performance modeling approaches.
This taxonomy extends existing classifications~\cite{timeloop2019,maestro2019} by incorporating the emerging diversity of ML-based methods and their distinctive design choices.

Our classification scheme serves two purposes.
First, it provides a systematic framework for understanding the design space of ML-based performance models---researchers can identify which combinations of targets, techniques, and representations have been explored versus those that remain open.
Second, it enables practitioners to select appropriate methods for their use cases by matching problem characteristics (target hardware, available data, accuracy requirements) to model capabilities.

\begin{figure}[t]
\centering
\small
\begin{tabular}{|l|l|}
\hline
\textbf{Dimension} & \textbf{Categories} \\
\hline
\multirow{5}{*}{Target Hardware} & CPU \\
& GPU \\
& DNN Accelerators (TPU, NPU) \\
& Edge/Mobile Devices \\
& Distributed Systems \\
\hline
\multirow{4}{*}{ML Technique} & Classical ML (RF, XGBoost) \\
& Deep Learning (MLP, RNN) \\
& Graph Neural Networks \\
& Hybrid Analytical+ML \\
\hline
\multirow{4}{*}{Input Representation} & Static Features \\
& Hardware Counters \\
& Graph Representations \\
& Learned Embeddings \\
\hline
\end{tabular}
\caption{Overview of our three-dimensional taxonomy for organizing ML-based performance modeling approaches.}
\label{fig:taxonomy-overview}
\end{figure}

\subsection{By Modeling Target}
\label{subsec:by-target}

The choice of hardware target fundamentally shapes model design, as different platforms exhibit distinct performance characteristics and modeling challenges.

\subsubsection{CPU Performance Modeling}

CPUs present complex modeling challenges due to deep out-of-order pipelines, sophisticated cache hierarchies, and branch prediction.
ML models for CPU performance must capture instruction-level parallelism, cache behavior, and memory access patterns.
Traditional approaches relied on microbenchmark-based linear regression~\cite{binkert2011gem5}, while recent work employs graph neural networks to model basic block throughput~\cite{granite2022}.
CPU modeling remains challenging due to the diversity of microarchitectures and the difficulty of capturing dynamic effects like branch misprediction and cache contention.

\subsubsection{GPU Performance Modeling}

GPUs dominate modern ML training and inference, making accurate GPU performance prediction critical.
GPU modeling must account for SIMT execution, warp scheduling, memory coalescing, and memory bandwidth limitations.
Early approaches used analytical roofline models~\cite{williams2009roofline}, but these struggle with the complex memory hierarchies and occupancy effects of modern GPUs.

ML-based GPU models have achieved remarkable accuracy.
NeuSight~\cite{neusight2025} introduces tile-based prediction that mirrors CUDA's execution model, achieving 2.3\% error on GPT-3 inference across H100, A100, and V100 GPUs.
Habitat~\cite{habitat2021} pioneered runtime-based cross-GPU prediction using wave scaling analysis.
These approaches demonstrate that learned models can capture GPU performance characteristics that elude analytical treatment.

\subsubsection{DNN Accelerator Modeling}

Custom DNN accelerators---including TPUs, NPUs, and systolic array designs---employ specialized dataflows optimized for matrix operations.
Modeling these devices requires understanding the interaction between dataflow, memory hierarchy, and tensor tiling.

Analytical frameworks like Timeloop~\cite{timeloop2019} and MAESTRO~\cite{maestro2019} provide systematic approaches for accelerator design space exploration.
Timeloop models data movement and compute utilization for any valid mapping of operations to hardware, achieving 5--10\% accuracy versus RTL simulation at 2000$\times$ speedup.
MAESTRO offers a data-centric perspective using intuitive dataflow directives.
Sparseloop~\cite{sparseloop2022} extends these frameworks to sparse tensor operations, critical for efficient transformer inference.

ML-based approaches complement analytical models by learning residual corrections or capturing effects not modeled analytically.
ArchGym~\cite{archgym2023} demonstrates that ML surrogate models can achieve 0.61\% RMSE while providing 2000$\times$ speedup over simulation, enabling rapid design space exploration for accelerator development.

\subsubsection{Edge and Mobile Device Modeling}

Edge devices impose strict power, memory, and latency constraints, making accurate prediction essential for deploying ML models on mobile phones, IoT devices, and embedded systems.
The diversity of edge hardware---spanning mobile CPUs, mobile GPUs, NPUs, and DSPs---creates significant challenges for cross-platform prediction.

nn-Meter~\cite{nnmeter2021} addresses this challenge through kernel-level prediction with adaptive sampling, achieving 99\% accuracy across mobile CPUs, GPUs, and Intel VPUs.
LitePred~\cite{litepred2024} extends this work with transfer learning, achieving 99.3\% accuracy across 85 edge platforms with less than one hour of adaptation per new device.
These results demonstrate that ML models can effectively generalize across the heterogeneous edge hardware landscape.

\subsubsection{Distributed System Modeling}

Multi-GPU and multi-node systems introduce communication overhead, synchronization barriers, and parallelism strategy choices that fundamentally change performance characteristics.
Distributed training performance depends on the interplay between compute, memory bandwidth, and network communication.

ASTRA-sim~\cite{astrasim2023} provides end-to-end distributed training simulation, modeling collective communication algorithms, network topology, and compute-communication overlap.
VIDUR~\cite{vidur2024} focuses specifically on LLM inference serving, capturing the unique characteristics of prefill and decode phases, KV cache management, and request scheduling.
These simulation frameworks achieve 5--15\% accuracy versus real clusters while enabling exploration of parallelization strategies at scale.

\subsection{By ML Technique}
\label{subsec:by-technique}

The choice of ML technique reflects trade-offs between accuracy, data efficiency, interpretability, and generalization capability.

\subsubsection{Classical Machine Learning}

Tree-based ensembles---random forests and gradient boosted trees (XGBoost, LightGBM)---remain highly effective for performance modeling, particularly in low-data regimes.
These methods handle non-linear relationships through recursive partitioning, provide feature importance rankings for interpretability, and require minimal hyperparameter tuning.

Classical ML models dominate when training data is limited ($<$10K samples) or when features are well-engineered.
nn-Meter~\cite{nnmeter2021} demonstrates that random forests achieve competitive accuracy with careful kernel-level feature engineering.
The ALCOP framework combines XGBoost with analytical pre-training, using analytical model predictions as features to accelerate autotuning convergence.

\subsubsection{Deep Learning}

Multi-layer perceptrons (MLPs) learn hierarchical feature representations without manual feature engineering.
MLPs are widely used as the prediction head in more complex architectures and as standalone models when sufficient training data is available.
NeuSight~\cite{neusight2025} uses MLPs to predict tile-level GPU utilization, learning complex interactions between tile parameters and hardware characteristics.

Recurrent neural networks (RNNs and LSTMs) process sequential inputs, making them suitable for modeling operator sequences in neural network execution.
However, sequential processing limits parallelization, and attention-based architectures increasingly replace RNNs for sequence modeling tasks.

\subsubsection{Graph Neural Networks}

Graph neural networks (GNNs) have emerged as particularly effective for performance modeling because computational graphs have natural graph structure.
Nodes represent operators with features (type, parameters, shapes), edges represent data dependencies with features (tensor dimensions, datatypes).
GNNs propagate performance-relevant information along these dependencies through message passing.

GRANITE~\cite{granite2022} applies GNNs to basic block throughput estimation, learning to predict CPU performance from instruction dependency graphs.
For DNN workloads, GNN-based models capture inter-operator interactions that flat feature representations miss.
The graph structure also enables natural handling of variable-size networks without padding or truncation.

\subsubsection{Hybrid Analytical+ML Models}

Hybrid approaches combine physics-based analytical models with learned components, achieving both interpretability and high accuracy.
The analytical component provides a strong prior based on hardware characteristics, while the ML component learns residual corrections and complex interactions.

This design philosophy has produced state-of-the-art results.
Analytical pre-training initializes ML models with reasonable predictions, reducing data requirements and improving convergence.
Physics-informed architectures incorporate analytical insights into model structure---NeuSight's tile-based prediction mirrors CUDA's execution model, providing inductive bias that improves generalization.
Residual learning trains ML models to predict the error of analytical models, combining analytical interpretability with ML's ability to capture unmodeled effects.

The latency predictor study~\cite{latencypredictorsnas2024} demonstrates that hybrid approaches with transfer learning achieve 22.5\% average improvement over baselines, with up to 87.6\% improvement on challenging cross-platform prediction tasks.

\subsection{By Input Representation}
\label{subsec:by-input}

Input representation determines what information the model can access and how effectively it can learn performance-relevant patterns.

\subsubsection{Static Features}

Static features derive from workload and hardware specifications without runtime measurement.
For DNN workloads, these include layer parameters (kernel size, channels, stride, batch size), tensor dimensions, and operator types.
Hardware specifications include core counts, memory sizes, bandwidth, and clock frequencies.

Static features enable prediction without profiling, supporting use cases like neural architecture search where thousands of candidate networks must be evaluated.
Feature engineering plays a critical role: effective representations capture computation-to-communication ratios, memory footprint estimates, and parallelization potential.

\subsubsection{Hardware Counters}

Performance counters provide runtime measurements of hardware behavior: cache miss rates, memory bandwidth utilization, instruction throughput, and stall cycles.
Counter-based models can capture dynamic effects invisible to static analysis, including contention, thermal throttling, and runtime scheduling decisions.

The primary limitation is that counter-based models require hardware execution, limiting their applicability for design space exploration or new architecture evaluation.
However, for optimizing existing deployments or debugging performance anomalies, counter-based models provide valuable insights that static approaches cannot match.

\subsubsection{Graph Representations}

Graph representations encode computational graphs with nodes representing operators and edges representing data dependencies.
Node features capture operator characteristics (type, parameters), while edge features encode tensor properties (shape, datatype, memory format).

Graph representations provide several advantages over flat feature vectors: they naturally handle variable-size networks, preserve structural information about operator interactions, and enable permutation-invariant predictions.
GNNs operating on these representations can learn which subgraph patterns indicate performance bottlenecks.

\subsubsection{Learned Embeddings}

Learned embeddings compress high-dimensional or categorical information into dense vector representations.
Hardware embeddings represent diverse devices as points in a learned feature space, enabling transfer learning across platforms.
Operator embeddings capture semantic similarities between operator types that may share performance characteristics.

HELP formulates hardware prediction as meta-learning, learning hardware embeddings that represent devices as black-box functions.
With just 10 measurement samples on a new device, HELP achieves accurate predictions by positioning the device appropriately in the learned embedding space.
This approach is particularly valuable for the fragmented edge hardware landscape, where collecting exhaustive training data for each device is impractical.

Table~\ref{tab:taxonomy-summary} summarizes representative papers across our taxonomy dimensions, illustrating the diversity of approaches and their key characteristics.

\begin{table*}[t]
\centering
\caption{Representative papers classified by our taxonomy dimensions. Accuracy reported as MAPE or correlation where available.}
\label{tab:taxonomy-summary}
\small
\begin{tabular}{llllll}
\toprule
\textbf{Paper} & \textbf{Target} & \textbf{Technique} & \textbf{Input} & \textbf{Accuracy} & \textbf{Key Contribution} \\
\midrule
NeuSight~\cite{neusight2025} & GPU & Hybrid & Static & 2.3\% & Tile-based prediction \\
nn-Meter~\cite{nnmeter2021} & Edge & Classical ML & Static & $<$5\% & Kernel detection \\
LitePred~\cite{litepred2024} & Edge & Transfer & Static & 0.7\% & 85-platform transfer \\
GRANITE~\cite{granite2022} & CPU & GNN & Graph & 0.97 corr & Basic block modeling \\
Timeloop~\cite{timeloop2019} & Accelerator & Analytical & Static & 5--10\% & Loop-nest DSE \\
ASTRA-sim~\cite{astrasim2023} & Distributed & Simulation & Traces & 5--15\% & Collective modeling \\
ArchGym~\cite{archgym2023} & Accelerator & Hybrid & Static & 0.61\% RMSE & ML-aided DSE \\
\bottomrule
\end{tabular}
\end{table*}

% ==============================================================================
% SURVEY OF APPROACHES
% ==============================================================================
\section{Survey of Approaches}
\label{sec:survey}

\subsection{CPU Performance Modeling}
\label{subsec:cpu-modeling}

\todo{Survey papers on ML-based CPU performance prediction.}

\subsection{GPU Performance Modeling}
\label{subsec:gpu-modeling}

\todo{Survey papers on ML-based GPU performance prediction.}

\subsection{Accelerator Performance Modeling}
\label{subsec:accelerator-modeling}

\todo{Survey papers on ML models for DNN accelerators, FPGAs, etc.}

\subsection{Memory System Modeling}
\label{subsec:memory-modeling}

\todo{Survey papers on cache, DRAM, and memory hierarchy modeling.}

\subsection{Cross-Platform and Transfer Learning}
\label{subsec:transfer-learning}

\todo{Survey approaches that generalize across hardware configurations.}

% ==============================================================================
% COMPARISON AND ANALYSIS
% ==============================================================================
\section{Comparison and Analysis}
\label{sec:comparison}

\subsection{Accuracy vs. Training Cost}
\label{subsec:accuracy-cost}

\todo{Compare prediction accuracy against data collection and training overhead.}

\subsection{Generalization Capabilities}
\label{subsec:generalization}

\todo{Analyze how well models generalize to unseen workloads and configurations.}

\subsection{Interpretability}
\label{subsec:interpretability}

\todo{Discuss model interpretability and insights gained from ML models.}

\todo{Create comparison tables summarizing key papers across multiple dimensions.}

% ==============================================================================
% OPEN CHALLENGES
% ==============================================================================
\section{Open Challenges and Future Directions}
\label{sec:challenges}

\subsection{Data Availability and Quality}
\label{subsec:data-challenges}

\todo{Discuss challenges in collecting training data, benchmark diversity.}

\subsection{Model Generalization}
\label{subsec:generalization-challenges}

\todo{Challenges in generalizing to new architectures and workloads.}

\subsection{Integration with Design Flows}
\label{subsec:integration-challenges}

\todo{Challenges in integrating ML models into architecture exploration workflows.}

\subsection{Emerging Opportunities}
\label{subsec:opportunities}

\todo{Foundation models for architecture, hardware-software co-design.}

% ==============================================================================
% CONCLUSION
% ==============================================================================
\section{Conclusion}
\label{sec:conclusion}

\todo{Summarize key findings and takeaways from the survey.}

\todo{Reiterate the most promising directions for future research.}

% ==============================================================================
% REFERENCES
% ==============================================================================
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
