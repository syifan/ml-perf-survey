% MICRO 2026 Survey Paper - ML Performance Models
% Using MICRO 59 ACM sigconf template
% Last compiled: 2026-02-07 (rebuild triggered)

%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
\documentclass[sigconf, screen, review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information - for submission
\setcopyright{none}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{}

%% Conference information
\acmConference[MICRO 2026]{The 59th IEEE/ACM International Symposium on Microarchitecture}{November 2026}{Austin, TX, USA}
\acmISBN{}

%% Disable ACM reference format printing for submission
\settopmatter{printfolios=true}
\settopmatter{printacmref=false}

%% Anonymous submission
\author{Anonymous Author(s)}
\affiliation{%
  \institution{Under Review}
  \country{Anonymous}
}

%% Additional packages (acmart already loads amsmath, amsfonts, amssymb, booktabs)
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows.meta,positioning,fit,backgrounds,patterns}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{plotmarks}

% Custom commands
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\begin{document}

\title{A Survey of High-Level Modeling and Simulation Methods for Modern Machine Learning Workloads}
\subtitle{\normalsize{MICRO 2026 Submission -- Confidential Draft -- Do NOT Distribute!!}}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

%%%%%% -- PAPER CONTENT STARTS-- %%%%%%%%

\begin{abstract}
As machine learning workloads grow in scale and complexity---spanning training and inference for CNNs, transformers, mixture-of-experts models, and LLMs---architects and system designers need fast, accurate methods to predict their performance across diverse hardware platforms.
This survey provides a comprehensive analysis of the tools and methods available for modeling and simulating the performance of ML workloads, covering analytical models, cycle-accurate simulators, trace-driven approaches, and ML-augmented hybrid techniques.
We survey 22 tools in depth (with 15+ additional tools discussed) drawn from 53 papers across architecture venues (MICRO, ISCA, HPCA, ASPLOS) and systems venues (MLSys, OSDI, NSDI) published between 2016--2026, spanning DNN accelerator modeling (Timeloop, MAESTRO, Sparseloop), GPU simulation (GPGPU-Sim, Accel-Sim, NeuSight), distributed training simulation (ASTRA-sim, Lumos, SimAI), and LLM inference serving (VIDUR, Frontier, AMALI).
We organize the literature along three dimensions---methodology type (analytical, simulation, ML-augmented, hybrid), target platform (accelerators, GPUs, distributed systems, edge devices), and abstraction level (kernel, model, system)---while additionally characterizing tools by workload coverage, revealing a pervasive CNN-validation bias.
Our analysis reveals that hybrid approaches combining analytical structure with learned components achieve the best accuracy-speed trade-offs, while pure analytical models offer superior interpretability for design space exploration.
We conduct hands-on reproducibility evaluations of five representative tools, finding that reproducibility varies dramatically: among our evaluated tools, those with Docker-first deployment score 8.5+/10 on our rubric, while tools relying on serialized ML models (e.g., pickle-based predictors) risk becoming unusable due to dependency incompatibilities.
We identify key open challenges including cross-workload generalization beyond CNNs, composition of kernel-level predictions to end-to-end accuracy, and support for emerging architectures.
This survey provides practitioners guidance for selecting appropriate modeling tools and researchers a roadmap for advancing the field of ML workload performance prediction.
\end{abstract}

%%
%% Keywords
\keywords{ML workload performance prediction, DNN accelerator modeling, GPU simulation, distributed training simulation, LLM inference serving, design space exploration, survey}

\maketitle

% ==============================================================================
% INTRODUCTION
% ==============================================================================
\section{Introduction}
\label{sec:introduction}

Machine learning workloads---spanning training and inference for CNNs, transformers, mixture-of-experts models, and graph neural networks---have become the dominant consumers of compute across datacenters and edge devices.
The shift toward domain-specific architectures~\cite{hennessy2019golden}---from Google's TPU~\cite{tpuv1_2017,tpuv4_2023} to custom accelerators---has created a heterogeneous landscape where architects need fast, accurate performance predictions for design space exploration, parallelization selection, and hardware-software co-design.
Yet ML workloads pose unique challenges: diverse computational patterns (dense matrix operations, sparse accesses, communication-bound collectives) across GPUs, TPUs, custom accelerators, and multi-device clusters.
A rich tool ecosystem has emerged: analytical models (Timeloop~\cite{timeloop2019}: 5--10\% error; MAESTRO~\cite{maestro2019}: 5--15\% error; both at microsecond speed), cycle-accurate simulators (GPGPU-Sim~\cite{gpgpusim2009}, Accel-Sim~\cite{accelsim2020}: hours per workload), trace-driven simulators (ASTRA-sim~\cite{astrasim2023}, VIDUR~\cite{vidur2024}), and hybrid approaches (NeuSight~\cite{neusight2025}: 2.3\% error).
Yet no comprehensive survey organizes these methods for the practitioner who must select a tool for a specific task.
Existing surveys focus on ML \emph{techniques} for modeling~\cite{granite2022} or specific hardware~\cite{timeloop2019}; this survey fills that gap with a methodology-centric view.

We make the following contributions:
\begin{itemize}
    \item A \textbf{methodology-centric taxonomy} organizing tools along three dimensions: methodology type (analytical, simulation, ML-augmented, hybrid), target platform, and abstraction level, with a coverage matrix identifying research gaps and a workload analysis exposing CNN-validation bias.
    \item A \textbf{systematic survey} of 22 tools (with 15+ additional tools discussed) from 53 papers across architecture venues (MICRO, ISCA, HPCA, ASPLOS) and systems venues (MLSys, OSDI, NSDI) published 2016--2026.
    \item A \textbf{comparative analysis} of accuracy--speed trade-offs with careful qualification of reported claims and cases where numbers are unverifiable.
    \item \textbf{Hands-on reproducibility evaluations} with a 10-point rubric, and identification of \textbf{open challenges} including the CNN-to-transformer generalization gap, kernel-to-end-to-end error composition, and emerging accelerator support.
\end{itemize}

The paper proceeds as follows: Section~\ref{sec:methodology} describes our methodology; Section~\ref{sec:background} provides background; Section~\ref{sec:taxonomy} presents the taxonomy; Section~\ref{sec:survey} surveys tools; Section~\ref{sec:comparison} compares accuracy; Section~\ref{sec:evaluation} presents evaluations; Section~\ref{sec:challenges} discusses challenges; and Section~\ref{sec:conclusion} concludes.
Figure~\ref{fig:timeline} illustrates the evolution of performance modeling tools from early analytical frameworks to modern hybrid approaches.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[
    node distance=0.3cm,
    yearnode/.style={font=\footnotesize\bfseries, text=black!70},
    eventnode/.style={font=\scriptsize, text width=2.8cm, align=center},
    catnode/.style={font=\tiny, text=white, rounded corners=1pt, inner sep=2pt}
]
% Timeline base
\draw[thick, ->, black!40] (0,0) -- (14.5,0);

% Year markers
\foreach \x/\year in {0.5/2016, 2.5/2018, 4.5/2020, 6.5/2022, 8.5/2024, 10.5/2025, 12.5/2026} {
    \draw[thick, black!40] (\x,-0.15) -- (\x,0.15);
    \node[yearnode, below] at (\x,-0.2) {\year};
}

% Events - staggered heights to avoid overlap
\node[eventnode, above] at (0.5,0.4) {Eyeriss~\cite{eyeriss2016}\\Paleo~\cite{paleo2017}};
\node[catnode, fill=blue!60] at (0.5,1.2) {ISCA/ICLR};

\node[eventnode, below] at (2.0,-0.4) {TVM~\cite{tvm2018}\\(compiler cost)};
\node[catnode, fill=green!50!black] at (2.0,-1.1) {OSDI};

\node[eventnode, above] at (3.0,0.4) {Timeloop~\cite{timeloop2019}\\MAESTRO~\cite{maestro2019}};
\node[catnode, fill=blue!60] at (3.0,1.2) {ISPASS/MICRO};

\node[eventnode, below] at (4.5,-0.4) {Ansor~\cite{ansor2020}\\ASTRA-sim~\cite{astrasim2020}};
\node[catnode, fill=green!50!black] at (4.5,-1.1) {OSDI/ISPASS};

\node[eventnode, above] at (5.5,0.4) {nn-Meter~\cite{nnmeter2021}\\Habitat~\cite{habitat2021}};
\node[catnode, fill=orange!70] at (5.5,1.2) {MobiSys/ATC};

\node[eventnode, below] at (6.5,-0.4) {Sparseloop~\cite{sparseloop2022}\\Accel-Sim~\cite{accelsim2020}};
\node[catnode, fill=blue!60] at (6.5,-1.1) {MICRO/ISCA};

\node[eventnode, above] at (7.5,0.4) {TLP~\cite{tlp2023}\\ASTRA-sim 2.0~\cite{astrasim2023}};
\node[catnode, fill=purple!60] at (7.5,1.2) {ASPLOS/ISPASS};

\node[eventnode, below] at (8.8,-0.4) {VIDUR~\cite{vidur2024}\\Splitwise~\cite{splitwise2024}};
\node[catnode, fill=red!60] at (8.8,-1.1) {MLSys/ISCA};

\node[eventnode, above] at (10.5,0.4) {NeuSight~\cite{neusight2025}\\AMALI~\cite{amali2025}};
\node[catnode, fill=purple!60] at (10.5,1.2) {ASPLOS/ISCA};

\node[eventnode, below] at (12.5,-0.4) {Frontier~\cite{frontier2025}\\Lumos~\cite{lumos2025}};
\node[catnode, fill=red!60] at (12.5,-1.1) {MLSys};

% Trend arrow
\draw[thick, ->, blue!50, dashed] (1,1.6) -- (12,1.6);
\node[font=\scriptsize, text=blue!60, above] at (6.5,1.6) {Toward hybrid analytical+learned models};

\end{tikzpicture}
}
\caption{Evolution of performance modeling tools for ML workloads (2016--2026). Early analytical frameworks (Eyeriss, Paleo) gave way to systematic accelerator modeling (Timeloop, MAESTRO) and distributed training simulation (ASTRA-sim). Recent work targets LLM-specific modeling (VIDUR, Frontier) and hybrid analytical+learned approaches (NeuSight).}
\label{fig:timeline}
\end{figure}

% ==============================================================================
% SURVEY METHODOLOGY
% ==============================================================================
\section{Survey Methodology}
\label{sec:methodology}

We searched ACM Digital Library, IEEE Xplore, Semantic Scholar, and arXiv using terms related to ML performance modeling, with backward/forward citation tracking from seminal works.
Target venues include architecture (MICRO, ISCA, HPCA, ASPLOS), systems (MLSys, OSDI, SOSP, NSDI), and related (NeurIPS, MobiSys, DAC, ISPASS).
Papers must propose or evaluate a tool for predicting ML workload performance with quantitative evaluation; we exclude non-performance tasks and general-purpose workloads.
From 287 initial candidates, title/abstract screening yielded 118 papers; full-text review reduced the set to 53 that met all criteria, supplemented by 12 foundational works for context.
We cover 2016--2026 and classify each paper by \emph{methodology type} (analytical, simulation, trace-driven, ML-augmented, hybrid), \emph{target platform}, and \emph{abstraction level} (kernel, model, system).

\subsection{Related Surveys}
\label{subsec:related-surveys}

Prior surveys address adjacent topics: Rakhshanfar and Zarandi~\cite{rakhshanfar2021survey} survey ML for processor DSE; Sze et al.~\cite{sze2017efficient} treat DNN hardware design (the foundation for Timeloop/MAESTRO); GPGPU-Sim~\cite{gpgpusim2009} and gem5~\cite{binkert2011gem5} have extensive evaluation literature; SST~\cite{sst2012} provides modular system-level simulation widely used for interconnect and HPC modeling; and MLPerf~\cite{mlperf_training2020,mlperf_inference2020} standardizes \emph{measurement} rather than \emph{prediction}.
Early ML accelerator modeling (2014--2018) established foundational approaches: DianNao~\cite{diannao2014} introduced analytical dataflow modeling for dedicated accelerators, Eyeriss~\cite{eyeriss2016} systematized row-stationary dataflow analysis, and Paleo~\cite{paleo2017} pioneered layer-wise analytical estimation.
This survey differs by spanning the full methodology spectrum across all major platforms with hands-on reproducibility evaluations.
The closest prior work, Dudziak et al.~\cite{latencypredictorsnas2024}, compares edge device predictors for NAS; we broaden to the full landscape.

% ==============================================================================
% BACKGROUND
% ==============================================================================
\section{Background}
\label{sec:background}

\subsection{ML Workload Characteristics}
\label{subsec:workload-characteristics}

ML workloads, defined as computation graphs in frameworks like PyTorch~\cite{pytorch2019} and TensorFlow~\cite{tensorflow2016}, have statically known operator shapes amenable to analytical modeling, though MoE and dynamic inference introduce input-dependent control flow.
Performance depends on tensor-to-memory mapping (dataflow, tiling), KV cache management for LLM inference~\cite{vllm2023}, and at scale, compute--memory--network interactions across data, tensor, pipeline, and expert parallelism~\cite{llama3scaling2025}.
LLM inference splits into compute-bound prefill and memory-bound decode phases~\cite{splitwise2024}, both modeled under batched serving~\cite{sarathi2024,orca2022}.

\subsection{Modeling Methodologies}
\label{subsec:modeling-methodologies}

We classify approaches into five categories.
\textbf{Analytical models} express performance as closed-form functions (e.g., the roofline model~\cite{williams2009roofline}: $P = \min(\pi, \beta \cdot I)$), offering microsecond evaluation but requiring per-architecture derivation.
\textbf{Cycle-accurate simulators} (GPGPU-Sim~\cite{gpgpusim2009}, Accel-Sim~\cite{accelsim2020}) achieve high fidelity at $1000$--$10000\times$ slowdown~\cite{gpgpusim2009,accelsim2020}; statistical sampling techniques~\cite{simpoint2002,smarts2003} and checkpoint-driven approaches~\cite{looppoint2022} reduce this cost by identifying representative execution phases, while dedicated memory simulators~\cite{dramsim3_2020,ramulator2_2023} provide cycle-accurate DRAM modeling.
\textbf{Trace-driven simulators} (ASTRA-sim~\cite{astrasim2023}, VIDUR~\cite{vidur2024}) trade fidelity for orders-of-magnitude speedup.
\textbf{ML-augmented approaches} learn from profiling data (nn-Meter~\cite{nnmeter2021}) but may not generalize beyond training distributions.
\textbf{Hybrid approaches} combine analytical structure with learned components (NeuSight~\cite{neusight2025}, Habitat~\cite{habitat2021}), aiming to balance accuracy, speed, and interpretability.

\subsection{Problem Formulation}
\label{subsec:problem-formulation}

Performance modeling maps workload $\mathcal{W}$ and hardware $\mathcal{H}$ to a metric $y$: $\hat{y} = f(\mathcal{W}, \mathcal{H}; \theta)$, with workloads represented at operator, graph, IR, or trace level, and hardware characterized by specifications, counters, or learned embeddings.
Prediction targets include latency, throughput, energy, and memory footprint; ground-truth measurements typically rely on hardware performance counters accessed via PAPI~\cite{papi2000} or LIKWID~\cite{likwid2010}.
Accuracy metrics---MAPE, RMSE, and rank correlation (Kendall's $\tau$)---vary across the literature, and differences in benchmarks, hardware targets, and evaluation protocols limit direct comparison (Section~\ref{sec:comparison}).

% ==============================================================================
% TAXONOMY
% ==============================================================================
\section{Taxonomy}
\label{sec:taxonomy}

We organize the literature along three dimensions.
The \emph{primary axis} is methodology type---how a tool predicts performance---because methodology determines the fundamental trade-offs between accuracy, speed, interpretability, and data requirements.
The \emph{secondary axes} are target platform and abstraction level, which together determine the scope and applicability of each tool.
We additionally characterize tools by workload coverage, exposing a pervasive CNN-validation bias in the literature.

Table~\ref{tab:taxonomy-matrix} provides a unified view combining the coverage matrix (number of surveyed tools per methodology--platform cell) with trade-off profiles, with empty cells highlighting research gaps.
The dominant pairings are: analytical models for accelerators, cycle-accurate simulation for GPUs/CPUs, trace-driven simulation for distributed systems, and ML-augmented approaches for edge devices.

% --- Unified Taxonomy Table (merged from Tables 1+2, issue #192) ---
\begin{table*}[t]
\centering
\caption{Methodology taxonomy: coverage matrix and trade-off profile.
Platform columns show the number of surveyed tools per cell; \textbf{0} indicates an explicit research gap.
Speed, data requirements, and interpretability determine practical applicability; the failure mode column identifies the primary condition under which each methodology breaks down.}
\label{tab:taxonomy-matrix}
\small
\begin{tabular}{l|ccccc|cccc}
\toprule
 & \textbf{DNN} & & \textbf{Distrib.} & \textbf{Edge/} & & \textbf{Eval.} & \textbf{Data} & & \textbf{Failure} \\
\textbf{Methodology} & \textbf{Accel.} & \textbf{GPU} & \textbf{Systems} & \textbf{Mobile} & \textbf{CPU} & \textbf{Speed} & \textbf{Req.} & \textbf{Interp.} & \textbf{Mode} \\
\midrule
Analytical       & 3 & 3 & 2 & \textbf{0} & \textbf{0} & $\mu$s & None & High & Dynamic effects \\
Cycle-Accurate   & 1 & 2 & \textbf{0} & \textbf{0} & 1 & Hours & Binary & High & Scale \\
Trace-Driven     & \textbf{0} & \textbf{0} & 7 & \textbf{0} & \textbf{0} & Min. & Traces & Med. & Trace fidelity \\
ML-Augmented     & \textbf{0} & 3 & \textbf{0} & 3 & 1 & ms & Profiling & Low & Distrib.\ shift \\
Hybrid           & 1 & 2 & \textbf{0} & \textbf{0} & 1 & ms & Mixed & Med. & Training domain \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:taxonomy-matrix} reveals three structural gaps: (1)~trace-driven \emph{execution replay} simulation (as distinct from instruction-trace-driven cycle-accurate simulation such as Accel-Sim) is used exclusively for distributed systems; (2)~edge devices are served only by ML-augmented approaches, lacking hybrid alternatives; (3)~no ML-augmented tool targets distributed systems directly.
Methodologies cluster into two speed regimes: sub-millisecond (analytical, ML-augmented, hybrid) for DSE, and minutes-to-hours (simulation, trace-driven) for validation.

Figure~\ref{fig:tool-architecture} illustrates how tools from different methodology types compose: analytical engines provide fast base estimates, ML components learn residual corrections, and trace-driven simulators orchestrate system-level execution.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[
    comp/.style={draw=black!60, rounded corners=3pt, minimum width=2.4cm, minimum height=0.65cm, align=center, font=\scriptsize},
    input/.style={draw=black!40, dashed, rounded corners=2pt, minimum width=1.8cm, minimum height=0.5cm, align=center, font=\tiny},
    arr/.style={-{Stealth[length=3pt]}, thick, black!50},
    lbl/.style={font=\tiny, text=black!50},
]

% Input layer
\node[input] (wl) at (0,4) {Workload\\(ONNX/Chakra)};
\node[input] (hw) at (3,4) {Hardware\\config};
\node[input] (prof) at (6,4) {Profiling\\data};

% Methodology layer
\node[comp, fill=blue!15] (analytical) at (0,2.5) {Analytical\\Engine};
\node[comp, fill=red!15] (simulator) at (3,2.5) {Cycle-Accurate\\Simulator};
\node[comp, fill=purple!15] (mlmodel) at (6,2.5) {ML Cost\\Model};

% Hybrid composition
\node[comp, fill=green!15, minimum width=3.2cm] (hybrid) at (3,1) {Hybrid Composition\\(analytical + learned)};

% System orchestration
\node[comp, fill=orange!15, minimum width=6.5cm] (system) at (3,-0.3) {System Orchestrator (trace-driven)};

% Output
\node[input, draw=black!60, fill=gray!10] (output) at (3,-1.5) {Latency / Energy / Throughput};

% Arrows - inputs to methods
\draw[arr] (wl) -- (analytical);
\draw[arr] (hw) -- (analytical);
\draw[arr] (hw) -- (simulator);
\draw[arr] (wl) -- (simulator);
\draw[arr] (prof) -- (mlmodel);
\draw[arr] (wl) -- (mlmodel);

% Arrows - methods to hybrid
\draw[arr] (analytical) -- (hybrid);
\draw[arr] (mlmodel) -- (hybrid);
\draw[arr, dashed, gray] (simulator) -- node[lbl, right] {validation} (hybrid);

% Arrows - to system
\draw[arr] (hybrid) -- (system);
\draw[arr] (analytical) |- (system);

% Arrows - output
\draw[arr] (system) -- (output);

% Example tools
\node[font=\tiny, text=blue!60, anchor=east] at (-0.5,2.5) {Timeloop};
\node[font=\tiny, text=blue!60, anchor=east] at (-0.5,2.2) {MAESTRO};
\node[font=\tiny, text=red!60, anchor=east] at (1.2,2.8) {Accel-Sim};
\node[font=\tiny, text=red!60, anchor=east] at (1.2,2.5) {GPGPU-Sim};
\node[font=\tiny, text=purple!60, anchor=west] at (7.6,2.5) {nn-Meter};
\node[font=\tiny, text=purple!60, anchor=west] at (7.6,2.2) {TVM};
\node[font=\tiny, text=green!50!black, anchor=west] at (5,1) {NeuSight, Habitat};
\node[font=\tiny, text=orange!60!black, anchor=west] at (6.7,-0.3) {ASTRA-sim, VIDUR};

\end{tikzpicture}%
}
\caption{Unified architecture showing how tool methodologies compose. Analytical engines and ML cost models feed into hybrid approaches, while system-level orchestrators (trace-driven) assemble component predictions into end-to-end estimates. Cycle-accurate simulators primarily serve as validation oracles.}
\label{fig:tool-architecture}
\end{figure}

\subsection{Primary Axis: Methodology Type}
\label{subsec:by-methodology}

The choice of methodology determines fundamental trade-offs between accuracy, evaluation speed, data requirements, and interpretability, as summarized in Table~\ref{tab:taxonomy-matrix}; Section~\ref{sec:survey} provides detailed per-tool analysis.

\textbf{Analytical models} (Timeloop~\cite{timeloop2019}: 5--10\% vs.\ RTL; MAESTRO~\cite{maestro2019}; Sparseloop~\cite{sparseloop2022}; AMALI~\cite{amali2025}) provide microsecond evaluation and full interpretability but require per-architecture derivation (AMALI's 23.6\% MAPE illustrates GPU dynamic effects).
\textbf{Cycle-accurate simulators} (GPGPU-Sim~\cite{gpgpusim2009}, Accel-Sim~\cite{accelsim2020}: 0.90--0.97 IPC; PyTorchSim~\cite{pytorchsim2025}) are impractical for DSE at $1000$--$10000\times$ slowdown~\cite{gpgpusim2009,accelsim2020}.
\textbf{Trace-driven simulators} (ASTRA-sim~\cite{astrasim2023}: 5--15\%; VIDUR~\cite{vidur2024}: $<$5\%; SimAI~\cite{simai2025}; Frontier~\cite{frontier2025}) replay execution traces for system-level modeling.
\textbf{ML-augmented models} (nn-Meter~\cite{nnmeter2021}; LitePred~\cite{litepred2024}; HELP~\cite{help2021}; TVM~\cite{tvm2018}/Ansor~\cite{ansor2020}) learn from profiling data but risk \emph{silent distribution shift}.
\textbf{Hybrid} approaches (NeuSight~\cite{neusight2025}: 2.3\% MAPE; Habitat~\cite{habitat2021}; ArchGym~\cite{archgym2023}) combine analytical priors with learned corrections~\cite{latencypredictorsnas2024}.

\subsection{Secondary Axes: Platform and Abstraction Level}
\label{subsec:by-platform}

Platform constrains methodology: \textbf{accelerators} use analytical models; \textbf{GPUs} span all types; \textbf{distributed systems} require trace-driven simulation; \textbf{edge devices} use ML-augmented approaches; \textbf{CPUs}~\cite{concorde2025,granite2022} are least studied.
Abstraction level determines composition errors (Figure~\ref{fig:abstraction-levels}): kernel-level tools achieve 2--3\% error, model-level 5--12\%, and system-level 5--15\%, with errors propagating through the chain.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[
  level/.style={draw, rounded corners=3pt, minimum width=3.2cm, minimum height=0.7cm, align=center, font=\small\bfseries},
  tool/.style={font=\scriptsize, text=black!70},
  err/.style={font=\scriptsize\itshape, text=red!70!black},
  arrow/.style={-{Stealth[length=3pt]}, thick, gray!60},
  compos/.style={-{Stealth[length=4pt]}, thick, red!50!black, dashed},
]

% Levels (bottom to top)
\node[level, fill=blue!15, draw=blue!50] (kernel) at (0,0) {Kernel / Operator};
\node[level, fill=green!15, draw=green!50] (model) at (0,1.6) {Model / End-to-End};
\node[level, fill=orange!15, draw=orange!50] (system) at (0,3.2) {System};

% Composition arrows between levels
\draw[arrow] (kernel) -- (model);
\draw[arrow] (model) -- (system);

% Tool names (left side)
\node[tool, anchor=east] at (-2.1,0) {NeuSight, nn-Meter,};
\node[tool, anchor=east] at (-2.1,-0.3) {TVM, GRANITE, TLP};
\node[tool, anchor=east] at (-2.1,1.6) {Paleo, Habitat,};
\node[tool, anchor=east] at (-2.1,1.3) {AMALI, Lumos, LIFE};
\node[tool, anchor=east] at (-2.1,3.2) {ASTRA-sim, VIDUR,};
\node[tool, anchor=east] at (-2.1,2.9) {SimAI, Frontier};

% Error ranges (right side)
\node[err, anchor=west] at (2.1,0) {2--3\% error};
\node[err, anchor=west] at (2.1,1.6) {5--12\% error};
\node[err, anchor=west] at (2.1,3.2) {5--15\% error};

% Composition problem annotation
\draw[compos] (3.6,0.2) -- (3.6,3.0);
\node[font=\scriptsize, text=red!50!black, rotate=90, anchor=south] at (3.9,1.6) {error accumulates};

\end{tikzpicture}%
}
\caption{Abstraction level hierarchy and the composition problem. Tools operate at one of three levels; composing predictions across levels accumulates error. Error ranges are representative values from surveyed papers.}
\label{fig:abstraction-levels}
\end{figure}

\subsection{Workload Coverage}
\label{subsec:workload-coverage}

Table~\ref{tab:workload-coverage} characterizes the workload types on which each tool has been validated, exposing a pervasive CNN-validation bias.

\begin{table}[t]
\centering
\caption{Workload validation coverage. \checkmark\ = validated in the original paper; $\circ$ = partial or indirect validation; --- = no validation.
Nearly all tools report accuracy on CNN workloads; transformer and MoE coverage is sparse.
Empty columns (diffusion, dynamic inference) represent workload types with \emph{no} validated performance modeling tools.}
\label{tab:workload-coverage}
\small
\begin{tabular}{lccccc}
\toprule
 & & \textbf{Trans-} & \textbf{LLM} & & \\
\textbf{Tool} & \textbf{CNN} & \textbf{former} & \textbf{Train} & \textbf{MoE} & \textbf{Diff.} \\
\midrule
Timeloop & \checkmark & $\circ$ & --- & --- & --- \\
MAESTRO & \checkmark & --- & --- & --- & --- \\
NeuSight & \checkmark & \checkmark & --- & --- & --- \\
Habitat & \checkmark & --- & --- & --- & --- \\
AMALI & --- & \checkmark & --- & --- & --- \\
ASTRA-sim & \checkmark & $\circ$ & \checkmark & --- & --- \\
VIDUR & --- & \checkmark & --- & --- & --- \\
SimAI & --- & --- & \checkmark & --- & --- \\
Lumos & --- & --- & \checkmark & --- & --- \\
Frontier & --- & \checkmark & --- & \checkmark & --- \\
nn-Meter & \checkmark & --- & --- & --- & --- \\
LitePred & \checkmark & --- & --- & --- & --- \\
HELP & \checkmark & --- & --- & --- & --- \\
TVM/Ansor & \checkmark & $\circ$ & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:validation-bias} quantifies this CNN-validation bias: of the 14 surveyed tools, 10 (71\%) include CNN validation, while only 1 tool validates on MoE workloads and none validates on diffusion models.
The table reveals that \textbf{no surveyed tool has been validated on diffusion models or dynamic inference workloads}~\cite{dynamicreasoning2026}, only Frontier~\cite{frontier2025} has validated MoE support, and no single tool offers validated transformer prediction across the full kernel-to-system stack.
Practitioners working with non-CNN workloads must accept unvalidated predictions, collect their own validation data, or fall back to measurement.

\begin{figure}[t]
\centering
\resizebox{0.85\columnwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
    xbar,
    y dir=reverse,
    xlabel={Number of tools with validation},
    xmin=0, xmax=12,
    ytick={0,1,2,3,4},
    yticklabels={CNN, Transformer, LLM Training, MoE, Diffusion},
    yticklabel style={font=\small},
    xticklabel style={font=\scriptsize},
    xlabel style={font=\small},
    bar width=10pt,
    height=5cm,
    width=7.5cm,
    nodes near coords,
    nodes near coords style={font=\small\bfseries, anchor=west},
    every node near coord/.append style={xshift=1pt},
    extra y ticks={1.5, 3.5},
    extra y tick labels={},
    extra y tick style={grid=major, grid style={black!20, dashed}},
]
\addplot[fill=blue!50, draw=blue!70] coordinates {(10,0) (6,1) (3,2) (1,3) (0,4)};
\end{axis}
% Annotation
\node[font=\tiny, text=red!60!black, align=center] at (5.5,3.7) {No validated\\tools exist};
\end{tikzpicture}%
}
\caption{Workload validation coverage across surveyed tools. CNN validation dominates (71\% of tools), while MoE and diffusion models have minimal or no validated prediction tools, highlighting a critical generalization gap.}
\label{fig:validation-bias}
\end{figure}

% ==============================================================================
% SURVEY OF APPROACHES
% ==============================================================================
\section{Survey of Approaches}
\label{sec:survey}

This section surveys performance modeling tools for ML workloads, organized by target platform, examining modeling challenges, available tools, and their strengths and limitations.
Table~\ref{tab:survey-summary} provides a comprehensive comparison.

\begin{table*}[t]
\centering
\caption{Summary of surveyed performance modeling tools for ML workloads, organized by target platform. \textbf{Methodology}: A=Analytical, S=Simulation, T=Trace-driven, M=ML-augmented, H=Hybrid. $^*$Accuracy measures surrogate-vs-simulator fidelity, not real hardware error. $^\dagger$Reported accuracy unverifiable due to reproducibility issues. $^\ddagger$No accuracy baseline against real hardware reported.}
\label{tab:survey-summary}
\small
\begin{tabular}{lllllll}
\toprule
\textbf{Tool} & \textbf{Platform} & \textbf{Method} & \textbf{Target} & \textbf{Accuracy} & \textbf{Speed} & \textbf{Key Capability} \\
\midrule
\multicolumn{7}{l}{\textit{DNN Accelerator Modeling}} \\
Timeloop~\cite{timeloop2019} & NPU & A & Latency/Energy & 5--10\% & $\mu$s & Loop-nest DSE \\
MAESTRO~\cite{maestro2019} & NPU & A & Latency/Energy & 5--15\% & $\mu$s & Data-centric directives \\
Sparseloop~\cite{sparseloop2022} & NPU & A & Sparse tensors & 5--10\% & $\mu$s & Compression modeling \\
PyTorchSim~\cite{pytorchsim2025} & NPU & S & Cycle-accurate & N/A$^\ddagger$ & Hours & PyTorch 2 integration \\
ArchGym~\cite{archgym2023} & Multi & H & Multi-objective & 0.61\%$^*$ & ms & ML-aided DSE \\
\midrule
\multicolumn{7}{l}{\textit{GPU Performance Modeling}} \\
Accel-Sim~\cite{accelsim2020} & GPU & S & Cycle-accurate & 10--20\% & Hours & SASS trace-driven \\
GPGPU-Sim~\cite{gpgpusim2009} & GPU & S & Cycle-accurate & 10--20\% & Hours & CUDA workloads \\
AMALI~\cite{amali2025} & GPU & A & LLM inference & 23.6\% & ms & Memory hierarchy \\
NeuSight~\cite{neusight2025} & GPU & H & Kernel/E2E latency & 2.3\% & ms & Tile-based prediction \\
Habitat~\cite{habitat2021} & GPU & H & Training time & 11.8\% & Per-kernel & Wave scaling \\
\midrule
\multicolumn{7}{l}{\textit{Distributed Training and LLM Serving}} \\
ASTRA-sim~\cite{astrasim2023} & Distributed & T & Training time & 5--15\% & Minutes & Collective modeling \\
SimAI~\cite{simai2025} & Distributed & T & Training time & 1.9\% & Minutes & Full-stack simulation \\
Lumos~\cite{lumos2025} & Distributed & T & LLM training & 3.3\% & Minutes & H100 training \\
VIDUR~\cite{vidur2024} & GPU cluster & T & LLM serving & $<$5\% & Seconds & Prefill/decode phases \\
Frontier~\cite{frontier2025} & Distributed & T & MoE inference & --- & Minutes & Stage-centric sim. \\
TrioSim~\cite{triosim2025} & Multi-GPU & T & DNN training & N/A$^\ddagger$ & Minutes & Lightweight multi-GPU \\
\midrule
\multicolumn{7}{l}{\textit{Edge Device Modeling}} \\
nn-Meter~\cite{nnmeter2021} & Edge & M & Latency & $<$1\%$^\dagger$ & ms & Kernel detection \\
LitePred~\cite{litepred2024} & Edge & M & Latency & 0.7\% & ms & 85-platform transfer \\
HELP~\cite{help2021} & Multi & M & Latency & 1.9\% & ms & 10-sample adaptation \\
\midrule
\multicolumn{7}{l}{\textit{Compiler Cost Models}} \\
TVM~\cite{tvm2018} & GPU & M & Schedule perf. & $\sim$15\% & ms & Autotuning guidance \\
Ansor~\cite{ansor2020} & GPU & M & Schedule perf. & $\sim$15\% & ms & Program sampling \\
TLP~\cite{tlp2023} & GPU & M & Tensor program & $<$10\% & ms & Transformer cost model \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{DNN Accelerator Modeling}
\label{subsec:accelerator-modeling}

DNN accelerators' computational regularity---fixed dataflow patterns and deterministic memory access---makes this domain uniquely amenable to analytical modeling~\cite{sze2017efficient}, building on early accelerator characterization pioneered by DianNao~\cite{diannao2014}.
Timeloop~\cite{timeloop2019} exploits this regularity by enumerating loop-nest orderings that determine data reuse: because each loop permutation maps to a unique set of spatial and temporal data movements, Timeloop can analytically compute the exact number of memory accesses at each hierarchy level, achieving 5--10\% error with 2000$\times$ speedup.
MAESTRO~\cite{maestro2019} raises the abstraction level with data-centric directives that describe \emph{which} data dimensions are spatially partitioned rather than \emph{how} loops are ordered; this enables fast operator-level analysis ($\mu$s per mapping) because MAESTRO avoids enumerating individual loop nests and instead reasons about data reuse algebraically.
The trade-off: MAESTRO's abstraction sacrifices Timeloop's ability to model per-PE utilization, explaining why Timeloop achieves tighter error bounds on architectures with irregular PE arrays.
SCALE-Sim~\cite{scalesim2019} complements both by providing cycle-accurate systolic array simulation for validation.
Sparseloop~\cite{sparseloop2022} extends the loop-nest framework to sparse tensors, modeling compression formats (CSR, bitmap) and their impact on irregular memory accesses.
PyTorchSim~\cite{pytorchsim2025} integrates PyTorch~2 with NPU simulation but lacks real-hardware validation; ArchGym~\cite{archgym2023} connects ML surrogates to simulators (0.61\% RMSE vs.\ simulator, not hardware).
Accelerator modeling is the most mature subdomain, with Timeloop as the de facto DSE standard. The key gap is silicon validation; emerging PIM tools~\cite{upimulator2024,attacc2024,neupims2024,paise2025} also lack hardware validation.

\subsection{GPU Performance Modeling}
\label{subsec:gpu-modeling}

GPUs dominate ML training and inference, requiring models for SIMT execution, warp scheduling, memory coalescing, and occupancy effects.

\textbf{Cycle-accurate simulation.}
GPGPU-Sim~\cite{gpgpusim2009} and Accel-Sim~\cite{accelsim2020} achieve 0.90--0.97 IPC correlation but at $1000$--$10000\times$ slowdown; reverse-engineering~\cite{dissectinggpu2025} improved Accel-Sim to 13.98\% MAPE.
These simulators integrate with memory subsystem models---from DRAMSim2~\cite{dramsim2_2011} and Ramulator~\cite{ramulator2015} to their modern successors DRAMSim3~\cite{dramsim3_2020} and Ramulator~2.0~\cite{ramulator2_2023}---for accurate DRAM timing, critical for memory-bound LLM inference.

\textbf{Analytical and hybrid models.}
AMALI~\cite{amali2025} models GPU performance through memory hierarchy analysis---computing data movement volumes across L1, L2, and HBM levels---but its 23.6\% MAPE reveals a fundamental limitation: analytical memory models cannot capture dynamic effects (warp scheduling contention, bank conflicts, memory coalescing efficiency) that dominate real GPU execution, especially for attention kernels with irregular access patterns.
The roofline model~\cite{williams2009roofline} provides upper bounds, with recent extensions adapting it to LLM-specific workloads~\cite{rooflinellm2024}.
NeuSight~\cite{neusight2025} achieves 2.3\% on GPT-3 by decomposing kernels into tiles that mirror CUDA's actual thread block tiling: because GPU hardware schedules thread blocks as atomic units with shared memory, tile-level predictions naturally capture occupancy effects and memory locality that per-operator models miss.
This structural alignment with hardware execution---predicting per-tile latency and aggregating---is why NeuSight achieves $10\times$ lower error than AMALI despite both being sub-second tools.
Habitat~\cite{habitat2021} achieves 11.8\% cross-GPU transfer via wave scaling.

\textbf{LLM-specific and compiler models.}
VIDUR~\cite{vidur2024} simulates LLM serving with scheduling strategies at $<$5\% error; LIFE~\cite{life2025} and HERMES~\cite{hermes2025} target inference; Omniwise~\cite{omniwise2025} and SwizzlePerf~\cite{swizzleperf2025} are emerging predictors.
TVM~\cite{tvm2018}/Ansor~\cite{ansor2020} ($\sim$15\% MAPE), TLP~\cite{tlp2023} ($<$10\%), and SynPerf~\cite{synperf2025} target compiler autotuning~\cite{tenset2021}.
GPU modeling spans 2\%--24\% error; NeuSight offers the best accuracy--speed trade-off for LLMs, while AMALI fills pre-silicon gaps.

\subsection{Distributed Training and LLM Serving}
\label{subsec:distributed-modeling}

Distributed systems require modeling communication, synchronization, and parallelism strategies~\cite{megatronlm2020,gpipe2019,zero2020}.
ASTRA-sim~\cite{astrasim2023} achieves 5--15\% error via Chakra traces~\cite{chakra2023}; SimAI~\cite{simai2025} reaches 1.9\% at Alibaba scale; Echo~\cite{echo2024} scales simulation to 10K+ devices; Lumos~\cite{lumos2025} 3.3\% on H100s; PRISM~\cite{prism2025} provides prediction intervals at 10K+ GPUs.
Paleo~\cite{paleo2017} pioneered analytical estimation; MAD Max~\cite{madmax2024} and Sailor~\cite{sailor2025} extend it; Llama~3~\cite{llama3scaling2025} provides validation ground truth at 16K GPUs.
For inference serving, VIDUR~\cite{vidur2024} models scheduling with vLLM~\cite{vllm2023}; DistServe~\cite{distserve2024} disaggregates prefill and decode for goodput optimization; Frontier~\cite{frontier2025} targets MoE; POD-Attention~\cite{podattention2025} and AQUA~\cite{aqua2025} address prefill-decode overlap and memory offloading respectively; ThrottLL'eM~\cite{throttllem2025} models power effects; speculative decoding~\cite{medusa2024} creates a moving target for all simulators.

\subsection{Edge Device Modeling}
\label{subsec:edge-modeling}

Hardware diversity makes per-device analytical modeling impractical.
nn-Meter~\cite{nnmeter2021} claims $<$1\% MAPE but is unverifiable (3/10 reproducibility, Section~\ref{sec:evaluation}); LitePred~\cite{litepred2024} achieves 0.7\% across 85 platforms; HELP~\cite{help2021} reaches 1.9\% with 10-sample meta-learning.
ESM~\cite{esm2025} finds well-tuned random forests match deep learning surrogates, and transfer learning provides 22.5\% improvement~\cite{latencypredictorsnas2024}---suggesting data quality matters more than model sophistication.

\subsection{Cross-Cutting Themes}
\label{subsec:cross-cutting}

Three architectural insights emerge from our analysis.

\emph{First, structural decomposition that mirrors hardware execution consistently outperforms black-box approaches.}
Timeloop's loop nests reflect systolic array dataflow, NeuSight's tiles mirror CUDA thread block scheduling, and VIDUR's prefill/decode split captures the GPU's distinct compute- vs.\ memory-bound regimes.
In each case, the modeling abstraction succeeds because it aligns with the hardware boundary that dominates performance: data reuse at the PE array level, occupancy at the SM level, and phase-level batching at the system level.
By contrast, AMALI's single-pass memory hierarchy model misses dynamic scheduling effects that cross these boundaries, explaining its higher error.
Notably, tools with \emph{verifiable} accuracy (e.g., Timeloop's reference outputs, VIDUR's Docker-based reproduction) appear more widely adopted than tools reporting high but unverifiable accuracy (e.g., nn-Meter).

\emph{Second, the most critical architectural features for accurate ML modeling differ by platform.}
For accelerators, data reuse (determined by dataflow and tiling) dominates because accelerator designs deliberately eliminate dynamic effects.
For GPUs, thread block occupancy and memory coalescing are most critical because the SIMT execution model introduces contention not present in systolic arrays.
For distributed systems, collective communication topology and pipeline bubble overhead dominate because compute per device is well-characterized but inter-device interaction is not.
This explains why no single methodology works across all platforms.

\emph{Third, a persistent \textbf{accuracy--generality--speed trade-off}} explains methodological diversity: cycle-accurate simulators (Accel-Sim, GPGPU-Sim) maximize accuracy by modeling microarchitectural state but sacrifice speed; analytical models (Timeloop, MAESTRO) maximize speed by exploiting structural regularity but sacrifice accuracy on dynamic workloads; ML-augmented approaches (nn-Meter, LitePred) achieve both speed and accuracy on trained distributions but sacrifice generality to unseen architectures.
Subdomain maturity correlates with economic incentive: accelerator DSE is most mature (irreversible chip errors), distributed training is fastest-growing (million-dollar runs), and edge modeling has weakest reproducibility.

% ==============================================================================
% COMPARISON AND ANALYSIS
% ==============================================================================
\section{Comparison and Analysis}
\label{sec:comparison}

We analyze trade-offs across methodology types along accuracy and speed dimensions (see Table~\ref{tab:survey-summary} for per-tool details); generalization and interpretability challenges are deferred to Section~\ref{sec:challenges}.
Figure~\ref{fig:accuracy-speed} visualizes the accuracy--speed trade-off space, revealing three distinct clusters of tools.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
    xlabel={Evaluation Speed},
    ylabel={Reported MAPE (\%)},
    ymin=0, ymax=28,
    xmin=-0.5, xmax=4.5,
    xtick={0,1,2,3,4},
    xticklabels={$\mu$s, ms, Seconds, Minutes, Hours},
    xticklabel style={font=\scriptsize},
    yticklabel style={font=\scriptsize},
    xlabel style={font=\small},
    ylabel style={font=\small},
    height=6.5cm,
    width=8.5cm,
    grid=both,
    grid style={gray!20},
    legend style={at={(0.02,0.98)}, anchor=north west, font=\tiny, draw=gray!50, fill=white, fill opacity=0.9, text opacity=1},
    legend cell align={left},
]
% Analytical (blue circles)
\addplot[only marks, mark=*, mark size=3pt, blue!70, thick] coordinates {(0,7.5) (0,10) (1,23.6)};
% Simulation (red squares)
\addplot[only marks, mark=square*, mark size=3pt, red!70, thick] coordinates {(4,15)};
% Hybrid (green diamonds)
\addplot[only marks, mark=diamond*, mark size=3.5pt, green!60!black, thick] coordinates {(1,2.3) (1,11.8)};
% Trace-driven (orange triangles)
\addplot[only marks, mark=triangle*, mark size=3.5pt, orange!80!black, thick] coordinates {(2,5) (3,1.9) (3,3.3) (3,10)};
% ML-augmented (purple pentagons)
\addplot[only marks, mark=pentagon*, mark size=3pt, purple!70, thick] coordinates {(1,0.7) (1,1.9) (1,15)};
% Labels
\node[font=\tiny, anchor=west] at (0.1,7.5) {Timeloop};
\node[font=\tiny, anchor=west] at (0.1,10) {MAESTRO};
\node[font=\tiny, anchor=south west] at (1.1,23.6) {AMALI};
\node[font=\tiny, anchor=south east] at (3.9,15) {Accel-Sim};
\node[font=\tiny, anchor=south west] at (1.1,2.3) {NeuSight};
\node[font=\tiny, anchor=north west] at (1.1,11.8) {Habitat};
\node[font=\tiny, anchor=south west] at (2.1,5) {VIDUR};
\node[font=\tiny, anchor=south west] at (3.1,1.9) {SimAI};
\node[font=\tiny, anchor=south west] at (3.1,3.3) {Lumos};
\node[font=\tiny, anchor=north west] at (3.1,10) {ASTRA-sim};
\node[font=\tiny, anchor=south east] at (0.9,0.7) {LitePred};
\node[font=\tiny, anchor=north west] at (1.1,1.9) {HELP};
\node[font=\tiny, anchor=south west] at (1.1,15) {TVM};
% Pareto frontier (dashed)
\draw[thick, dashed, black!40] (0,7.5) -- (1,2.3) -- (2,5) -- (3,1.9);
\legend{Analytical, Simulation, Hybrid, Trace-driven, ML-augmented}
\end{axis}
\end{tikzpicture}%
}
\caption{Accuracy--speed trade-off across surveyed tools. Each point represents a tool's reported MAPE vs.\ evaluation speed (log-scale categories). The dashed line traces the approximate Pareto frontier. Hybrid (NeuSight) and trace-driven (SimAI) approaches dominate the frontier.}
\label{fig:accuracy-speed}
\end{figure}

\subsection{Accuracy by Problem Difficulty}
\label{subsec:accuracy-difficulty}

We organize accuracy results by inherent problem difficulty rather than comparing across incompatible benchmarks (Figure~\ref{fig:accuracy-comparison}).
Accelerator dataflow modeling is most tractable (Timeloop: 5--10\%) because systolic arrays exhibit deterministic data movement---the absence of dynamic scheduling, speculative execution, and shared caches eliminates the primary sources of modeling error.
Single-GPU kernel prediction achieves 2--12\% via hybrid methods (NeuSight, Habitat), where the accuracy gap relative to accelerators arises from GPU-specific dynamic effects: warp scheduling decisions that depend on runtime register pressure, memory coalescing efficiency that varies with access patterns, and shared memory bank conflicts that are input-dependent.
Distributed systems reach 2--15\% (SimAI 1.9\%, ASTRA-sim 5--15\%), where the dominant error source shifts from compute modeling to communication topology and collective algorithm selection.
Cross-platform edge prediction achieves 0.7--2\% but requires per-device profiling; GPU analytical modeling remains hardest (AMALI: 23.6\%).
Setup costs vary dramatically: analytical models require only architecture specifications, ML-augmented approaches need 10--10K profiling samples per device, and cycle-accurate simulators require hardware-specific binaries or traces.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
    xbar,
    y dir=reverse,
    xlabel={Reported MAPE (\%)},
    xmin=0, xmax=28,
    ytick={0,1,2,3,4,5,6,7,8,9,10,11,12},
    yticklabels={
        Timeloop,
        MAESTRO,
        AMALI,
        Accel-Sim,
        NeuSight,
        Habitat,
        SimAI,
        Lumos,
        VIDUR,
        ASTRA-sim,
        LitePred,
        HELP,
        TVM/Ansor
    },
    yticklabel style={font=\scriptsize},
    xticklabel style={font=\scriptsize},
    xlabel style={font=\small},
    bar width=6pt,
    height=7.5cm,
    width=8cm,
    nodes near coords,
    nodes near coords style={font=\tiny, anchor=west},
    every node near coord/.append style={xshift=1pt},
    legend style={at={(0.97,0.03)}, anchor=south east, font=\tiny, draw=none, fill=white, fill opacity=0.8, text opacity=1},
    legend cell align={left},
    extra y ticks={2.5, 5.5, 9.5},
    extra y tick labels={},
    extra y tick style={grid=major, grid style={black!30, dashed}},
]
% Analytical (blue)
\addplot[fill=blue!50, draw=blue!70] coordinates {(7.5,0) (10,1) (23.6,2)};
% Simulation (red)
\addplot[fill=red!50, draw=red!70] coordinates {(15,3)};
% Hybrid (green)
\addplot[fill=green!50, draw=green!70] coordinates {(2.3,4) (11.8,5)};
% Trace-driven (orange)
\addplot[fill=orange!50, draw=orange!70] coordinates {(1.9,6) (3.3,7) (5,8) (10,9)};
% ML-augmented (purple)
\addplot[fill=purple!50, draw=purple!70] coordinates {(0.7,10) (1.9,11) (15,12)};
\legend{Analytical, Simulation, Hybrid, Trace-driven, ML-augmented}
\end{axis}
\end{tikzpicture}%
}
\caption{Reported accuracy (MAPE) of surveyed tools, grouped by methodology type. Range midpoints used where ranges are reported. Cross-tool comparison is approximate due to differing benchmarks, workloads, and hardware targets.}
\label{fig:accuracy-comparison}
\end{figure}

\subsection{Practitioner Tool Selection}
\label{subsec:tool-selection}

Tool selection depends on the target platform, acceptable error margin, and available setup time; Figure~\ref{fig:tool-selection} provides a decision flowchart.
For \emph{accelerator DSE}, use Timeloop or MAESTRO for microsecond-speed exhaustive search with interpretable bottleneck feedback; Sparseloop extends this to sparse workloads.
For \emph{GPU evaluation}, NeuSight offers the best accuracy--speed balance for LLMs; use Accel-Sim when microarchitectural detail is needed, accepting the $1000\times$ slowdown.
For \emph{distributed systems}, use VIDUR for LLM serving configuration and ASTRA-sim or SimAI for training parallelism at scale; MAD Max provides fast analytical estimates when trace collection is impractical.
For \emph{edge devices}, LitePred offers the broadest platform coverage, while HELP excels with minimal profiling data.
Practitioners should consider tools with Docker-first deployment (VIDUR, Timeloop, ASTRA-sim) over tools with unpinned dependencies, as our evaluation of five tools suggests containerization correlates with higher reproducibility scores.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[
    decision/.style={diamond, draw=black!70, fill=yellow!15, text width=1.6cm, align=center, font=\scriptsize, inner sep=1pt, aspect=2},
    rec/.style={rectangle, draw=black!70, fill=green!15, rounded corners=3pt, text width=2cm, align=center, font=\scriptsize, minimum height=0.7cm},
    arr/.style={-{Stealth[length=3pt]}, thick, black!60},
    lbl/.style={font=\tiny, text=black!60},
]
% Start
\node[decision] (platform) at (0,0) {Target\\platform?};

% Branches
\node[decision] (accel_q) at (-4,-1.8) {Sparse\\workload?};
\node[decision] (gpu_q) at (-1.3,-1.8) {Need $\mu$arch\\detail?};
\node[decision] (dist_q) at (1.3,-1.8) {Training or\\serving?};
\node[decision] (edge_q) at (4,-1.8) {Profiling\\budget?};

% Platform labels
\node[lbl] at (-2.8,-0.5) {Accelerator};
\node[lbl] at (-0.8,-0.6) {GPU};
\node[lbl] at (0.7,-0.6) {Distributed};
\node[lbl] at (2.8,-0.5) {Edge};

% Arrows from platform
\draw[arr] (platform) -- (accel_q);
\draw[arr] (platform) -- (gpu_q);
\draw[arr] (platform) -- (dist_q);
\draw[arr] (platform) -- (edge_q);

% Accelerator recommendations
\node[rec, fill=blue!15] (sparseloop) at (-5.2,-3.5) {Sparseloop};
\node[rec, fill=blue!15] (timeloop) at (-2.8,-3.5) {Timeloop/\\MAESTRO};
\draw[arr] (accel_q) -- node[lbl, left] {Yes} (sparseloop);
\draw[arr] (accel_q) -- node[lbl, right] {No} (timeloop);

% GPU recommendations
\node[rec, fill=red!15] (accelsim) at (-2,-4.8) {Accel-Sim};
\node[rec, fill=green!15] (neusight) at (-0.3,-3.5) {NeuSight};
\draw[arr] (gpu_q) -- node[lbl, left] {Yes} (accelsim);
\draw[arr] (gpu_q) -- node[lbl, right] {No} (neusight);

% Distributed recommendations
\node[rec, fill=orange!15] (astrasim) at (0.5,-3.5) {ASTRA-sim/\\SimAI};
\node[rec, fill=orange!15] (vidur) at (2.2,-3.5) {VIDUR};
\draw[arr] (dist_q) -- node[lbl, left] {Train} (astrasim);
\draw[arr] (dist_q) -- node[lbl, right] {Serve} (vidur);

% Edge recommendations
\node[rec, fill=purple!15] (help) at (3.2,-3.5) {HELP};
\node[rec, fill=purple!15] (litepred) at (4.9,-3.5) {LitePred};
\draw[arr] (edge_q) -- node[lbl, left] {Low} (help);
\draw[arr] (edge_q) -- node[lbl, right] {High} (litepred);

\end{tikzpicture}%
}
\caption{Tool selection decision flowchart. Practitioners choose based on target platform, then refine by workload characteristics and resource constraints. Colors indicate methodology type: blue=analytical, green=hybrid, orange=trace-driven, purple=ML-augmented.}
\label{fig:tool-selection}
\end{figure}

% ==============================================================================
% EXPERIMENTAL EVALUATION
% ==============================================================================
\section{Experimental Evaluation}
\label{sec:evaluation}

We conducted hands-on evaluations of five tools spanning methodology types: Timeloop (analytical), ASTRA-sim (trace-driven, distributed), VIDUR (trace-driven, LLM serving), nn-Meter (ML-augmented, edge), and NeuSight (hybrid, GPU).

\textbf{Environment and rubric.}
All evaluations ran on Apple M2 Ultra (aarch64, 192\,GB RAM) using Docker containers where provided---no GPU hardware was available, so we cannot validate absolute accuracy claims.
We score each tool on a 10-point rubric: \emph{Setup} (3 pts), \emph{Reproducibility} (4 pts), \emph{Usability} (3 pts).
Table~\ref{tab:evaluation-summary} summarizes results.

\begin{table}[t]
\centering
\caption{Reproducibility evaluation scores (10-point rubric). Tools are ranked by total score. $^\dagger$Timeloop CLI works but Python bindings fail.}
\label{tab:evaluation-summary}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Tool} & \textbf{Setup} & \textbf{Reprod.} & \textbf{Usability} & \textbf{Total} \\
\midrule
VIDUR & 2.5 & 3.5 & 3 & 9/10 \\
Timeloop$^\dagger$ & 3 & 4 & 2 & 9/10 \\
ASTRA-sim & 2.5 & 3 & 3 & 8.5/10 \\
NeuSight & 2 & 3 & 2.5 & 7.5/10 \\
nn-Meter & 2 & 0 & 1 & 3/10 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Per-Tool Results}
\label{subsec:per-tool-results}

\textbf{VIDUR} (9/10).
We simulated Llama-2-7B on a simulated A100 (Table~\ref{tab:vidur-results}).
Sarathi achieves 12.2\% lower latency than vLLM via chunked prefill~\cite{sarathi2024}; TPOT differs by only 3.5\%; vLLM preempted 26.5\% of requests vs.\ zero for Sarathi---matching KV-cache management differences~\cite{vllm2023}.

\begin{table}[t]
\centering
\caption{VIDUR simulation results for Llama-2-7B inference serving on a simulated A100 GPU. All metrics from our own experiments.}
\label{tab:vidur-results}
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{vLLM} & \textbf{Sarathi} \\
\midrule
Requests & 200 & 50 \\
Avg E2E latency (s) & 0.177 & 0.158 \\
P99 E2E latency (s) & 0.320 & 0.270 \\
Avg TTFT (s) & 0.027 & 0.025 \\
Avg TPOT (s) & 0.0093 & 0.0090 \\
Requests preempted & 53 & 0 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Timeloop} (9/10).
Docker CLI produces deterministic, bit-identical outputs for Eyeriss-like configurations; reference outputs enable hardware-free verification.
Python bindings fail (\texttt{ImportError: libbarvinok.so.23}).

\textbf{ASTRA-sim} (8.5/10).
We ran collective microbenchmarks and ResNet-50 training at 2--8 GPUs (Table~\ref{tab:astrasim-results}).
Reduce-Scatter takes half the time of All-Reduce (consistent with half the data); communication overhead scales 5.76$\times$ for 4$\times$ more GPUs, matching ring All-Reduce scaling.

\begin{table}[t]
\centering
\caption{ASTRA-sim quantitative results from our experiments on the HGX-H100 configuration. Top: collective microbenchmarks (8 NPUs, 1\,MB). Bottom: ResNet-50 data-parallel training scaling.}
\label{tab:astrasim-results}
\small
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{l}{\textbf{Collective Microbenchmarks (8 NPUs, 1\,MB)}} \\
\midrule
\textbf{Collective} & \textbf{Cycles} & \textbf{Ratio vs.\ AR} \\
\midrule
All-Reduce & 57,426 & 1.000 \\
All-Gather & 44,058 & 0.767 \\
Reduce-Scatter & 28,950 & 0.504 \\
All-to-All & 114,000 & 1.985 \\
\midrule
\multicolumn{3}{l}{\textbf{ResNet-50 Data-Parallel Training}} \\
\midrule
\textbf{GPUs} & \textbf{Comm Cycles} & \textbf{Comm Overhead} \\
\midrule
2 & 574,289 & 0.05\% \\
4 & 1,454,270 & 0.13\% \\
8 & 3,307,886 & 0.30\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{NeuSight} (7.5/10).
Tile-based decomposition mirrors CUDA tiling for dense operations; irregular workloads had limited examples.

\textbf{nn-Meter} (3/10).
After four attempts ($>$4h), no predictions ran: pickle-serialized predictors (scikit-learn 0.23.1) are incompatible with current versions. The claimed $<$1\% MAPE is \textbf{unverifiable}.

Figure~\ref{fig:reproducibility-scores} visualizes the reproducibility scores, showing that Docker-first tools scored consistently higher in our evaluation.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
    xbar,
    y dir=reverse,
    xlabel={Reproducibility Score (out of 10)},
    xmin=0, xmax=11,
    ytick={0,1,2,3,4},
    yticklabels={VIDUR, Timeloop, ASTRA-sim, NeuSight, nn-Meter},
    yticklabel style={font=\small},
    xticklabel style={font=\scriptsize},
    xlabel style={font=\small},
    bar width=10pt,
    height=5cm,
    width=8cm,
    nodes near coords,
    nodes near coords style={font=\small\bfseries, anchor=west},
    every node near coord/.append style={xshift=1pt},
    extra y ticks={3.5},
    extra y tick labels={},
    extra y tick style={grid=major, grid style={red!40, thick, dashed}},
]
% Docker-first tools (green)
\addplot[fill=green!50, draw=green!70] coordinates {(9,0) (9,1) (8.5,2)};
% Non-Docker tools (red)
\addplot[fill=orange!50, draw=orange!70] coordinates {(7.5,3)};
\addplot[fill=red!50, draw=red!70] coordinates {(3,4)};
\end{axis}
% Annotations
\node[font=\tiny, text=green!50!black, align=center] at (5.5,0.1) {Docker-first};
\node[font=\tiny, text=red!60!black, align=center] at (5.5,3.2) {Pickle-based};
\end{tikzpicture}%
}
\caption{Reproducibility scores for evaluated tools. Docker-first tools (VIDUR, Timeloop, ASTRA-sim) consistently score 8.5+/10, while tools relying on serialized ML models (nn-Meter) become unusable. The dashed line separates Docker-based from non-Docker deployments.}
\label{fig:reproducibility-scores}
\end{figure}

\subsection{Lessons and Threats to Validity}
\label{subsec:eval-lessons}

Five lessons emerge:
(1)~\textbf{Docker-first deployment} correlates with high reproducibility in our sample (Docker tools: 8.5+/10; nn-Meter without Docker: 3/10), though our sample of five tools limits causal conclusions.
(2)~\textbf{ML model serialization is fragile}---nn-Meter's pickle-based predictors became unusable within two years.
(3)~\textbf{Reference outputs enable trust without hardware}---Timeloop and ASTRA-sim include verifiable baselines.
(4)~\textbf{Scale-limited evaluation understates system tools}---our 2--8 GPU tests show only 0.30\% communication overhead, far below production scales~\cite{llama3scaling2025}.
(5)~\textbf{Reproducible accuracy claims should be weighted higher} than unreproducible ones.

\textbf{Threats.}
Our venue-focused search may under-represent industry and non-English publications; we exclude proprietary tools (Nsight Compute, internal TPU models); and accuracy metrics vary across papers (MAPE, RMSE, Kendall's $\tau$), limiting direct comparison.

% ==============================================================================
% OPEN CHALLENGES
% ==============================================================================
\section{Open Challenges and Future Directions}
\label{sec:challenges}

\textbf{Generalization gaps.}
\emph{Workload}: CNN$\rightarrow$transformer transfer is largely unvalidated (NeuSight excepted); MoE, diffusion~\cite{dynamicreasoning2026}, and dynamic inference lack validated tools; scaling laws~\cite{kaplan2020scaling,hoffmann2022chinchilla,scalinglaws2024,scalinglawguide2025} predict loss but not latency.
Figure~\ref{fig:workload-coverage} shows the shift toward LLM workloads since 2023.
\emph{Hardware}: cross-family transfer (GPU$\rightarrow$TPU$\rightarrow$PIM) remains unsolved despite meta-learning (HELP) and feature-based transfer (LitePred).
\emph{Temporal}: software stack evolution silently invalidating models is addressed by no tool.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}
\begin{axis}[
    ybar stacked,
    bar width=14pt,
    xlabel={Publication year},
    ylabel={Number of tools},
    ymin=0, ymax=16,
    xtick={2016,2018,2020,2022,2024,2026},
    xticklabels={2016--17,2018--19,2020--21,2022--23,2024--25,2026},
    xticklabel style={font=\scriptsize},
    yticklabel style={font=\scriptsize},
    xlabel style={font=\small},
    ylabel style={font=\small},
    legend style={at={(0.02,0.98)}, anchor=north west, font=\tiny, draw=none, fill=white, fill opacity=0.9, text opacity=1, legend columns=2},
    legend cell align={left},
    height=5.5cm,
    width=8.5cm,
    enlarge x limits={abs=0.8cm},
]
\addplot[fill=blue!50, draw=blue!70] coordinates {(2016,2) (2018,2) (2020,4) (2022,3) (2024,2) (2026,0)};
\addplot[fill=orange!50, draw=orange!70] coordinates {(2016,0) (2018,1) (2020,0) (2022,2) (2024,3) (2026,0)};
\addplot[fill=red!50, draw=red!70] coordinates {(2016,0) (2018,0) (2020,1) (2022,1) (2024,8) (2026,2)};
\addplot[fill=green!50, draw=green!70] coordinates {(2016,0) (2018,0) (2020,1) (2022,0) (2024,2) (2026,1)};
\legend{CNN only, CNN+Transformer, LLM/Distributed, Cross-workload}
\end{axis}
\end{tikzpicture}%
}
\caption{Workload coverage of surveyed tools by publication period. The shift toward transformer and LLM workloads accelerates from 2023, but MoE and diffusion models remain largely uncharacterized.}
\label{fig:workload-coverage}
\end{figure}

\textbf{The composition problem.}
Composing kernel-level predictions into end-to-end estimates is unsolved (Figure~\ref{fig:error-composition}): NeuSight's 2.3\% kernel MAPE yields $\sim$$10\times$ higher variance at model level ($\sigma_{\text{model}} \approx \sigma_{\text{kernel}} \cdot \sqrt{N}$), and correlated errors can compound linearly.
VIDUR sidesteps this by profiling entire prefill/decode phases.

\begin{figure}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tikzpicture}[
    box/.style={draw=black!60, rounded corners=2pt, minimum width=1.2cm, minimum height=0.5cm, align=center, font=\tiny},
    err/.style={font=\tiny\itshape, text=red!60!black},
    arr/.style={-{Stealth[length=3pt]}, thick, black!50},
    brace/.style={decorate, decoration={brace, amplitude=4pt, mirror}, thick, black!40},
]

% Kernel level
\node[box, fill=blue!10] (k1) at (0,0) {Conv};
\node[box, fill=blue!10] (k2) at (1.5,0) {Attn};
\node[box, fill=blue!10] (k3) at (3,0) {FFN};
\node[box, fill=blue!10] (k4) at (4.5,0) {Norm};
\node[box, fill=blue!10] (k5) at (6,0) {Softmax};
\node[err] at (0,-0.55) {2.3\%};
\node[err] at (1.5,-0.55) {2.1\%};
\node[err] at (3,-0.55) {1.8\%};
\node[err] at (4.5,-0.55) {3.5\%};
\node[err] at (6,-0.55) {2.0\%};
\node[font=\scriptsize\bfseries, anchor=east] at (-0.8,0) {Kernel};

% Composition arrows
\draw[arr] (0.5,0) -- (1,0);
\draw[arr] (2,0) -- (2.5,0);
\draw[arr] (3.5,0) -- (4,0);
\draw[arr] (5,0) -- (5.5,0);

% Hidden errors
\node[box, fill=yellow!20, dashed, draw=orange!60] (h1) at (0.75,0.8) {Launch\\overhead};
\node[box, fill=yellow!20, dashed, draw=orange!60] (h2) at (2.25,0.8) {Mem\\alloc};
\node[box, fill=yellow!20, dashed, draw=orange!60] (h3) at (3.75,0.8) {Data\\movement};
\node[box, fill=yellow!20, dashed, draw=orange!60] (h4) at (5.25,0.8) {Sync\\barrier};

% Model level
\node[box, fill=green!10, minimum width=6.5cm] (model) at (3,1.9) {Model-level prediction};
\node[err] at (3,1.4) {5--12\% (hidden overhead accumulates)};
\node[font=\scriptsize\bfseries, anchor=east] at (-0.8,1.9) {Model};

% System level
\node[box, fill=orange!10, minimum width=6.5cm] (system) at (3,3.0) {System-level prediction};
\node[err] at (3,2.5) {+ communication, scheduling, contention};
\node[font=\scriptsize\bfseries, anchor=east] at (-0.8,3.0) {System};
\node[err] at (3,3.55) {5--15\% total error};

% Braces
\draw[brace] (-0.5,-0.8) -- (6.5,-0.8) node[midway, below=5pt, font=\tiny, text=red!60!black] {$\sigma_{\text{model}} \approx \sigma_{\text{kernel}} \cdot \sqrt{N}$ (uncorrelated) to $N \cdot \sigma_{\text{kernel}}$ (correlated)};

\end{tikzpicture}%
}
\caption{Error composition across abstraction levels. Kernel-level predictions (2--3\% each) accumulate through hidden overheads (kernel launch, memory allocation, data movement, synchronization) that are not captured by kernel-level tools, yielding 5--12\% model-level error. System-level errors add communication and scheduling overhead.}
\label{fig:error-composition}
\end{figure}

\textbf{Emerging hardware and reproducibility.}
PIM~\cite{upimulator2024,attacc2024,neupims2024,paise2025}, chiplets, and disaggregated designs blur memory hierarchy assumptions; FlashAttention~\cite{flashattention2022} changes the landscape faster than models retrain.
No MLPerf~\cite{mlperf_training2020,mlperf_inference2020} equivalent exists for performance \emph{prediction}.

\textbf{Future directions}: (1)~validated non-CNN tools; (2)~bounded composition error; (3)~unified energy-latency-memory prediction~\cite{mlperfpower2025}; (4)~temporal robustness benchmarks; (5)~Docker-first deployment with portable formats (ONNX, Chakra~\cite{chakra2023}).

% ==============================================================================
% CONCLUSION
% ==============================================================================
\section{Conclusion}
\label{sec:conclusion}

This survey analyzed 22 tools in depth for predicting ML workload performance, organized by methodology type, target platform, and abstraction level.
Key findings:
(1)~\emph{No single methodology dominates}---analytical models offer microsecond interpretable evaluation, trace-driven simulators provide 2--15\% system-level error, and hybrid approaches achieve the best accuracy--speed balance (NeuSight: 2.3\% MAPE), with the right choice depending on the practitioner's priorities.
(2)~\emph{LLM workloads demand specialized modeling}---prefill/decode distinctions, KV cache management, and dynamic batching require purpose-built tools (VIDUR, Frontier) rather than CNN-era extensions.
(3)~\emph{Reproducibility is a practical bottleneck}---Docker-first tools score 8.5+/10 while tools relying on serialized ML models have become unusable.
(4)~\emph{Accuracy claims require scrutiny} due to varying benchmarks and metrics.

The most pressing gaps are CNN-to-transformer generalization, kernel-to-end-to-end composition, emerging hardware support (PIM, chiplets), and reproducibility failures.
As ML workloads grow in scale and diversity, this survey provides practitioners guidance for tool selection and researchers a roadmap for advancing the field.

%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
