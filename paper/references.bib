% Bibliography for ML Performance Models Survey

% ============================================================================
% Foundational and Traditional Modeling
% ============================================================================

@article{williams2009roofline,
  author    = {Williams, Samuel and Waterman, Andrew and Patterson, David},
  title     = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
  journal   = {Communications of the ACM},
  volume    = {52},
  number    = {4},
  pages     = {65--76},
  year      = {2009},
  doi       = {10.1145/1498765.1498785}
}

@article{binkert2011gem5,
  author    = {Binkert, Nathan and Beckmann, Bradford and Black, Gabriel and Reinhardt, Steven K. and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and Hower, Derek R. and Krishna, Tushar and Sardashti, Somayeh and Sen, Rathijit and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. and Wood, David A.},
  title     = {The gem5 Simulator},
  journal   = {ACM SIGARCH Computer Architecture News},
  volume    = {39},
  number    = {2},
  pages     = {1--7},
  year      = {2011},
  doi       = {10.1145/2024716.2024718}
}

% ============================================================================
% ML-Based Performance Modeling - GPUs
% ============================================================================

@inproceedings{neusight2025,
  author    = {Lee, Seunghyun and Phanishayee, Amar and Mahajan, Divya},
  title     = {{NeuSight}: {GPU} Performance Forecasting via Tile-Based Execution Analysis},
  booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2025},
  pages     = {1--15}
}

@inproceedings{habitat2021,
  author    = {Yu, Geoffrey X. and Gao, Yubo and Golber, Pavel and Cidon, Asaf},
  title     = {Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training},
  booktitle = {Proceedings of the USENIX Annual Technical Conference (ATC)},
  year      = {2021},
  pages     = {503--521}
}

% ============================================================================
% ML-Based Performance Modeling - Edge Devices
% ============================================================================

@inproceedings{nnmeter2021,
  author    = {Zhang, Li Lyna and Han, Shihao and Wei, Jianyu and Zheng, Ningxin and Cao, Ting and Yang, Yuqing and Liu, Yunxin},
  title     = {nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices},
  booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)},
  year      = {2021},
  pages     = {81--93},
  note      = {Best Paper Award},
  doi       = {10.1145/3458864.3467882}
}

@inproceedings{litepred2024,
  author    = {Feng, Yang and Li, Zhehao and Yang, Jiacheng and Liu, Yunxin},
  title     = {{LitePred}: Transferable and Scalable Latency Prediction for Hardware-Aware Neural Architecture Search},
  booktitle = {Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
  year      = {2024},
  pages     = {1--18}
}

% ============================================================================
% Analytical Models for Accelerators
% ============================================================================

@inproceedings{eyeriss2016,
  author    = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  title     = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
  booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture (ISCA)},
  year      = {2016},
  pages     = {367--379},
  doi       = {10.1109/ISCA.2016.40}
}

@inproceedings{timeloop2019,
  author    = {Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Muber, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},
  title     = {Timeloop: A Systematic Approach to {DNN} Accelerator Evaluation},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2019},
  pages     = {304--315},
  doi       = {10.1109/ISPASS.2019.00042}
}

@inproceedings{maestro2019,
  author    = {Kwon, Hyoukjun and Chatarasi, Prasanth and Sarber, Michael and Pellauer, Michael and Parashar, Angshuman and Krishna, Tushar},
  title     = {{MAESTRO}: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of {DNN} Mappings},
  booktitle = {Proceedings of the 52nd IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2019},
  pages     = {1--14},
  doi       = {10.1145/3352460.3358292}
}

@inproceedings{sparseloop2022,
  author    = {Wu, Yannan Nellie and Emer, Joel and Sze, Vivienne},
  title     = {Sparseloop: An Analytical Approach to Sparse Tensor Accelerator Modeling},
  booktitle = {Proceedings of the 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2022},
  pages     = {1--15},
  doi       = {10.1109/MICRO56248.2022.00078}
}

% ============================================================================
% Compiler Cost Models
% ============================================================================

@inproceedings{tvm2018,
  author    = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  title     = {{TVM}: An Automated End-to-End Optimizing Compiler for Deep Learning},
  booktitle = {Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2018},
  pages     = {578--594}
}

@inproceedings{ansor2020,
  author    = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, Ion},
  title     = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
  booktitle = {Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2020},
  pages     = {863--879}
}

% ============================================================================
% LLM Inference Modeling
% ============================================================================

@inproceedings{vidur2024,
  author    = {Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S. and Ramachandran, Ramachandran},
  title     = {{VIDUR}: A Large-Scale Simulation Framework for {LLM} Inference},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  pages     = {1--15}
}

% ============================================================================
% Transfer Learning and Cross-Platform
% ============================================================================

@inproceedings{latencypredictorsnas2024,
  author    = {Dudziak, Lukasz and Chau, Thomas and Abdelfattah, Mohamed S. and Lee, Royson and Kim, Hyeji and Lane, Nicholas D.},
  title     = {Latency Predictors for Neural Architecture Search},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  pages     = {1--14}
}

% ============================================================================
% Hybrid Approaches
% ============================================================================

@inproceedings{archgym2023,
  author    = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jouppi, Norman P. and Parmar, Jignesh and Kim, Hyoukjun and Laudon, James and Narayanaswami, Chandrakant},
  title     = {{ArchGym}: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
  booktitle = {Proceedings of the 50th International Symposium on Computer Architecture (ISCA)},
  year      = {2023},
  pages     = {1--16},
  doi       = {10.1145/3579371.3589049}
}

% ============================================================================
% GPU Simulation
% ============================================================================

@inproceedings{gpgpusim2009,
  author    = {Bakhoda, Ali and Yuan, George L. and Fung, Wilson W. L. and Wong, Henry and Aamodt, Tor M.},
  title     = {Analyzing {CUDA} Workloads Using a Detailed {GPU} Simulator},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2009},
  pages     = {163--174},
  doi       = {10.1109/ISPASS.2009.4919648}
}

@inproceedings{accelsim2020,
  author    = {Khairy, Mahmoud and Shen, Zhesheng and Aamodt, Tor M. and Rogers, Timothy G.},
  title     = {Accel-Sim: An Extensible Simulation Framework for Validated {GPU} Modeling},
  booktitle = {Proceedings of the 47th International Symposium on Computer Architecture (ISCA)},
  year      = {2020},
  pages     = {473--486},
  doi       = {10.1109/ISCA45697.2020.00047}
}

% ============================================================================
% Distributed Training Simulation
% ============================================================================

@inproceedings{astrasim2020,
  author    = {Rashidi, Saeed and Srinivasan, Srinivas and Hamedani, Kazem and Krishna, Tushar},
  title     = {{ASTRA-SIM}: Enabling {SW/HW} Co-Design Exploration for Distributed {DL} Training Platforms},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2020},
  pages     = {81--92},
  doi       = {10.1109/ISPASS48437.2020.00018}
}

@inproceedings{astrasim2023,
  author    = {Won, William and Heo, Taekyung and Rashidi, Saeed and Talati, Saeed and Srinivasan, Srinivas and Krishna, Tushar},
  title     = {{ASTRA-sim2.0}: Modeling Hierarchical Networks and Disaggregated Systems for Large-Model Training at Scale},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2023},
  pages     = {283--294},
  doi       = {10.1109/ISPASS57527.2023.00035}
}

@article{chakra2023,
  author    = {Sridharan, Srinivas and Heo, Taekyung and Choi, Jinwoo and Yu, Garyfallia and Rashidi, Saeed and Won, William and Meng, Zhaodong and Krishna, Tushar},
  title     = {Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces},
  journal   = {arXiv preprint arXiv:2305.14516},
  year      = {2023}
}

@article{echo2024,
  author    = {Cai, Kai and Miao, Wei and Zhu, Junyu and Chen, Jiaxu and Shan, Hao and Li, Huanyu and Zhang, Chi},
  title     = {Echo: Simulating Distributed Training At Scale},
  journal   = {arXiv preprint arXiv:2412.12487},
  year      = {2024}
}

% ============================================================================
% GNN-Based Performance Modeling
% ============================================================================

@inproceedings{granite2022,
  author    = {Sykora, Ondrej and Rucker, Alexis and Mendis, Charith and Barik, Rajkishore and Phothilimthana, Phitchaya Mangpo and Amarasinghe, Saman},
  title     = {GRANITE: A Graph Neural Network Model for Basic Block Throughput Estimation},
  booktitle = {Proceedings of the IEEE International Symposium on Workload Characterization (IISWC)},
  year      = {2022},
  pages     = {1--13},
  doi       = {10.1109/IISWC55918.2022.00014}
}

% ============================================================================
% Meta-Learning and Transfer Learning
% ============================================================================

@inproceedings{help2021,
  author    = {Lee, Hayeon and Lee, Sewoong and Chong, Song and Hwang, Sung Ju},
  title     = {{HELP}: Hardware-Adaptive Efficient Latency Prediction for {NAS} via Meta-Learning},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {34},
  year      = {2021},
  pages     = {27016--27028}
}

% ============================================================================
% Additional References for Section 4
% ============================================================================

@inproceedings{tenset2021,
  author    = {Zheng, Lianmin and Liu, Ruochen and Shao, Junru and Chen, Tianqi and Gonzalez, Joseph E. and Stoica, Ion and Zhang, Zhihao},
  title     = {{TenSet}: A Large-scale Program Performance Dataset for Learned Tensor Compilers},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {34},
  year      = {2021},
  pages     = {29876--29888}
}

@inproceedings{vllm2023,
  author    = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
  title     = {Efficient Memory Management for Large Language Model Serving with {PagedAttention}},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles (SOSP)},
  year      = {2023},
  pages     = {611--626},
  doi       = {10.1145/3600006.3613165}
}

@inproceedings{flashattention2022,
  author    = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
  title     = {{FlashAttention}: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  year      = {2022},
  pages     = {16344--16359}
}

@article{synperf2025,
  author    = {Wang, Zixian and others},
  title     = {{SynPerf}: Synthesizing High-Performance GPU Kernels via Pipeline Decomposition},
  journal   = {arXiv preprint},
  year      = {2025},
  note      = {Under review}
}

@inproceedings{medusa2024,
  author    = {Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D. and Chen, Deming and Dao, Tri},
  title     = {{MEDUSA}: Simple {LLM} Inference Acceleration Framework with Multiple Decoding Heads},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year      = {2024},
  pages     = {1--15}
}

@inproceedings{orca2022,
  author    = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  title     = {{ORCA}: A Distributed Serving System for Transformer-Based Generative Models},
  booktitle = {Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2022},
  pages     = {521--538}
}

@inproceedings{distserve2024,
  author    = {Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianyu and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  title     = {{DistServe}: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving},
  booktitle = {Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2024},
  pages     = {1--18}
}

@article{rooflinellm2024,
  author    = {Imai, Ryota and Harada, Kentaro and Sato, Ryo and Nakaike, Toshio},
  title     = {Roofline-Driven Machine Learning for Large Language Model Performance Prediction},
  journal   = {NeurIPS Workshop on Machine Learning for Systems},
  year      = {2024}
}

% ============================================================================
% Additional Key Papers - Added by Maya
% ============================================================================

@inproceedings{paleo2017,
  author    = {Qi, Hang and Sparks, Evan R. and Talwalkar, Ameet},
  title     = {Paleo: A Performance Model for Deep Neural Networks},
  booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
  year      = {2017},
  url       = {https://openreview.net/forum?id=SyVVJ85lg}
}

@inproceedings{splitwise2024,
  author    = {Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aakanksha and Goiri, \'I\~nigo and Maleki, Saeed and Bianchini, Ricardo},
  title     = {Splitwise: Efficient Generative {LLM} Inference Using Phase Splitting},
  booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2024},
  pages     = {118--132},
  note      = {Best Paper Award},
  doi       = {10.1109/ISCA59077.2024.00019}
}

@inproceedings{sarathi2024,
  author    = {Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S. and Tumanov, Alexey and Ramachandran, Ramachandran},
  title     = {Taming Throughput-Latency Tradeoff in {LLM} Inference with {Sarathi-Serve}},
  booktitle = {Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2024},
  pages     = {117--134}
}

@inproceedings{madmax2024,
  author    = {Hsia, Samuel and Chandra, Kartik and Olukotun, Kunle},
  title     = {{MAD} Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems},
  booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2024},
  pages     = {753--766},
  doi       = {10.1109/ISCA59077.2024.00064}
}

% ============================================================================
% Tensor Program Cost Models (Tool: TLP)
% ============================================================================

@inproceedings{tlp2023,
  author    = {Zhai, Yi and Wang, Yu Cheng and Jiang, Peng and Kang, Congming},
  title     = {{TLP}: A Deep Learning-based Cost Model for Tensor Program Tuning},
  booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2023},
  pages     = {833--845},
  doi       = {10.1145/3575693.3575736}
}

% ============================================================================
% Recent 2025-2026 Papers - Added by Maya
% ============================================================================

@inproceedings{life2025,
  author    = {Gavriilidis, Paraskevas and others},
  title     = {{LIFE}: Forecasting {LLM} Inference Performance via Hardware-Agnostic Analytical Modeling},
  booktitle = {arXiv preprint arXiv:2508.00904},
  year      = {2025},
  note      = {Hardware-agnostic analytical model for LLM inference performance forecasting}
}

@inproceedings{throttllem2025,
  author    = {Kakolyris, Andreas Kosmas and Masouros, Dimosthenis and Vavaroutsos, Petros and Xydis, Sotirios and Soudris, Dimitrios},
  title     = {{throttLL'eM}: Predictive {GPU} Throttling for Energy Efficient {LLM} Inference Serving},
  booktitle = {Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year      = {2025},
  pages     = {1--14},
  note      = {Achieves up to 43.8\% lower energy consumption for LLM inference}
}

@inproceedings{aqua2025,
  author    = {Shen, Zhuomin and Kim, Jaeho and others},
  title     = {{AQUA}: Network-Accelerated Memory Offloading for {LLMs} in Scale-Up {GPU} Domains},
  booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2025},
  pages     = {1--16},
  doi       = {10.1145/3676641.3715983},
  note      = {Improves LLM inference responsiveness by 20x through network-accelerated memory offloading}
}

@inproceedings{esm2025,
  author    = {various},
  title     = {{ESM}: A Framework for Building Effective Surrogate Models for Hardware-Aware Neural Architecture Search},
  booktitle = {Proceedings of the Design Automation Conference (DAC)},
  year      = {2025},
  note      = {97.6\% accuracy surrogate model framework for HW-aware NAS}
}

% ============================================================================
% 2025-2026 Papers - Added by Maya (Cycle 2)
% ============================================================================

@article{omniwise2025,
  author    = {Ameer Haj-Ali and others},
  title     = {{Omniwise}: Predicting {GPU} Kernels Performance with {LLMs}},
  journal   = {arXiv preprint arXiv:2506.20886},
  year      = {2025},
  note      = {First LLM-based GPU kernel performance prediction, 90\% within 10\% error on AMD MI250/MI300X}
}

@article{hermes2025,
  author    = {Bambhaniya, Abhimanyu Rajeshkumar and others},
  title     = {{HERMES}: Understanding and Optimizing Multi-Stage {AI} Inference Pipelines},
  journal   = {arXiv preprint arXiv:2504.09775},
  year      = {2025},
  note      = {Heterogeneous multi-stage LLM inference simulator with analytical modeling}
}

@inproceedings{podattention2025,
  author    = {Hao, Yanbin and others},
  title     = {{POD-Attention}: Unlocking Full Prefill-Decode Overlap for Faster {LLM} Inference},
  booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2025},
  pages     = {1--15},
  note      = {Full overlap between prefill and decode phases for LLM inference}
}

% ============================================================================
% 2025-2026 Papers - Added by Maya (Cycle 3)
% ============================================================================

@article{frontier2025,
  author    = {Ghosh, Siddharth and others},
  title     = {{Frontier}: Simulating the Next Generation of {LLM} Inference Systems},
  journal   = {arXiv preprint arXiv:2508.03148},
  year      = {2025},
  note      = {Stage-centric simulator for MoE and disaggregated LLM inference, models expert parallelism and cross-cluster routing}
}

@article{swizzleperf2025,
  author    = {Tschand, Adrian and Awad, Mohamed and others},
  title     = {{SwizzlePerf}: Hardware-Aware {LLMs} for {GPU} Kernel Performance Optimization},
  journal   = {arXiv preprint arXiv:2508.20258},
  year      = {2025},
  note      = {LLM-based spatial optimization for GPU kernels, up to 2.06x speedup via swizzling}
}

% ============================================================================
% ISCA 2025 / MLSys 2025 / MICRO 2025 / HPCA 2026 Papers - Added by Sage
% ============================================================================

@inproceedings{concorde2025,
  author    = {Nasr-Esfahany, Amir and others},
  title     = {Concorde: Fast and Accurate {CPU} Performance Modeling with Compositional Analytical-{ML} Fusion},
  booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2025},
  pages     = {1--15},
  note      = {Hybrid analytical-ML approach achieving 2\% CPI error at 5 orders of magnitude faster than gem5}
}

@inproceedings{amali2025,
  author    = {Cao, Zheng and others},
  title     = {{AMALI}: An Analytical Model for Accurately Modeling {LLM} Inference on Modern {GPUs}},
  booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2025},
  pages     = {1--14},
  doi       = {10.1145/3695053.3731064},
  note      = {Reduces GPU LLM inference MAPE from 127.56\% to 23.59\% vs GCoM baseline}
}

@inproceedings{triosim2025,
  author    = {Li, Jianbo and others},
  title     = {{TrioSim}: A Lightweight Simulator for Large-Scale {DNN} Workloads on Multi-{GPU} Systems},
  booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2025},
  pages     = {1--13},
  note      = {Multi-GPU DNN simulation with lightweight approach for distributed training analysis}
}

@inproceedings{lumos2025,
  author    = {Liang, Wenxuan and others},
  title     = {Lumos: Efficient Performance Modeling and Estimation for Large-scale {LLM} Training},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2025},
  pages     = {1--16},
  note      = {Trace-driven performance modeling achieving 3.3\% error on H100 GPUs for LLM training}
}

@inproceedings{pytorchsim2025,
  author    = {Kim, Jungho and others},
  title     = {{PyTorchSim}: A Comprehensive, Fast, and Accurate {NPU} Simulation Framework},
  booktitle = {Proceedings of the 58th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2025},
  pages     = {1--14},
  doi       = {10.1145/3725843.3756045},
  note      = {PyTorch 2-integrated NPU simulator with custom RISC-V ISA and Tile-Level Simulation}
}

@inproceedings{dynamicreasoning2026,
  author    = {Kim, Jiin and Shin, Byeongjun and Chung, Jinha and Rhu, Minsoo},
  title     = {The Cost of Dynamic Reasoning: Demystifying {AI} Agents and Test-Time Scaling from an {AI} Infrastructure Perspective},
  booktitle = {Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year      = {2026},
  month     = jan,
  pages     = {1--14},
  note      = {HPCA 2026 (Jan 31--Feb 4, 2026, Las Vegas). First comprehensive system-level analysis of AI agents; quantifies resource usage, latency, and datacenter power consumption}
}
