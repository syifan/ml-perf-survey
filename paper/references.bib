% Bibliography for ML Performance Models Survey

% ============================================================================
% Foundational and Traditional Modeling
% ============================================================================

@article{williams2009roofline,
  author    = {Williams, Samuel and Waterman, Andrew and Patterson, David},
  title     = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
  journal   = {Communications of the ACM},
  volume    = {52},
  number    = {4},
  pages     = {65--76},
  year      = {2009},
  doi       = {10.1145/1498765.1498785}
}

@article{binkert2011gem5,
  author    = {Binkert, Nathan and Beckmann, Bradford and Black, Gabriel and Reinhardt, Steven K. and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and Hower, Derek R. and Krishna, Tushar and Sardashti, Somayeh and Sen, Rathijit and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. and Wood, David A.},
  title     = {The gem5 Simulator},
  journal   = {ACM SIGARCH Computer Architecture News},
  volume    = {39},
  number    = {2},
  pages     = {1--7},
  year      = {2011},
  doi       = {10.1145/2024716.2024718}
}

% ============================================================================
% ML-Based Performance Modeling - GPUs
% ============================================================================

@inproceedings{neusight2025,
  author    = {Lee, Seunghyun and Phanishayee, Amar and Mahajan, Divya},
  title     = {{NeuSight}: {GPU} Performance Forecasting via Tile-Based Execution Analysis},
  booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2025},
  pages     = {1--15}
}

@inproceedings{habitat2021,
  author    = {Yu, Geoffrey X. and Gao, Yubo and Golber, Pavel and Cidon, Asaf},
  title     = {Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training},
  booktitle = {Proceedings of the USENIX Annual Technical Conference (ATC)},
  year      = {2021},
  pages     = {503--521}
}

% ============================================================================
% ML-Based Performance Modeling - Edge Devices
% ============================================================================

@inproceedings{nnmeter2021,
  author    = {Zhang, Li Lyna and Han, Shihao and Wei, Jianyu and Zheng, Ningxin and Cao, Ting and Yang, Yuqing and Liu, Yunxin},
  title     = {nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices},
  booktitle = {Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys)},
  year      = {2021},
  pages     = {81--93},
  note      = {Best Paper Award},
  doi       = {10.1145/3458864.3467882}
}

@inproceedings{litepred2024,
  author    = {Feng, Yang and Li, Zhehao and Yang, Jiacheng and Liu, Yunxin},
  title     = {{LitePred}: Transferable and Scalable Latency Prediction for Hardware-Aware Neural Architecture Search},
  booktitle = {Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
  year      = {2024},
  pages     = {1--18}
}

% ============================================================================
% Analytical Models for Accelerators
% ============================================================================

@inproceedings{eyeriss2016,
  author    = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  title     = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
  booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture (ISCA)},
  year      = {2016},
  pages     = {367--379},
  doi       = {10.1109/ISCA.2016.40}
}

@inproceedings{timeloop2019,
  author    = {Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Muber, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},
  title     = {Timeloop: A Systematic Approach to {DNN} Accelerator Evaluation},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2019},
  pages     = {304--315},
  doi       = {10.1109/ISPASS.2019.00042}
}

@inproceedings{maestro2019,
  author    = {Kwon, Hyoukjun and Chatarasi, Prasanth and Sarber, Michael and Pellauer, Michael and Parashar, Angshuman and Krishna, Tushar},
  title     = {{MAESTRO}: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of {DNN} Mappings},
  booktitle = {Proceedings of the 52nd IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2019},
  pages     = {1--14},
  doi       = {10.1145/3352460.3358292}
}

@inproceedings{sparseloop2022,
  author    = {Wu, Yannan Nellie and Emer, Joel and Sze, Vivienne},
  title     = {Sparseloop: An Analytical Approach to Sparse Tensor Accelerator Modeling},
  booktitle = {Proceedings of the 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2022},
  pages     = {1--15},
  doi       = {10.1109/MICRO56248.2022.00078}
}

% ============================================================================
% Compiler Cost Models
% ============================================================================

@inproceedings{tvm2018,
  author    = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  title     = {{TVM}: An Automated End-to-End Optimizing Compiler for Deep Learning},
  booktitle = {Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2018},
  pages     = {578--594}
}

@inproceedings{ansor2020,
  author    = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, Ion},
  title     = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
  booktitle = {Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2020},
  pages     = {863--879}
}

% ============================================================================
% LLM Inference Modeling
% ============================================================================

@inproceedings{vidur2024,
  author    = {Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S. and Ramachandran, Ramachandran},
  title     = {{VIDUR}: A Large-Scale Simulation Framework for {LLM} Inference},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  pages     = {1--15}
}

% ============================================================================
% Transfer Learning and Cross-Platform
% ============================================================================

@inproceedings{latencypredictorsnas2024,
  author    = {Dudziak, Lukasz and Chau, Thomas and Abdelfattah, Mohamed S. and Lee, Royson and Kim, Hyeji and Lane, Nicholas D.},
  title     = {Latency Predictors for Neural Architecture Search},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2024},
  pages     = {1--14}
}

% ============================================================================
% Hybrid Approaches
% ============================================================================

@inproceedings{archgym2023,
  author    = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jouppi, Norman P. and Parmar, Jignesh and Kim, Hyoukjun and Laudon, James and Narayanaswami, Chandrakant},
  title     = {{ArchGym}: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
  booktitle = {Proceedings of the 50th International Symposium on Computer Architecture (ISCA)},
  year      = {2023},
  pages     = {1--16},
  doi       = {10.1145/3579371.3589049}
}

% ============================================================================
% GPU Simulation
% ============================================================================

@inproceedings{gpgpusim2009,
  author    = {Bakhoda, Ali and Yuan, George L. and Fung, Wilson W. L. and Wong, Henry and Aamodt, Tor M.},
  title     = {Analyzing {CUDA} Workloads Using a Detailed {GPU} Simulator},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2009},
  pages     = {163--174},
  doi       = {10.1109/ISPASS.2009.4919648}
}

@inproceedings{accelsim2020,
  author    = {Khairy, Mahmoud and Shen, Zhesheng and Aamodt, Tor M. and Rogers, Timothy G.},
  title     = {Accel-Sim: An Extensible Simulation Framework for Validated {GPU} Modeling},
  booktitle = {Proceedings of the 47th International Symposium on Computer Architecture (ISCA)},
  year      = {2020},
  pages     = {473--486},
  doi       = {10.1109/ISCA45697.2020.00047}
}

% ============================================================================
% Distributed Training Simulation
% ============================================================================

@inproceedings{astrasim2020,
  author    = {Rashidi, Saeed and Srinivasan, Srinivas and Hamedani, Kazem and Krishna, Tushar},
  title     = {{ASTRA-SIM}: Enabling {SW/HW} Co-Design Exploration for Distributed {DL} Training Platforms},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2020},
  pages     = {81--92},
  doi       = {10.1109/ISPASS48437.2020.00018}
}

@inproceedings{astrasim2023,
  author    = {Won, William and Heo, Taekyung and Rashidi, Saeed and Talati, Saeed and Srinivasan, Srinivas and Krishna, Tushar},
  title     = {{ASTRA-sim2.0}: Modeling Hierarchical Networks and Disaggregated Systems for Large-Model Training at Scale},
  booktitle = {Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2023},
  pages     = {283--294},
  doi       = {10.1109/ISPASS57527.2023.00035}
}

@article{chakra2023,
  author    = {Sridharan, Srinivas and Heo, Taekyung and Choi, Jinwoo and Yu, Garyfallia and Rashidi, Saeed and Won, William and Meng, Zhaodong and Krishna, Tushar},
  title     = {Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces},
  journal   = {arXiv preprint arXiv:2305.14516},
  year      = {2023}
}

@article{echo2024,
  author    = {Cai, Kai and Miao, Wei and Zhu, Junyu and Chen, Jiaxu and Shan, Hao and Li, Huanyu and Zhang, Chi},
  title     = {Echo: Simulating Distributed Training At Scale},
  journal   = {arXiv preprint arXiv:2412.12487},
  year      = {2024}
}

% ============================================================================
% GNN-Based Performance Modeling
% ============================================================================

@inproceedings{granite2022,
  author    = {Sykora, Ondrej and Rucker, Alexis and Mendis, Charith and Barik, Rajkishore and Phothilimthana, Phitchaya Mangpo and Amarasinghe, Saman},
  title     = {GRANITE: A Graph Neural Network Model for Basic Block Throughput Estimation},
  booktitle = {Proceedings of the IEEE International Symposium on Workload Characterization (IISWC)},
  year      = {2022},
  pages     = {1--13},
  doi       = {10.1109/IISWC55918.2022.00014}
}

% ============================================================================
% Meta-Learning and Transfer Learning
% ============================================================================

@inproceedings{help2021,
  author    = {Lee, Hayeon and Lee, Sewoong and Chong, Song and Hwang, Sung Ju},
  title     = {{HELP}: Hardware-Adaptive Efficient Latency Prediction for {NAS} via Meta-Learning},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {34},
  year      = {2021},
  pages     = {27016--27028}
}

% ============================================================================
% Additional References for Section 4
% ============================================================================

@inproceedings{tenset2021,
  author    = {Zheng, Lianmin and Liu, Ruochen and Shao, Junru and Chen, Tianqi and Gonzalez, Joseph E. and Stoica, Ion and Zhang, Zhihao},
  title     = {{TenSet}: A Large-scale Program Performance Dataset for Learned Tensor Compilers},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {34},
  year      = {2021},
  pages     = {29876--29888}
}

@inproceedings{vllm2023,
  author    = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
  title     = {Efficient Memory Management for Large Language Model Serving with {PagedAttention}},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles (SOSP)},
  year      = {2023},
  pages     = {611--626},
  doi       = {10.1145/3600006.3613165}
}

@inproceedings{flashattention2022,
  author    = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
  title     = {{FlashAttention}: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  year      = {2022},
  pages     = {16344--16359}
}

@article{synperf2025,
  author    = {Wang, Zixian and others},
  title     = {{SynPerf}: Synthesizing High-Performance GPU Kernels via Pipeline Decomposition},
  journal   = {arXiv preprint},
  year      = {2025},
  note      = {Under review}
}

@inproceedings{medusa2024,
  author    = {Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D. and Chen, Deming and Dao, Tri},
  title     = {{MEDUSA}: Simple {LLM} Inference Acceleration Framework with Multiple Decoding Heads},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year      = {2024},
  pages     = {1--15}
}

@inproceedings{orca2022,
  author    = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  title     = {{ORCA}: A Distributed Serving System for Transformer-Based Generative Models},
  booktitle = {Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2022},
  pages     = {521--538}
}

@inproceedings{distserve2024,
  author    = {Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianyu and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  title     = {{DistServe}: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving},
  booktitle = {Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2024},
  pages     = {1--18}
}

@article{rooflinellm2024,
  author    = {Imai, Ryota and Harada, Kentaro and Sato, Ryo and Nakaike, Toshio},
  title     = {Roofline-Driven Machine Learning for Large Language Model Performance Prediction},
  journal   = {NeurIPS Workshop on Machine Learning for Systems},
  year      = {2024}
}

% ============================================================================
% Additional Key Papers - Added by Maya
% ============================================================================

@inproceedings{paleo2017,
  author    = {Qi, Hang and Sparks, Evan R. and Talwalkar, Ameet},
  title     = {Paleo: A Performance Model for Deep Neural Networks},
  booktitle = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
  year      = {2017},
  url       = {https://openreview.net/forum?id=SyVVJ85lg}
}

@inproceedings{splitwise2024,
  author    = {Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aakanksha and Goiri, \'I\~nigo and Maleki, Saeed and Bianchini, Ricardo},
  title     = {Splitwise: Efficient Generative {LLM} Inference Using Phase Splitting},
  booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2024},
  pages     = {118--132},
  note      = {Best Paper Award},
  doi       = {10.1109/ISCA59077.2024.00019}
}

@inproceedings{sarathi2024,
  author    = {Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S. and Tumanov, Alexey and Ramachandran, Ramachandran},
  title     = {Taming Throughput-Latency Tradeoff in {LLM} Inference with {Sarathi-Serve}},
  booktitle = {Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2024},
  pages     = {117--134}
}

@inproceedings{madmax2024,
  author    = {Hsia, Samuel and Chandra, Kartik and Olukotun, Kunle},
  title     = {{MAD} Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems},
  booktitle = {Proceedings of the 51st Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2024},
  pages     = {753--766},
  doi       = {10.1109/ISCA59077.2024.00064}
}

% ============================================================================
% Tensor Program Cost Models (Tool: TLP)
% ============================================================================

@inproceedings{tlp2023,
  author    = {Zhai, Yi and Wang, Yu Cheng and Jiang, Peng and Kang, Congming},
  title     = {{TLP}: A Deep Learning-based Cost Model for Tensor Program Tuning},
  booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2023},
  pages     = {833--845},
  doi       = {10.1145/3575693.3575736}
}

% ============================================================================
% Recent 2025-2026 Papers - Added by Maya
% ============================================================================

@article{life2025,
  author    = {Gavriilidis, Paraskevas and others},
  title     = {{LIFE}: Forecasting {LLM} Inference Performance via Hardware-Agnostic Analytical Modeling},
  journal   = {arXiv preprint arXiv:2508.00904},
  year      = {2025},
  note      = {Hardware-agnostic analytical model for LLM inference performance forecasting}
}

@inproceedings{throttllem2025,
  author    = {Kakolyris, Andreas Kosmas and Masouros, Dimosthenis and Vavaroutsos, Petros and Xydis, Sotirios and Soudris, Dimitrios},
  title     = {{throttLL'eM}: Predictive {GPU} Throttling for Energy Efficient {LLM} Inference Serving},
  booktitle = {Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year      = {2025},
  pages     = {1--14},
  note      = {Achieves up to 43.8\% lower energy consumption for LLM inference}
}

@inproceedings{aqua2025,
  author    = {Shen, Zhuomin and Kim, Jaeho and others},
  title     = {{AQUA}: Network-Accelerated Memory Offloading for {LLMs} in Scale-Up {GPU} Domains},
  booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2025},
  pages     = {1--16},
  doi       = {10.1145/3676641.3715983},
  note      = {Improves LLM inference responsiveness by 20x through network-accelerated memory offloading}
}

@inproceedings{esm2025,
  author    = {Nasir, Azaz-Ur-Rehman and Shoaib, Samroz Ahmad and Hanif, Muhammad Abdullah and Shafique, Muhammad},
  title     = {{ESM}: A Framework for Building Effective Surrogate Models for Hardware-Aware Neural Architecture Search},
  booktitle = {Proceedings of the 62nd ACM/IEEE Design Automation Conference (DAC)},
  year      = {2025},
  pages     = {1--6},
  note      = {97.6\% accuracy surrogate model framework for HW-aware NAS}
}

% ============================================================================
% 2025-2026 Papers - Added by Maya (Cycle 2)
% ============================================================================

@article{omniwise2025,
  author    = {Haj-Ali, Ameer and others},
  title     = {{Omniwise}: Predicting {GPU} Kernels Performance with {LLMs}},
  journal   = {arXiv preprint arXiv:2506.20886},
  year      = {2025},
  note      = {First LLM-based GPU kernel performance prediction, 90\% within 10\% error on AMD MI250/MI300X}
}

@article{hermes2025,
  author    = {Bambhaniya, Abhimanyu Rajeshkumar and others},
  title     = {{HERMES}: Understanding and Optimizing Multi-Stage {AI} Inference Pipelines},
  journal   = {arXiv preprint arXiv:2504.09775},
  year      = {2025},
  note      = {Heterogeneous multi-stage LLM inference simulator with analytical modeling}
}

@inproceedings{podattention2025,
  author    = {Hao, Yanbin and others},
  title     = {{POD-Attention}: Unlocking Full Prefill-Decode Overlap for Faster {LLM} Inference},
  booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2025},
  pages     = {1--15},
  note      = {Full overlap between prefill and decode phases for LLM inference}
}

% ============================================================================
% 2025-2026 Papers - Added by Maya (Cycle 3)
% ============================================================================

@article{frontier2025,
  author    = {Ghosh, Siddharth and others},
  title     = {{Frontier}: Simulating the Next Generation of {LLM} Inference Systems},
  journal   = {arXiv preprint arXiv:2508.03148},
  year      = {2025},
  note      = {Stage-centric simulator for MoE and disaggregated LLM inference, models expert parallelism and cross-cluster routing}
}

@article{swizzleperf2025,
  author    = {Tschand, Adrian and Awad, Mohamed and others},
  title     = {{SwizzlePerf}: Hardware-Aware {LLMs} for {GPU} Kernel Performance Optimization},
  journal   = {arXiv preprint arXiv:2508.20258},
  year      = {2025},
  note      = {LLM-based spatial optimization for GPU kernels, up to 2.06x speedup via swizzling}
}

% ============================================================================
% ISCA 2025 / MLSys 2025 / MICRO 2025 / HPCA 2026 Papers - Added by Sage
% ============================================================================

@inproceedings{concorde2025,
  author    = {Nasr-Esfahany, Amir and others},
  title     = {Concorde: Fast and Accurate {CPU} Performance Modeling with Compositional Analytical-{ML} Fusion},
  booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2025},
  pages     = {1--15},
  note      = {Hybrid analytical-ML approach achieving 2\% CPI error at 5 orders of magnitude faster than gem5}
}

@inproceedings{amali2025,
  author    = {Cao, Zheng and others},
  title     = {{AMALI}: An Analytical Model for Accurately Modeling {LLM} Inference on Modern {GPUs}},
  booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2025},
  pages     = {1--14},
  doi       = {10.1145/3695053.3731064},
  note      = {Reduces GPU LLM inference MAPE from 127.56\% to 23.59\% vs GCoM baseline}
}

@inproceedings{triosim2025,
  author    = {Li, Jianbo and others},
  title     = {{TrioSim}: A Lightweight Simulator for Large-Scale {DNN} Workloads on Multi-{GPU} Systems},
  booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2025},
  pages     = {1--13},
  note      = {Multi-GPU DNN simulation with lightweight approach for distributed training analysis}
}

@inproceedings{lumos2025,
  author    = {Liang, Wenxuan and others},
  title     = {Lumos: Efficient Performance Modeling and Estimation for Large-scale {LLM} Training},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2025},
  pages     = {1--16},
  note      = {Trace-driven performance modeling achieving 3.3\% error on H100 GPUs for LLM training}
}

@inproceedings{pytorchsim2025,
  author    = {Kim, Jungho and others},
  title     = {{PyTorchSim}: A Comprehensive, Fast, and Accurate {NPU} Simulation Framework},
  booktitle = {Proceedings of the 58th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2025},
  pages     = {1--14},
  doi       = {10.1145/3725843.3756045},
  note      = {PyTorch 2-integrated NPU simulator with custom RISC-V ISA and Tile-Level Simulation}
}

@inproceedings{dynamicreasoning2026,
  author    = {Kim, Jiin and Shin, Byeongjun and Chung, Jinha and Rhu, Minsoo},
  title     = {The Cost of Dynamic Reasoning: Demystifying {AI} Agents and Test-Time Scaling from an {AI} Infrastructure Perspective},
  booktitle = {Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year      = {2026},
  month     = jan,
  pages     = {1--14},
  note      = {HPCA 2026 (Jan 31--Feb 4, 2026, Las Vegas). First comprehensive system-level analysis of AI agents; quantifies resource usage, latency, and datacenter power consumption}
}

% ============================================================================
% Foundational Papers - Simulation Sampling (Added by Maya)
% ============================================================================

@inproceedings{simpoint2002,
  author    = {Sherwood, Timothy and Perelman, Erez and Hamerly, Greg and Calder, Brad},
  title     = {Automatically Characterizing Large Scale Program Behavior},
  booktitle = {Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2002},
  pages     = {45--57},
  doi       = {10.1145/605397.605403},
  note      = {Introduces SimPoint: automatic selection of representative simulation points using k-means clustering}
}

@inproceedings{smarts2003,
  author    = {Wunderlich, Roland E. and Wenisch, Thomas F. and Falsafi, Babak and Hoe, James C.},
  title     = {{SMARTS}: Accelerating Microarchitecture Simulation via Rigorous Statistical Sampling},
  booktitle = {Proceedings of the 30th Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2003},
  pages     = {84--97},
  doi       = {10.1109/ISCA.2003.1206991},
  note      = {Statistical sampling achieving 0.64\% CPI error with 35x speedup over detailed simulation}
}

@inproceedings{looppoint2022,
  author    = {Sabu, Alen and Patil, Harish and Haj-Ali, Ameer and Carlson, Trevor E.},
  title     = {{LoopPoint}: Checkpoint-driven Sampled Simulation for Multi-threaded Applications},
  booktitle = {Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year      = {2022},
  pages     = {606--618},
  doi       = {10.1109/HPCA53966.2022.00052},
  note      = {Extends sampling to multi-threaded applications with 2.3\% error and up to 800x speedup}
}

% ============================================================================
% Foundational Papers - Memory Simulation (Added by Maya)
% ============================================================================

@article{dramsim2_2011,
  author    = {Rosenfeld, Paul and Cooper-Balis, Elliott and Jacob, Bruce},
  title     = {{DRAMSim2}: A Cycle Accurate Memory System Simulator},
  journal   = {IEEE Computer Architecture Letters},
  volume    = {10},
  number    = {1},
  pages     = {16--19},
  year      = {2011},
  doi       = {10.1109/L-CA.2011.4},
  note      = {Widely-used cycle-accurate DDR2/DDR3 memory simulator validated against manufacturer Verilog models}
}

@article{dramsim3_2020,
  author    = {Li, Shang and Yang, Zhiyuan and Reddy, Dhiraj and Srivastava, Ankur and Jacob, Bruce},
  title     = {{DRAMsim3}: A Cycle-Accurate, Thermal-Capable {DRAM} Simulator},
  journal   = {IEEE Computer Architecture Letters},
  volume    = {19},
  number    = {2},
  pages     = {106--109},
  year      = {2020},
  doi       = {10.1109/LCA.2020.2973991},
  note      = {Modernized DRAM simulator with thermal modeling and HMC support}
}

@article{ramulator2015,
  author    = {Kim, Yoongu and Yang, Weikun and Mutlu, Onur},
  title     = {Ramulator: A Fast and Extensible {DRAM} Simulator},
  journal   = {IEEE Computer Architecture Letters},
  volume    = {15},
  number    = {1},
  pages     = {45--49},
  year      = {2016},
  doi       = {10.1109/LCA.2015.2414456},
  note      = {Fast extensible DRAM simulator supporting DDRx, LPDDRx, GDDRx, WIOx, HBMx standards}
}

@article{ramulator2_2023,
  author    = {Luo, Haocong and Tugrul, Yahya Can and Bostanc{\i}, F. Nisa and Olgun, Ataberk and Ya{\u{g}}l{\i}kc{\i}, A. Giray and Mutlu, Onur},
  title     = {Ramulator 2.0: A Modern, Modular, and Extensible {DRAM} Simulator},
  journal   = {IEEE Computer Architecture Letters},
  volume    = {22},
  number    = {2},
  pages     = {129--132},
  year      = {2023},
  doi       = {10.1109/LCA.2023.3333759},
  note      = {Modular DRAM simulator with DDR5, LPDDR5, HBM3, GDDR6 support and RowHammer mitigation modeling}
}

% ============================================================================
% Foundational Papers - Hardware Performance Counters (Added by Maya)
% ============================================================================

@article{papi2000,
  author    = {Browne, Shirley and Dongarra, Jack and Garner, Nathan and Ho, George and Mucci, Philip},
  title     = {A Portable Programming Interface for Performance Evaluation on Modern Processors},
  journal   = {International Journal of High Performance Computing Applications},
  volume    = {14},
  number    = {3},
  pages     = {189--204},
  year      = {2000},
  doi       = {10.1177/109434200001400303},
  note      = {PAPI: portable API for hardware performance counters, foundational tool for performance analysis}
}

@inproceedings{likwid2010,
  author    = {Treibig, Jan and Hager, Georg and Wellein, Gerhard},
  title     = {{LIKWID}: A Lightweight Performance-Oriented Tool Suite for x86 Multicore Environments},
  booktitle = {Proceedings of the 39th International Conference on Parallel Processing Workshops (ICPPW)},
  year      = {2010},
  pages     = {207--216},
  doi       = {10.1109/ICPPW.2010.38},
  note      = {Lightweight tools for thread/cache topology, affinity, and performance counter measurement}
}

% ============================================================================
% NEW: Distributed Training Simulation — Added by Maya (issue #148)
% ============================================================================

@inproceedings{simai2025,
  author    = {Wang, Xizheng and Li, Qingxu and Xu, Yichi and Lu, Gang and Zhou, Heyang and Zhang, Sen and Zhu, Yikai and Liu, Yang and Zhang, Pengcheng and Qian, Kun and others},
  title     = {{SimAI}: Unifying Architecture Design and Performance Tuning for Large-Scale {LLM} Training with Scalability and Precision},
  booktitle = {Proceedings of the 22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
  year      = {2025},
  pages     = {1--18},
  note      = {Full-stack LLM training simulator achieving 98.1\% alignment with real-world results. Alibaba Cloud/Tsinghua.}
}

@article{prism2025,
  author    = {Golden, Alicia and others},
  title     = {{PRISM}: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training},
  journal   = {arXiv preprint arXiv:2510.15596},
  year      = {2025},
  note      = {Probabilistic performance modeling for distributed training at 10K+ GPU scale. Meta.}
}

@inproceedings{sailor2025,
  author    = {Strati, Foteini and Zhang, Zhendong and Manos, George and Periz, Ixeia Sanchez and Hu, Qinghao and Chen, Tiancheng and Buzcu, Berk and Han, Song and Delgado, Pamela and Klimovic, Ana},
  title     = {Sailor: Automating Distributed Training over Dynamic, Heterogeneous, and Geo-distributed Clusters},
  booktitle = {Proceedings of the 30th ACM Symposium on Operating Systems Principles (SOSP)},
  year      = {2025},
  pages     = {1--18},
  note      = {Automated distributed training with runtime/memory simulation over heterogeneous resources. ETH Zurich/MIT.}
}

@inproceedings{llama3scaling2025,
  author    = {Chu, Weiwei and Xie, Xinfeng and Yu, Jiecao and Wang, Jie and Balaji, Pavan and Chu, Ching-Hsiang and Park, Jongsoo and others},
  title     = {Scaling {Llama 3} Training with Efficient Parallelism Strategies},
  booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2025},
  pages     = {1--15},
  note      = {4D parallelism for Llama 3 405B on 16K H100 GPUs. Achieves 400 TFLOPs/GPU. Meta.}
}

% ============================================================================
% NEW: Emerging Accelerator Modeling (PIM) — Added by Maya (issue #148)
% ============================================================================

@inproceedings{upimulator2024,
  author    = {Hyun, Bongjoon and Kim, Taehun and Lee, Dongjae and Rhu, Minsoo},
  title     = {Pathfinding Future {PIM} Architectures by Demystifying a Commercial {PIM} Technology},
  booktitle = {Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year      = {2024},
  pages     = {1--15},
  note      = {uPIMulator: cycle-accurate PIM simulation framework for UPMEM. KAIST.}
}

@inproceedings{attacc2024,
  author    = {Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho},
  title     = {{AttAcc}! Unleashing the Power of {PIM} for Batched Transformer-based Generative Model Inference},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2024},
  pages     = {1--16},
  note      = {PIM-based accelerator for batched transformer attention. Seoul National University/UIUC.}
}

@inproceedings{neupims2024,
  author    = {Heo, Guseul and Lee, Sangyeop and Cho, Jaehong and Choi, Hyunmin and Lee, Sanghyeon and Ham, Hyungkyu and Kim, Gwangsun and Mahajan, Divya and Park, Jongse},
  title     = {{NeuPIMs}: {NPU-PIM} Heterogeneous Acceleration for Batched {LLM} Inferencing},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
  year      = {2024},
  pages     = {1--17},
  note      = {NPU-PIM heterogeneous architecture for LLM inference with performance modeling. KAIST/Georgia Tech.}
}

@inproceedings{paise2025,
  author    = {Lee, Hyojung and Baek, Daehyeon and Son, Jimyoung and Choi, Jieun and Moon, Kihyo and Jang, Minsung},
  title     = {{PAISE}: {PIM}-Accelerated Inference Scheduling Engine for Transformer-based {LLM}},
  booktitle = {Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year      = {2025},
  pages     = {1--14},
  note      = {PIM-based LLM inference scheduling. 48.3\% speedup, 11.5\% power reduction. Samsung.}
}

% ============================================================================
% NEW: Training Time / Scaling Law Prediction — Added by Maya (issue #148)
% ============================================================================

@inproceedings{scalinglaws2024,
  author    = {Hagele, Alexander and Bakouch, Elie and Kosson, Atli and Allal, Loubna Ben and Von Werra, Leandro and Jaggi, Martin},
  title     = {Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {37},
  year      = {2024},
  note      = {Spotlight. Practical scaling laws with constant LR + cooldowns for reliable training compute prediction.}
}

@inproceedings{scalinglawguide2025,
  author    = {Choshen, Leshem and Zhang, Yang and Andreas, Jacob},
  title     = {A Hitchhiker's Guide to Scaling Law Estimation},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year      = {2025},
  pages     = {1--25},
  note      = {Practical guidance for scaling law estimation from 485 published pretrained models. IBM/MIT.}
}

% ============================================================================
% Foundational References — Added by Leo (issue #173)
% ============================================================================

@inproceedings{tpuv1_2017,
  author    = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borber, Al and others},
  title     = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2017},
  pages     = {1--12},
  doi       = {10.1145/3079856.3080246},
  note      = {First dedicated ML inference accelerator; 15--30x over CPUs/GPUs on CNN inference}
}

@article{tpuv4_2023,
  author    = {Jouppi, Norman P. and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Patil, Nishant and Laudon, James and Young, Cliff and Patterson, David},
  title     = {{TPU} v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
  journal   = {Proceedings of the 50th Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2023},
  pages     = {1--14},
  doi       = {10.1145/3579371.3589350},
  note      = {4096-chip pods with 3D optical interconnect; up to 1.7x/2.1x faster than TPU v3}
}

@inproceedings{sze2017efficient,
  author    = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  title     = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  booktitle = {Proceedings of the IEEE},
  volume    = {105},
  number    = {12},
  pages     = {2295--2329},
  year      = {2017},
  doi       = {10.1109/JPROC.2017.2761740},
  note      = {Canonical DNN accelerator taxonomy covering dataflows, data reuse, and energy efficiency}
}

@inproceedings{pytorch2019,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  title     = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {32},
  year      = {2019},
  pages     = {8024--8035}
}

@inproceedings{tensorflow2016,
  author    = {Abadi, Mart\'{\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  title     = {{TensorFlow}: A System for Large-Scale Machine Learning},
  booktitle = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2016},
  pages     = {265--283}
}

@inproceedings{zero2020,
  author    = {Rajbhandari, Samyam and Rasley, Jeff and Rber, Olatunji and He, Yuxiong},
  title     = {{ZeRO}: Memory Optimizations Toward Training Trillion Parameter Models},
  booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (SC)},
  year      = {2020},
  pages     = {1--16},
  doi       = {10.1109/SC41405.2020.00024},
  note      = {DeepSpeed ZeRO optimizer partitioning for memory-efficient distributed training}
}

@inproceedings{megatronlm2020,
  author    = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  title     = {{Megatron-LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  booktitle = {arXiv preprint arXiv:1909.08053},
  year      = {2020},
  note      = {Intra-layer tensor parallelism for large language model training}
}

@inproceedings{gpipe2019,
  author    = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia Xu and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  title     = {{GPipe}: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {32},
  year      = {2019},
  pages     = {103--112}
}

@article{kaplan2020scaling,
  author    = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title     = {Scaling Laws for Neural Language Models},
  journal   = {arXiv preprint arXiv:2001.08361},
  year      = {2020},
  note      = {Original neural scaling laws: power-law relationships between model size, dataset size, compute, and loss}
}

@article{hoffmann2022chinchilla,
  author    = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  title     = {Training Compute-Optimal Large Language Models},
  journal   = {arXiv preprint arXiv:2203.15556},
  year      = {2022},
  note      = {Chinchilla scaling laws: compute-optimal training requires scaling data proportionally to model size}
}

@inproceedings{mlperf_training2020,
  author    = {Mattson, Peter and Cheng, Christine and Coleman, Cody and Diamos, Greg and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and others},
  title     = {{MLPerf} Training Benchmark},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  year      = {2020},
  pages     = {336--349},
  note      = {Standard ML training benchmark suite covering image classification, object detection, NLP, recommendation, reinforcement learning}
}

@inproceedings{mlperf_inference2020,
  author    = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breeshekov, Maxim and Duber, Mark and others},
  title     = {{MLPerf} Inference Benchmark},
  booktitle = {Proceedings of the 47th International Symposium on Computer Architecture (ISCA)},
  year      = {2020},
  pages     = {446--459},
  doi       = {10.1109/ISCA45697.2020.00045},
  note      = {Standard ML inference benchmark suite with server and offline scenarios}
}

@inproceedings{mlperfpower2025,
  author    = {Reddi, Vijay Janapa and others},
  title     = {{MLPerf} Power: Benchmarking the Energy Efficiency of Machine Learning Inference},
  booktitle = {Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year      = {2025},
  pages     = {1--14},
  note      = {Energy efficiency benchmarking for ML inference workloads}
}

@article{hennessy2019golden,
  author    = {Hennessy, John L. and Patterson, David A.},
  title     = {A New Golden Age for Computer Architecture},
  journal   = {Communications of the ACM},
  volume    = {62},
  number    = {2},
  pages     = {48--60},
  year      = {2019},
  doi       = {10.1145/3282307},
  note      = {Turing Award Lecture: domain-specific architectures and the end of Dennard scaling}
}

@article{rakhshanfar2021survey,
  author    = {Rakhshanfar, Mehdi and Zarandi, Aliakbar},
  title     = {A Survey on Machine Learning-based Design Space Exploration for Processor Architectures},
  journal   = {Journal of Systems Architecture},
  volume    = {121},
  pages     = {102339},
  year      = {2021},
  doi       = {10.1016/j.sysarc.2021.102339}
}

% ============================================================================
% NEW: GPU Simulation Advances — Added by Maya (issue #148)
% ============================================================================

@inproceedings{dissectinggpu2025,
  author    = {Huerta, Rodrigo and Shoushtary, Mojtaba Abaie and Cruz, Jose-Lorenzo and Gonzalez, Antonio},
  title     = {Dissecting and Modeling the Architecture of Modern {GPU} Cores},
  booktitle = {Proceedings of the 58th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year      = {2025},
  pages     = {369--384},
  note      = {Reverse-engineers modern NVIDIA GPU cores, improves Accel-Sim to 13.98\% MAPE. UPC Barcelona.}
}
